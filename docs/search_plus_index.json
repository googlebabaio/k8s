{"./":{"url":"./","title":"Introduction","keywords":"","body":"K8S小百科 每个人都对kubernetes有自己的理解，在这个系列中我会将我的理解一一描述下。 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-22 13:24:32 "},"docker/0-docker介绍.html":{"url":"docker/0-docker介绍.html","title":"docker介绍","keywords":"","body":"先来个docker的架构图 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"docker/1-docker的安装.html":{"url":"docker/1-docker的安装.html","title":"docker的安装","keywords":"","body":" 有网络的条件下安装 获取repo源(这个地方选用阿里的) 用yum进行安装 安装完成后启动 查看docker的一些基本信息 将docker作为服务启动 docker官方镜像仓库 配置镜像加速 有网络的条件下安装 说明:环境为centos7.4 获取repo源(这个地方选用阿里的) cd /etc/yum.repos.d/ wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 用yum进行安装 yum install docker-ce -y 安装完成后启动 [root@nazeebodan yum.repos.d]# systemctl start docker 查看docker的一些基本信息 [root@nazeebodan yum.repos.d]# docker info Containers: 0 Running: 0 Paused: 0 Stopped: 0 Images: 0 Server Version: 18.06.0-ce Storage Driver: overlay2 Backing Filesystem: extfs Supports d_type: true Native Overlay Diff: true Logging Driver: json-file Cgroup Driver: cgroupfs Plugins: Volume: local Network: bridge host macvlan null overlay Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog Swarm: inactive Runtimes: runc Default Runtime: runc Init Binary: docker-init containerd version: d64c661f1d51c48782c9cec8fda7604785f93587 runc version: 69663f0bd4b60df09991c08812a60108003fa340 init version: fec3683 Security Options: seccomp Profile: default Kernel Version: 3.10.0-693.2.2.el7.x86_64 Operating System: CentOS Linux 7 (Core) OSType: linux Architecture: x86_64 CPUs: 2 Total Memory: 3.702GiB Name: nazeebodan ID: YRUC:F42Z:PJ3T:CENP:73ZJ:DTKP:T33U:EO66:PBXK:MB7K:V4EN:VWH5 Docker Root Dir: /var/lib/docker Debug Mode (client): false Debug Mode (server): false Registry: https://index.docker.io/v1/ Labels: Experimental: false Insecure Registries: 127.0.0.0/8 Live Restore Enabled: false 将docker作为服务启动 [root@nazeebodan yum.repos.d]# systemctl enable docker Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service. docker官方镜像仓库 docker官方镜像仓库地址为: https://hub.docker.com/ 我们可以通过search命令进行进行的搜索 docker search centos 注意: docker中的centos --这种是没有内核的。 有些时候可以用内核的特性是因为依赖于宿主机的内核的原因 配置镜像加速 注册阿里云 https://dev.aliyun.com/search.html 点击“创建我的容器镜像”，创建完成后再镜像加速器就有相关的操作提示了。 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"docker/2-docker的常用命令.html":{"url":"docker/2-docker的常用命令.html","title":"docker的常用命令","keywords":"","body":" 示例: 1.启动一个应用，后台运行,命名为mydevportal，端口映射为9527 docker run -d -p 9527:9527 --name mydevportal 192.168.3.27:8888/project/devportal 2.批量删除标签为的镜像 docker rmi -f `docker images | grep '' | awk '{print $3}'` 3.Docker释放空间 docker system prune -a 清理所有不被使用的，正在使用的镜像和容器是不会被删除的，但是这个命令要慎用，因为它把整个docker空间都释放掉，有可能你忘记起哪个服务，使用之后会发现这个镜像没有了，就要重新构建镜像。 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"docker/3-dockerfile介绍.html":{"url":"docker/3-dockerfile介绍.html","title":"dockerfile介绍","keywords":"","body":" dockerfile命令 build命令 Dockerfile的基本指令 FROM MAINTAINER ENV RUN CMD EXPOSE ARG ADD COPY ENTRYPOINT VOLUME USER WORKDIR HEALTHCHECK ONBUILD dockerfile命令 build命令 和dockerfile配套使用的指令,根据dockerfile进行进行打包。 Usage: docker build [OPTIONS] PATH | URL | - OPTIONS： -t， --tag list  #指定构建的镜像名称和标记名称 -f， --file string #指定Dockerfiile文件路径 示例： docker build . #不指定镜像名称的话，将会默认以作为镜像名称和标记名称 docker build -t myip:v1 . #镜像名称为nginx，标记名称为v1 docker build -t myip:v1 -f /path/Dockerfile /path #指定dockerfile文件路径 说明： 在构建镜像时，Docker daemon会首先将Dockerfile所在的目录构建成一个context（上下文），然后通过Dockerfile里的COPY或者ADD语句将context里的文件复制到指定的镜像目录里。 所以需要复制到镜像里的文件，无论是脚本、安装包还是配置文件，都需要跟Dockerfile文件放在同一个目录下。 当执行docker build命令后，返回的第一条信息便是在构建上下文。 Dockerfile的基本指令 基本指令有十多个，分别是： FROM MAINTAINER ENV RUN CMD EXPOSE ARG ADD COPY ENTRYPOINT VOLUME USER WORKDIR HEALTHCHECK ONBUILD FROM 语法： FROM 说明：第一个指令必须是FROM了，其指定一个构建镜像的基础源镜像，如果本地没有就会从公共库中拉取，没有指定镜像的标签会使用默认的latest标签，可以出现多次，如果需要在一个Dockerfile中构建多个镜像。 MAINTAINER 语法： MAINTAINER 说明：描述镜像的创建者，名称和邮箱 ENV 用法： ENV #设置一个环境变量 ENV = =... #设置多个环境变量 > 说明：设置容器的环境变量，可以让其后面的RUN命令使用，容器运行的时候这个变量也会保留。 RUN 语法： RUN RUN [\"executable\", \"param1\", \"param2\" ...] 说明：构建镜像时运行的shell命令 例如：RUN yum install httpd或者RUN [\"yum\", \"install\", \"httpd\"] Dockerfile中每一个指令都会建立一层，RUN也不例外。多少个RUN就构建了多少层镜像，多个RUN会产生非常臃肿、多层的镜像，不仅仅增加了构建部署的时间，还容易出错。我们在写Dockerfile的时候，要尽量避免使用多个RUN，尽量将需要执行的命令都写在一个RUN里。多条命令可使用\\来换行，换行的命令前面加上&&，最后还需要将不需要的文件及目录删除，比如安装包、临时目录等，减少容器的大小。 注：Union FS是有最大层数限制的，比如AUFS，曾经是最大不得超过42层，现在是不得超过127层。 CMD 用法： CMD CMD [\"可执行文件\", \"参数1\", \"参数2\"...] 说明： CMD在Dockerfile中只能出现一次，有多个，只有最后一个会有效。其作用是在启动容器的时候提供一个默认的命令项。如果用户执行docker run的时候提供了命令项，就会覆盖掉这个命令。没提供就会使用构建时的命令。 容器运行时执行的shell命令，CMD指令一般应为Dockerfile文件的最后一行。 例如：CMD echo $HOME或者CMD [ \"sh\", \"-c\", \"echo $HOME\" ] 举个例子，启动nginx服务不能使用CMD systemctl start nginx，而应该使用CMD [\"nginx\", \"-g\", \"daemon off;\"]。因为docker会将CMD systemctl start nginx命令理解为CMD [\"sh\", \"-c\", \"systemctl start nginx\"]，主进程实际上是sh，当systemctl start nginx命令执行完后，sh作为主进程退出了，自然容器也会退出。这就是为什么使用CMD systemctl start nginx指令，容器运行不起来的原因。正确的作法是将nginx以前台形式运行，即CMD [\"nginx\", \"-g\", \"daemon off;\"]。 EXPOSE 语法： EXPOSE [...] 说明：告诉Docker服务器容器对外映射的容器端口号，在docker run -p的时候生效。 ARG 语法： ARG #不设置默认值的话，需要使用--build-arg来设置参数的值 ARG [=] #设置参数的默认值 说明： 定义参数，即临时变量，只限于构建镜像时使用，容器运行后是不会存在的。 例如：ARG APT=apt-get 注：在构建命令docker build中用--build-arg = 可以覆盖此参数的值。 例如：docker build --build-arg APT=yum -t myip:v1 . ADD 语法： ADD ... ADD [\"\",... \"\"] 说明： 复制本机文件或目录或远程文件，添加到指定的容器目录，支持GO的正则模糊匹配。路径是绝对路径，不存在会自动创建。 如果源是一个目录，只会复制目录下的内容，目录本身不会复制。 ADD命令会将复制的压缩文件夹自动解压，这也是与COPY命令最大的不同。 源路径是一个压缩文件的话，将会解压到容器的目标路径下；源路径是一个URL的话，会下载或者解压到容器的目标路径下 COPY 语法： COPY ... COPY [\"\",... \"\"] 说明： 源路径可以是多个，甚至可以使用通配符 COPY除了不能自动解压，也不能复制网络文件。其它功能和ADD相同。 ENTRYPOINT 语法： ENTRYPOINT ENTRYPOINT [\"可执行文件\", \"参数1\", \"参数2\"...] 说明： 这个命令和CMD命令一样，唯一的区别是不能被docker run命令的执行命令覆盖，如果要覆盖需要带上选项--entrypoint，如果有多个选项，只有最后一个会生效。 VOLUME 语法： VOLUME VOLUME [\"\", \"\"...] > 说明： 在主机上创建一个挂载，挂载到容器的指定路径。docker run -v命令也能完成这个操作，而且更强大。这个命令不能指定主机的需要挂载到容器的文件夹路径。但docker run -v可以，而且其还可以挂载数据容器。 USER 语法： USER daemon 说明：指定运行容器时的用户名或UID，后续的RUN、CMD、ENTRYPOINT也会使用指定的用户运行命令。 WORKDIR 语法： WORKDIR path 说明：为RUN、CMD、ENTRYPOINT指令配置工作目录。可以使用多个WORKDIR指令，后续参数如果是相对路径，则会基于之前的命令指定的路径。如：WORKDIR /home　　WORKDIR test 。最终的路径就是/home/test。path路径也可以是环境变量，比如有环境变量HOME=/home，WORKDIR $HOME/test也就是/home/test。 HEALTHCHECK 语法: HEALTHCHECK [选项] CMD HEALTHCHECK NONE #如果基础镜像有健康检查指令，使用这行可以取消健康检查指令 说明：容器健康状态检查指令 例如：HEALTHCHECK --interval=5s --timeout=3s CMD curl -fs http://localhost/ || exit 1 HEALTHCHECK的选项有： --interval=：两次健康检查的间隔时间，默认为30秒 --timeout=：每次检查的超时时间，超过这个时间，本次检查就视为失败，默认30秒 --retries=：指定健康检查的次数，默认3次，如果指定次数都失败后，则容器的健康状态为unhealthy（不健康） ONBUILD 语法： ONBUILD [INSTRUCTION] > 说明： 指定以当前镜像为基础镜像构建的下一级镜像运行的命令 配置当前所创建的镜像作为其它新创建镜像的基础镜像时，所执行的操作指令。 换句话说，就是这个镜像创建后，如果其它镜像以这个镜像为基础，会先执行这个镜像的ONBUILD命令。 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"docker/4-dockerfile实战.html":{"url":"docker/4-dockerfile实战.html","title":"dockerfile实战","keywords":"","body":" 镜像的制作分为三类 制作基础镜像 制作一个安装了扩展操作系统的基础镜像 基于这个基础镜像os2制作jenkins的slave镜像 制作运行环境镜像 直接拖取已有的java运行环境镜像 制作tomcat镜像 直接从基础镜像制作nginx镜像 制作php镜像 制作app镜像 利用上面制作好的tomcat85镜像部署应用 将应用部署到java运行环境的镜像 镜像的制作分为三类 基础镜像 运行环境镜像 app镜像 制作基础镜像 制作一个安装了扩展操作系统的基础镜像 FROM debian:stretch MAINTAINER suredandan xrzp@qq.com ENV TIMEZONE=Asia/Shanghai \\ LANG=zh_CN.UTF-8 RUN echo \"${TIMEZONE}\" > /etc/timezone \\ && echo \"$LANG UTF-8\" > /etc/locale.gen \\ && apt-get update -q \\ && ln -sf /usr/share/zoneinfo/${TIMEZONE} /etc/localtime \\ && mkdir -p /home/jenkins/.jenkins \\ && mkdir -p /home/jenkins/agent \\ && mkdir -p /usr/share/jenkins # COPY chhostname.sh /usr/local/bin/chhostname.sh # java/locale/DinD/svn/jnlp RUN DEBIAN_FRONTEND=noninteractive apt-get install -yq vim wget curl apt-utils dialog locales apt-transport-https build-essential bzip2 ca-certificates sudo jq unzip zip gnupg2 software-properties-common \\ && update-locale LANG=$LANG \\ && locale-gen $LANG \\ && DEBIAN_FRONTEND=noninteractive dpkg-reconfigure locales \\ && curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/debian/gpg | sudo apt-key add - \\ && add-apt-repository \"deb [arch=amd64] https://mirrors.aliyun.com/docker-ce/linux/debian $(lsb_release -cs) stable\" \\ && apt-get update -y \\ && apt-get install -y docker-ce \\ && apt-get install -y subversion \\ && groupadd -g 10000 jenkins \\ && useradd -c \"Jenkins user\" -d $HOME -u 10000 -g 10000 -m jenkins \\ && usermod -a -G docker jenkins \\ && sed -i '/^root/a\\jenkins ALL=(ALL:ALL) NOPASSWD:ALL' /etc/sudoers USER root WORKDIR /home/jenkins 推送到私有镜像仓库中: docker build -t 192.168.3.27:8888/ops/os2 . 基于这个基础镜像os2制作jenkins的slave镜像 FROM 192.168.3.27:8888/ops/os2 MAINTAINER suredandan xrzp@qq.com RUN mkdir -p /usr/local/maven \\ && mkdir -p /usr/local/jdk \\ && mkdir -p /root/.kube COPY jdk /usr/local/jdk COPY maven /usr/local/maven COPY kubectl /usr/local/bin/kubectl COPY jenkins-slave /usr/local/bin/jenkins-slave COPY slave.jar /usr/share/jenkins COPY config /root/.kube/ ENV JAVA_HOME=/usr/local/jdk \\ MAVEN_HOME=/usr/local/maven \\ PATH=/usr/local/jdk/bin:/usr/local/maven/bin:$PATH ENTRYPOINT [\"jenkins-slave\"] 制作运行环境镜像 直接拖取已有的java运行环境镜像 docker pull java docker tag java 192.168.3.27:8888/ops/java 制作tomcat镜像 FROM ubuntu MAINTAINER xrzp@qq.com ENV VERSION=8.5.31 ENV JAVA_HOME /usr/local/jdk COPY apache-tomcat-8.5.31.tar.gz . RUN apt-get update && \\ apt-get install wget curl unzip iproute2 net-tools -y && \\ apt-get clean all && \\ tar zxf apache-tomcat-${VERSION}.tar.gz && \\ mv apache-tomcat-${VERSION} /usr/local/tomcat && \\ rm -rf apache-tomcat-${VERSION}.tar.gz /usr/local/tomcat/webapps/* && \\ sed -i '1a JAVA_OPTS=\"-Djava.security.egd=file:/dev/./urandom\"' /usr/local/tomcat/bin/catalina.sh && \\ ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime WORKDIR /usr/local/tomcat EXPOSE 8080 CMD [\"./bin/catalina.sh\", \"run\"] 将镜像推送到私有镜像仓库 docker build -t 192.168.3.27:8888/ops/tomcat85 直接从基础镜像制作nginx镜像 FROM centos:7 MAINTAINER xrzp@qq.com RUN yum install -y gcc gcc-c++ make \\ openssl-devel pcre-devel gd-devel libxslt-devel \\ iproute net-tools telnet wget curl && \\ yum clean all && \\ rm -rf /var/cache/yum/* RUN wget http://nginx.org/download/nginx-1.15.6.tar.gz && \\ tar zxf nginx-1.15.6.tar.gz && \\ cd nginx-1.15.6 && \\ ./configure --prefix=/usr/local/nginx \\ --with-http_ssl_module \\ --with-http_v2_module \\ --with-http_realip_module \\ --with-http_image_filter_module \\ --with-http_gunzip_module \\ --with-http_gzip_static_module \\ --with-http_secure_link_module \\ --with-http_stub_status_module \\ --with-stream \\ --with-stream_ssl_module && \\ make -j 4 && make install && \\ mkdir -p /usr/local/nginx/conf/vhost && \\ rm -rf /usr/local/nginx/html/* && \\ echo \"ok\" >> /usr/local/nginx/html/status.html && \\ cd / && rm -rf nginx-1.15.6* ENV PATH $PATH:/usr/local/nginx/sbin WORKDIR /usr/local/nginx EXPOSE 80 CMD [\"nginx\", \"-g\", \"daemon off;\"] 将镜像推送到私有镜像仓库 docker build -t 192.168.3.27:8888/ops/nginx 制作php镜像 FROM centos:7 MAINTAINER xrzp@qq.com RUN yum install epel-release -y && \\ yum install -y gcc gcc-c++ make gd-devel libxml2-devel \\ libcurl-devel libjpeg-devel libpng-devel openssl-devel \\ libmcrypt-devel libxslt-devel libtidy-devel autoconf \\ iproute net-tools telnet wget curl && \\ yum clean all && \\ rm -rf /var/cache/yum/* RUN wget http://docs.php.net/distributions/php-5.6.36.tar.gz && \\ tar zxf php-5.6.36.tar.gz && \\ cd php-5.6.36 && \\ ./configure --prefix=/usr/local/php \\ --with-config-file-path=/usr/local/php/etc \\ --with-config-file-scan-dir=/usr/local/php/etc/php.d \\ --enable-fpm --enable-opcache --enable-static=no \\ --with-mysql --with-mysqli --with-pdo-mysql \\ --enable-phar --with-pear --enable-session \\ --enable-sysvshm --with-tidy --with-openssl \\ --with-zlib --with-curl --with-gd --enable-bcmath \\ --with-jpeg-dir --with-png-dir --with-freetype-dir \\ --with-iconv --enable-posix --enable-zip \\ --enable-mbstring --with-mhash --with-mcrypt --enable-hash \\ --enable-xml --enable-libxml --enable-debug=no && \\ make -j 4 && make install && \\ cp php.ini-production /usr/local/php/etc/php.ini && \\ cp sapi/fpm/php-fpm.conf /usr/local/php/etc/php-fpm.conf && \\ sed -i \"90a \\daemonize = no\" /usr/local/php/etc/php-fpm.conf && \\ mkdir /usr/local/php/log && \\ cd / && rm -rf php* ENV PATH $PATH:/usr/local/php/sbin WORKDIR /usr/local/php EXPOSE 9000 CMD [\"php-fpm\"] 制作app镜像 利用上面制作好的tomcat85镜像部署应用 FROM 192.168.3.27:8888/ops/tomcat85 ADD target/solo.war /tmp RUN unzip -q /tmp/solo.war -d /usr/local/tomcat/webapps/ROOT COPY deploy/docker-entrypoint.sh /usr/bin/docker-entrypoint.sh ENTRYPOINT [\"docker-entrypoint.sh\"] EXPOSE 8080 CMD [\"./bin/catalina.sh\", \"run\"] 将应用部署到java运行环境的镜像 利用上面的java 镜像 部署我们的java应用 FROM 192.168.3.27:8888/ops/java MAINTAINER suredandan xrzp@qq.com ADD target/devPortal-web-1.0.0-SNAPSHOT.jar /tmp EXPOSE 9527 ENTRYPOINT java -jar /tmp/devPortal-web-1.0.0-SNAPSHOT.jar Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"docker/5-dockerfile最佳实践.html":{"url":"docker/5-dockerfile最佳实践.html","title":"dockerfile最佳实践","keywords":"","body":" dockerfile最佳实践 dockerfile最佳实践 构建出来的镜像尽可能“小” 选择适合的尽量小的基础镜像 尽量让每个镜像的用途都比较集中、单一，避免构造大而复杂、多功能的镜像 过大的基础镜像会造成构建出臃肿的镜像，一般推荐比较小巧的镜像作为基础镜像 提供详细的注释和维护者信息 Dockerfile也是一种代码，需要考虑方便后续扩展和他人使用 使用明确的具体数字信息的版本号信息，而非latest，可以避免无法确认具体版本号，统一环境 尽量减少镜像层的数目 减少镜像层数建议尽量合并RUN指令，可以将多条RUN指令的内容通过&&连接 不要安装不必要的包 构建后删除临时的包 让构建镜像过程尽可能“快” 使用清洁的构建上下文(build context) 及时删除临时和缓存文件，避免构造的镜像过于臃肿，并且这些缓存文件并没有实际用途； 利用构建时缓存(build cache) 合理使用缓存、减少目录下的使用文件，使用.dockeringore文件等 在开启缓存的情况下，内容不变的指令尽量放在前面，这样可以提高指令的复用性； 让镜像变得“好用” 选择官方基础镜像 设置正确的程序入口点 不要加入不必要的限制 让构建镜像变得”省事、省心“ 选择高级别基础镜像 如果确实要从外部引入数据，需要制定持久的地址，并带有版本信息，让他人可以重复使用而不出错。 保持Dockerfile尽可能简单 使用经过安全扫描的基础镜像 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"docker/6-私有镜像仓库介绍.html":{"url":"docker/6-私有镜像仓库介绍.html","title":"私有镜像仓库介绍","keywords":"","body":" 一、harbor的安装 1. 下载安装docker-compose 2. 生成harbor的pem相关文件 3.设置docker证书 4. 配置harcfg文件（修改密码，域名，协议[https]等） 5.执行install.sh进行安装 6. 进行验证 二、harbor使用 1.harbor的业务架构 2.api访问的方式 3.harbor的生命周期管理 三、harbor与k8s结合 1.镜像创建的步骤 2.公有仓库的镜像可以直接拉取 3.私有仓库镜的拉取 harbor的地址: https://github.com/vmware/harbor/releases 一、harbor的安装 安装harbor的步骤： 下载安装docker-compose 生成harbor的pem相关文件 设置docker证书 配置harcfg文件（修改密码，域名，协议[https]等） 执行install.sh 进行安装 进行验证 接下来，我将准备把harbor部署到192.168.3.6这个节点上。 1. 下载安装docker-compose pip install docker-compose wget https://github.com/docker/compose/releases/download/1.23.0-rc2/docker-compose-Linux-x86_64 chmod a+x docker-compose-Linux-x86_64 mv docker-compose-Linux-x86_64 /etc/kubernetes/bin/docker-compose 2. 生成harbor的pem相关文件 [root@k8s-master temp]# cat harbor-csr.jso { \"CN\": \"harbor\", \"hosts\": [ \"127.0.0.1\", \"192.168.3.6\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" } ] } cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/etc/kubernetes/ssl/ca-config.json \\ -profile=kubernetes harbor-csr.json | cfssljson -bare harbor [root@k8s-master temp]# ls -l ha* -rw-r--r-- 1 root root 1050 Oct 15 14:51 harbor.csr -rw-r--r-- 1 root root 258 Oct 15 14:49 harbor-csr.json -rw------- 1 root root 1679 Oct 15 14:51 harbor-key.pem -rw-r--r-- 1 root root 1419 Oct 15 14:51 harbor.pem 3.设置docker证书 这一步是在配置ssl访问才会有的，把ca.pem放在这个目录中 如果如下目录不存在，请创建，如果有域名请按此格式依次创建 mkdir -p /etc/docker/certs.d/192.168.3.6 # mkdir -p /etc/docker/certs.d/[IP2] # mkdir -p /etc/docker/certs.d/[example1.com] 如果端口为443，则不需要指定。如果为自定义端口，请指定端口 # /etc/docker/certs.d/yourdomain.com:port # 将ca根证书依次复制到上述创建的目录中 cp /etckubernetes/ssl/ca.pem /etc/docker/certs.d/192.168.3.6/ 4. 配置harcfg文件（修改密码，域名，协议[https]等） [root@k8s-master harbor]# pwd /usr/local/src/harbor [root@k8s-master harbor]# ll total 854960 drwxr-xr-x 3 root root 23 Oct 15 14:53 common -rw-r--r-- 1 root root 1185 May 2 23:34 docker-compose.clair.yml -rw-r--r-- 1 root root 1725 May 2 23:34 docker-compose.notary.yml -rw-r--r-- 1 root root 3596 May 2 23:34 docker-compose.yml drwxr-xr-x 3 root root 156 May 2 23:34 ha -rw-r--r-- 1 root root 6687 May 2 23:34 harbor.cfg -rw-r--r-- 1 root root 875401338 May 2 23:36 harbor.v1.5.0.tar.gz -rwxr-xr-x 1 root root 5773 May 2 23:34 install.sh -rw-r--r-- 1 root root 10771 May 2 23:34 LICENSE -rw-r--r-- 1 root root 482 May 2 23:34 NOTICE -rwxr-xr-x 1 root root 27379 May 2 23:34 prepare 修改harbor.cfg中的配置信息，主要是修改密码，域名，协议[https]等 注意: 1.如果要改默认的端口80，那么要改2个地方： 1. harbor.cfg中的hostname=192.168.x.x:8888 2. docker-compose.yml 对应的ports prepare #这个用于已经install之后的,如果是第一次就不许执行prepare 2.要修改默认的存放位置,需要修改docker-compose.yml中对应的/data位置 5.执行install.sh进行安装 [root@k8s-master harbor]# ./install.sh [Step 0]: checking installation environment ... Note: docker version: 18.06.1 Note: docker-compose version: 1.23.0 [Step 1]: loading Harbor images ... ... [Step 2]: preparing environment ... ... [Step 3]: checking existing instance of Harbor ... ... [Step 4]: starting Harbor ... Creating network \"harbor_harbor\" with the default driver Creating harbor-log ... done Creating registry ... done Creating redis ... done Creating harbor-adminserver ... done Creating harbor-db ... done Creating harbor-ui ... done Creating harbor-jobservice ... done Creating nginx ... done ✔ ----Harbor has been installed and started successfully.---- Now you should be able to visit the admin portal at http://192.168.3.6 . For more details, please visit https://github.com/vmware/harbor . 6. 进行验证 首先进行login登录验证 发现有报错，如下： [root@k8s-master mysql]# docker login 192.168.3.6 Username: admin Password: Error response from daemon: Get https://192.168.3.6/v2/: dial tcp 192.168.3 .6:443: connect: connection refused 原因是安装的时候使用的是http协议，所以需要改成https协议 先关闭harbor [root@k8s-master harbor]# docker-compose down -v vi harbor.cfg 修改ui_url_protocol为https #set ui_url_protocol ui_url_protocol = https 修改完后prepare，再启动harbor [root@k8s-master harbor]# vim harbor.cfg [root@k8s-master harbor]# ./prepare [root@k8s-master harbor]# docker-compose up -d Creating network \"harbor_harbor\" with the default driver Creating harbor-log ... done Creating redis ... done Creating harbor-adminserver ... done Creating harbor-db ... done Creating registry ... done Creating harbor-ui ... done Creating nginx ... done Creating harbor-jobservice ... done 再次登录，成功： [root@k8s-master /]# docker login 192.168.3.6 Username: admin Password: WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded 其他非admin用户,非默认端口的登录示例: [root@master harbor]# docker login -uyfzx -p1qaz@WSX 192.168.3.27:8888 WARNING! Using --password via the CLI is insecure. Use --password-stdin. WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded 注意： 登录成功后，认证信息自动保存到 ~/.docker/config.json 文件 [root@k8s-master 192.168.3.6]# cat ~/.docker/config.json { \"auths\": { \"192.168.3.6\": { \"auth\": \"YWRtaW46U3RhcioyMDE0\" } }, \"HttpHeaders\": { \"User-Agent\": \"Docker-Client/18.06.1-ce (linux)\" } } 在其他节点同样登录，失败了！！！ [root@k8s-node3 ~]# docker login 192.168.3.6 Authenticating with existing credentials... Login did not succeed, error: Error response from daemon: Get https://192.168.3.6/v2/: x509: certificate signed by unknown authority Username (admin): admin Password: Error response from daemon: Get https://192.168.3.6/v2/: x509: certificate signed by unknown authority 这个应该是CA证书的问题，网上找了找方法没有解决到。 最终还是使用http协议作为非安全的方式进行访问 ：（ ，修改： [root@k8s-node3~]# vim /etc/docker/daemon.json { \"registry-mirrors\": [\"https://re0o947o.mirror.aliyuncs.com\"], \"insecure-registries\":[\"192.168.3.6\"] } 使用ssl的方式留在后面来研究。。。。。。。。。。。。。。。。 通过web端进行登录： 二、harbor使用 1.harbor的业务架构 harbor的业务架构：用户->仓库，其中仓库又分为公共仓库和私有仓库 私有仓库需要登录才能有访问的权限 公共仓库直接可以进行访问 2.api访问的方式 http： curl -u \"admin:Star*2014\" -X GET -H \"Content-Type: application/json\" \"http://192.168.3.6/api/projects/k8spublic\" https： curl -k --cert /etc/kubernetes/ssl/harbor.pem --key /etc/kubernetes/ssl/harbor-key.pem -X GET -H \"Content-Type: application/json\" \"https://192.168.3.6/api/projects/k8spublic\" 3.harbor的生命周期管理 可以使用docker-compose来管理Harbor的生命周期。在harbor的安装目录下，执行相关命令。 docker-compose ps 查看 docker-compose stop 停止 docker-compose start 启动 docker-compose down 删除，利用./install.sh可以重新安装 三、harbor与k8s结合 在docker环境下直接使用,分为三步: docker login docker tag docker push 在k8s中使用harbor分为两种，一种是使用公共仓库的镜像，一种是使用私有仓库的镜像。 1.镜像创建的步骤 创建用户 创建项目 将镜像打标签 docker tag 192.168.3.6/k8spublic/nginx 192.168.3.6/k8sprivate/nginx 将镜像push到仓库中去 [root@k8s-master ~]# docker push 192.168.3.6/k8sprivate/nginx 拉取镜像对于公共仓库和私有仓库就有区别了： 2.公有仓库的镜像可以直接拉取 参考一个yaml文件的例子： apiVersion: extensions/v1beta1 kind: Deployment metadata: labels: run: mynginx name: mynginx namespace: default spec: replicas: 3 selector: matchLabels: run: mynginx template: metadata: labels: run: mynginx spec: containers: - image: 192.168.3.6/k8spublic/nginx imagePullPolicy: Always name: mynginx resources: requests: cpu: 100m terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 可以看到镜像是192.168.3.6/k8spublic/nginx，而这个镜像是属于共有仓库的，所以创建pod的时候可以直接被拉取下来。如果镜像是非共有的，那么创建pod的时候，会报错： [root@k8s-master doc]# kubectl get pod NAME READY STATUS RESTARTS AGE mynginx-7f9dc98fd8-49jlk 0/1 ImagePullBackOff 0 5s mynginx-7f9dc98fd8-lhkqm 0/1 ImagePullBackOff 0 5s mynginx-7f9dc98fd8-pvzjv 0/1 ImagePullBackOff 0 5s troublshooting一下： Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 18s default-scheduler Successfully assigned default/mynginx-7f9dc98fd8-49jlk to k8s-node3 Normal BackOff 12s (x2 over 13s) kubelet, k8s-node3 Back-off pulling image \"192.168.3.6/k8sprivate/nginx\" Warning Failed 12s (x2 over 13s) kubelet, k8s-node3 Error: ImagePullBackOff Normal Pulling 1s (x2 over 15s) kubelet, k8s-node3 pulling image \"192.168.3.6/k8sprivate/nginx\" Warning Failed 1s (x2 over 15s) kubelet, k8s-node3 Failed to pull image \"192.168.3.6/k8sprivate/nginx\": rpc error: code = Unknown desc = Error response from daemon: pull access denied for 192.168.3.6/k8sprivate/nginx, repository does not exist or may require 'docker login' Warning Failed 1s (x2 over 15s) kubelet, k8s-node3 Error: ErrImagePull 会有提示报错: pull access denied for 192.168.3.6/k8sprivate/nginx, repository does not exist or may require 'docker login' Warning Failed,这个说明需要先login 3.私有仓库镜的拉取 所以私有仓库镜像在k8s中的正确的拉取方式是： 3.1 首先获取用户名和密码信息，并以base64的方式进行转码 ``` [root@k8s-master doc]# cat /root/.docker/config.json | base64 -w 0 ewoJImF1dGhzIjogewoJCSIxOTIuMTY4LjMuNiI6IHsKCQkJImF1dGgiOiAiWVdSdGFXNDZVM1JoY2lveU1ERTAiCgkJfQoJfSwKCSJIdHRwSGVhZGVycyI6IHsKCQkiVXNlci1BZ2VudCI6ICJEb2NrZXItQ2xpZW50LzE4LjA2LjEtY2UgKGxpbnV4KSIKCX0KfQ== ``` 3.2 然后将harbor的用户名和密码以secret的方式创建 apiVersion: v1 kind: Secret metadata: name: harborsecret data: .dockerconfigjson: ewoJImF1dGhzIjogewoJCSIxOTIuMTY4LjMuNiI6IHsKCQkJImF1dGgiOiAiWVdSdGFXNDZVM1JoY2lveU1ERTAiCgkJfQoJfSwKCSJIdHRwSGVhZGVycyI6IHsKCQkiVXNlci1BZ2VudCI6ICJEb2NrZXItQ2xpZW50Lz E4LjA2LjEtY2UgKGxpbnV4KSIKCX0KfQ== type: kubernetes.io/dockerconfigjson 3.3 在拉取的时候加上imagePullSecrets [root@k8s-master doc]# cat deploy-secret.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: labels: run: mynginx name: mynginx namespace: default spec: replicas: 3 selector: matchLabels: run: mynginx template: metadata: labels: run: mynginx spec: containers: - image: 192.168.3.6/k8spublic/nginx imagePullPolicy: Always name: mynginx resources: requests: cpu: 100m terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 imagePullSecrets: - name: harborsecret 注意最后的imagePullSecrets: - name: harborsecret Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"docker/7-harbor中物理删除不要的镜像.html":{"url":"docker/7-harbor中物理删除不要的镜像.html","title":"harbor中物理删除不要的镜像","keywords":"","body":" 物理删除harbor中不要的镜像 查看harbor相关镜像的名称 停止harbor 物理删除 启动harbor 再次查看空间 参考：harbor的用户指南 物理删除harbor中不要的镜像 在harbor的UI端删除镜像的操作是逻辑删除的,需要物理删除不要的镜像达到释放空间的目的,是需要将harbor停下来再删除的。 步骤如下： 查看harbor相关镜像的名称 [root@master harbor]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest 93fd78260bd1 3 weeks ago 86.2MB debian stretch 4879790bd60d 3 weeks ago 101MB jenkins/jenkins lts-alpine bcc31d32159f 4 weeks ago 221MB centos latest 75835a67d134 2 months ago 200MB vmware/redis-photon v1.5.0 7c03076402d9 7 months ago 207MB vmware/clair-photon v2.0.1-v1.5.0 7ae4e0670a3f 7 months ago 301MB vmware/notary-server-photon v0.5.1-v1.5.0 0b2b23300552 7 months ago 211MB vmware/notary-signer-photon v0.5.1-v1.5.0 67c41b4a1283 7 months ago 209MB vmware/registry-photon v2.6.2-v1.5.0 3059f44f4b9a 7 months ago 198MB vmware/nginx-photon v1.5.0 e100456182fc 7 months ago 135MB vmware/harbor-log v1.5.0 62bb6b8350d9 7 months ago 200MB vmware/harbor-jobservice v1.5.0 aca9fd2e867f 7 months ago 194MB vmware/harbor-ui v1.5.0 1055166068d0 7 months ago 212MB vmware/harbor-adminserver v1.5.0 019bc4544829 7 months ago 183MB vmware/harbor-db v1.5.0 82354dcf564f 7 months ago 526MB vmware/mariadb-photon v1.5.0 403e7e656499 7 months ago 526MB vmware/postgresql-photon v1.5.0 35c891dea9cf 7 months ago 221MB vmware/harbor-migrator v1.5.0 466c57ab0dc3 7 months ago 1.16GB vmware/photon 1.0 4b481ecbef2a 7 months ago 130MB 注意:我们接下来将用到的镜像是vmware/registry-photon:v2.6.2-v1.5.0 停止harbor [root@master harbor]# docker-compose stop Stopping nginx ... done Stopping harbor-jobservice ... done Stopping harbor-ui ... done Stopping registry ... done Stopping harbor-adminserver ... done Stopping harbor-db ... done Stopping redis ... done Stopping harbor-log ... done 物理删除 docker run -it --name gc --rm --volumes-from registry vmware/registry-photon:v2.6.2-v1.5.0 garbage-collect /etc/registry/config.yml 注意,如果只想看下会删除哪些镜像而不是真正的去删除,则需要加上选项--dry-run 如下: docker run -it --name gc --rm --volumes-from registry vmware/registry-photon:v2.6.2-v1.5.0 garbage-collect --dry-run /etc/registry/config.yml 启动harbor [root@master harbor]# docker-compose up -d Creating network \"harbor_harbor\" with the default driver Creating harbor-log ... done Creating registry ... done Creating harbor-adminserver ... done Creating redis ... done Creating harbor-db ... done Creating harbor-ui ... done Creating harbor-jobservice ... done Creating nginx ... done [root@master harbor]# docker-compose ps Name Command State Ports ------------------------------------------------------------------------------------------------------------------------------------------------- harbor-adminserver /harbor/start.sh Up (health: starting) harbor-db /usr/local/bin/docker-entr ... Up (health: starting) 3306/tcp harbor-jobservice /harbor/start.sh Up harbor-log /bin/sh -c /usr/local/bin/ ... Up (health: starting) 127.0.0.1:1514->10514/tcp harbor-ui /harbor/start.sh Up (health: starting) nginx nginx -g daemon off; Up (health: starting) 0.0.0.0:8443->443/tcp, 0.0.0.0:4443->4443/tcp, 0.0.0.0:8888->80/tcp redis docker-entrypoint.sh redis ... Up 6379/tcp registry /entrypoint.sh serve /etc/ ... Up (health: starting) 5000/tcp 再次查看空间 df -h /harbor的初始化目录 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"docker/8-修改docker默认的数据存放目录.html":{"url":"docker/8-修改docker默认的数据存放目录.html","title":"修改docker默认的数据存放目录","keywords":"","body":" 修改docker默认的数据存放目录 修改docker默认的数据存放目录 systemctl stop docker cd /var/lib cp -rf docker docker.bak cp -rf docker /xxx/ rm -rf docker ln -s /xxx/docker docker systemctl start docker docker info Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"docker/9-使用docker的需要考虑的地方.html":{"url":"docker/9-使用docker的需要考虑的地方.html","title":"使用docker的需要考虑的地方","keywords":"","body":" docker根目录的位置与大小 考虑私有镜像的搭建: https 非80端口 docker私有仓库的位置与大小 os初始镜像的制作应该优于env环境镜像 cmd与entrypoint的区别 dockerfile的最佳实践 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"docker/10-docker网络.html":{"url":"docker/10-docker网络.html","title":"docker网络","keywords":"","body":"参考:https://www.kubernetes.org.cn/4105.html bridge：这是Docker默认的网络驱动，此模式会为每一个容器分配Network Namespace和设置IP等，并将容器连接到一个虚拟网桥上。如果未指定网络驱动，这默认使用此驱动。 host：此网络驱动直接使用宿主机的网络。 none：此驱动不构造网络环境。采用了none 网络驱动，那么就只能使用loopback网络设备，容器只能使用127.0.0.1的本机网络。 overlay：此网络驱动可以使多个Docker daemons连接在一起，并能够使用swarm服务之间进行通讯。也可以使用overlay网络进行swarm服务和容器之间、容器之间进行通讯， macvlan：此网络允许为容器指定一个MAC地址，允许容器作为网络中的物理设备，这样Docker daemon就可以通过MAC地址进行访问的路由。对于希望直接连接网络网络的遗留应用，这种网络驱动有时可能是最好的选择。 Network plugins：可以安装和使用第三方的网络插件。可以在Docker Store或第三方供应商处获取这些插件。 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"docker/11-docker容器的重启策略.html":{"url":"docker/11-docker容器的重启策略.html","title":"docker容器的重启策略","keywords":"","body":" 参考: https://docs.docker.com/engine/reference/run/ docker容器的重启策略 docker通过--restart选项，可以设置容器的重启策略，用于决定在容器退出时docker守护进程是否重启刚刚退出的容器。 no 默认策略，在容器退出时不重启容器 on-failure 在容器非正常退出时（退出状态非0），才会重启容器 on-failure:3 在容器非正常退出时重启容器，最多重启3次 always 在容器退出时总是重启容器 unless-stopped 在容器退出时总是重启容器，但是不考虑在Docker守护进程启动时就已经停止了的容器 docker run -d --restart=always nginx docker run -d --restart=on-failure:10 nginx 注意: --restart选项通常只用于detached模式的容器。 --restart选项不能与--rm选项同时使用。显然，--restart选项适用于detached模式的容器，而--rm选项适用于foreground模式的容器 其他补充 docker 的退出代码 0 表示正常退出 非0，表示异常退出（退出状态码采用chroot标准） 125，Docker守护进程本身的错误 126，容器启动后，要执行的默认命令无法调用 127，容器启动后，要执行的默认命令不存在 其他命令状态码，容器启动后正常执行命令，退出命令时该命令的返回状态码作为容器的退出状态码 查看容器重启次数 查看容器最后一次的启动时间 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-22 16:46:18 "},"docker/12-批量删除多余的镜像或容器.html":{"url":"docker/12-批量删除多余的镜像或容器.html","title":"批量删除多余的镜像或容器","keywords":"","body":"显示所有的容器，过滤出Exited状态的容器，取出这些容器的ID， docker ps -a|grep Exited|awk '{print $1}' 查询所有的容器，过滤出Exited状态的容器，列出容器ID，删除这些容器 docker rm `docker ps -a|grep Exited|awk '{print $1}'` 删除所有未运行的容器（已经运行的删除不了，未运行的就一起被删除了） docker rm $(docker ps -a -q) 根据容器的状态，删除Exited状态的容器 docker rm $(docker ps -qf status=exited) Docker 1.13版本以后，可以使用 docker containers prune 命令，删除孤立的容器。 docker container prune Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"docker/13-harbor升级和迁移.html":{"url":"docker/13-harbor升级和迁移.html","title":"harbor升级和迁移","keywords":"","body":"Harbor upgrade and database migration guide When upgrading your existing Harbor instance to a newer version, you may need to migrate the data in your database and the settings in harbor.cfg. Since the migration may alter the database schema and the settings of harbor.cfg, you should always back up your data before any migration. NOTE: Again, you must back up your data before any data migration. This guide only covers the migration from v1.6.0 to current version, if you are upgrading from earlier versions please refer to the migration guide in release branch to upgrade to v1.6.0 and follow this guide to do the migration to later version. From v1.6.0 on, Harbor will automatically try to do the migrate the DB schema when it starts, so if you are upgrading from v1.6.0 or above it's not necessary to call the migrator tool to migrate the schema. From v1.6.0 on, Harbor migrates DB from MariaDB to PostgreSQL, and combines Harbor, Notary and Clair DB into one. For the change in Database schema please refer to change log. Upgrading Harbor and migrating data Log in to the host that Harbor runs on, stop and remove existing Harbor instance if it is still running: cd harbor docker-compose down Back up Harbor's current files so that you can roll back to the current version when it is necessary. mv harbor /my_backup_dir/harbor Back up database (by default in diretory /data/database) cp -r /data/database /my_backup_dir/ Get the latest Harbor release package from Github: https://github.com/goharbor/harbor/releases Before upgrading Harbor, perform migration first. The migration tool is delivered as a docker image, so you should pull the image from docker hub. Replace [tag] with the release version of Harbor (e.g. v1.5.0) in the below command: docker pull goharbor/harbor-migrator:[tag] Upgrade harbor.cfg. NOTE: The ${harbor_cfg} will be overwritten, you must move it to your installation directory after migration. docker run -it --rm -v ${harbor_cfg}:/harbor-migration/harbor-cfg/harbor.cfg goharbor/harbor-migrator:[tag] --cfg up NOTE: The schema upgrade and data migration of Database is performed by core when Harbor starts, if the migration fails, please check the log of core to debug. Under the directory ./harbor, run the ./install.sh script to install the new Harbor instance. If you choose to install Harbor with components like Notary, Clair, and chartmuseum, refer to Installation & Configuration Guide for more information. Roll back from an upgrade For any reason, if you want to roll back to the previous version of Harbor, follow the below steps: NOTE: Roll back doesn't support upgrade across v1.5.0, like from v1.2.0 to v1.7.0. This is because Harbor changes DB to PostgreSQL from v1.7.0, the migrator cannot roll back data to MariaDB. Stop and remove the current Harbor service if it is still running. cd harbor docker-compose down Remove current Harbor instance. rm -rf harbor Restore the older version package of Harbor. mv /my_backup_dir/harbor harbor Restore database, copy the data files from backup directory to you data volume, by default /data/database. Restart Harbor service using the previous configuration. If previous version of Harbor was installed by a release build: cd harbor ./install.sh 参考: https://blog.51cto.com/dangzhiqiang/1962874 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"docker/14-解决一个静态编译的问题.html":{"url":"docker/14-解决一个静态编译的问题.html","title":"解决一个静态编译的问题","keywords":"","body":"今天在树莓派上测试跑docker，同事给的交付物是后台起的docker和containerd，没有配置文件，因为需要修改/etc/docker/daemon.json这个文件，所以还是需要配置文件以systemctl的方式启动，方便重启。 配置文件如下： containerd [root@localhost system]# pwd /usr/lib/systemd/system [root@localhost system]# cat containerd.service [Unit] Description=containerd container runtime Documentation=https://containerd.io After=network.target [Service] ExecStartPre=/sbin/modprobe overlay ExecStart=/usr/local/bin/containerd KillMode=process Delegate=yes LimitNOFILE=1048576 # Having non-zero Limit*s causes performance problems due to accounting overhead # in the kernel. We recommend using cgroups to do container-local accounting. LimitNPROC=infinity LimitCORE=infinity TasksMax=infinity [Install] WantedBy=multi-user.targe 启动 systemctl enable containerd systemctl start containerd docker [root@localhost system]# pwd /usr/lib/systemd/system [root@localhost system]# cat docker.service [Unit] Description=Docker Application Container Engine Documentation=https://docs.docker.com After=network-online.target firewalld.service Wants=network-online.target [Service] Type=notify # the default is not to use systemd for cgroups because the delegate issues still # exists and systemd currently does not support the cgroup feature set required # for containers run by docker ExecStart=/usr/local/bin/dockerd ExecReload=/bin/kill -s HUP $MAINPID # Having non-zero Limit*s causes performance problems due to accounting overhead # in the kernel. We recommend using cgroups to do container-local accounting. LimitNOFILE=infinity LimitNPROC=infinity LimitCORE=infinity # Uncomment TasksMax if your systemd version supports it. # Only systemd 226 and above support this version. #TasksMax=infinity TimeoutStartSec=0 # set delegate yes so that systemd does not reset the cgroups of docker containers Delegate=yes # kill only the docker process, not all processes in the cgroup KillMode=process # restart the docker process if it exits prematurely Restart=on-failure StartLimitBurst=3 StartLimitInterval=60s [Install] WantedBy=multi-user.target 重启 systemctl enable docker systemctl start docker 在重新启动 时候,报错如下: scheme \\\"unix\\\" not registered, fallback to default scheme\" module=grpc Error starting daemon: error while opening volume store metadata database: timeout 网上查了下，是这个问题： https://github.com/moby/moby/issues/22507 https://stackoverflow.com/questions/43537790/docker-fails-to-start-due-to-volume-store-metadata-database-timeout 解决办法： ps axf | grep docker | grep -v grep | awk '{print \"kill -9 \" $1}' | sudo sh 然后重启docker systemctl restart docker ======================================= 另外一个问题 docker logs xxxxxx standard_init_linux.go:207: exec user process caused \"exec format error\" 这个很有可能就是image制作的有问题,所以启动的时候起来了又停了,可以看看docker ps -a Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-22 13:33:27 "},"arch/0-kubernetes介绍.html":{"url":"arch/0-kubernetes介绍.html","title":"kubernetes介绍","keywords":"","body":"参考： 官网：kubernetes.io 中文社区：https://www.kubernetes.org.cn/ Github：https://github.com/kubernetes/kubernetes 命令行参考：https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands# Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"arch/1-kubernetes的设计理念.html":{"url":"arch/1-kubernetes的设计理念.html","title":"kubernetes的设计理念","keywords":"","body":" 一、分层架构 二、API设计原则 三、控制机制设计原则 一、分层架构 Kubernetes设计理念和功能其实就是一个类似Linux的分层架构，如下图所示: 核心层：Kubernetes最核心的功能，对外提供API构建高层的应用，对内提供插件式应用执行环境 应用层： 部署（无状态应用、有状态应用、批处理任务、集群应用等） 路由（服务发现、DNS解析等） 管理层： 系统度量（如基础设施、容器和网络的度量） 自动化（如自动扩展、动态Provision等） 策略管理（RBAC、Quota、PSP、NetworkPolicy等） 接口层： kubectl命令行工具 客户端SDK 集群联邦 生态系统：在接口层之上的庞大容器集群管理调度的生态系统，可以划分为两个范畴 Kubernetes外部：日志、监控、配置管理、CI、CD、Workflow、FaaS、OTS应用、ChatOps等 Kubernetes内部：CRI、CNI、CVI、镜像仓库、Cloud Provider、集群自身的配置和管理等 二、API设计原则 所有API应该是声明式的 API对象是彼此互补而且可组合的 高层API以操作意图为基础设计 低层API根据高层API的控制需要设计 尽量避免简单封装，不要有在外部API无法显式知道的内部隐藏的机制 API操作复杂度与对象数量成正比 API对象状态不能依赖于网络连接状态 尽量避免让操作机制依赖于全局状态，因为在分布式系统中要保证全局状态的同步是非常困难的 三、控制机制设计原则 控制逻辑应该只依赖于当前状态 假设任何错误的可能，并做容错处理 尽量避免复杂状态机，控制逻辑不要依赖无法监控的内部状态 假设任何操作都可能被任何操作对象拒绝，甚至被错误解析 每个模块都可以在出错后自动恢复 每个模块都可以在必要时优雅地降级服务 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"arch/2-kubernetes的架构.html":{"url":"arch/2-kubernetes的架构.html","title":"kubernetes的架构","keywords":"","body":" 一、Borg 架构 二、kubernetes 架构 三、k8s组件间的通信 四、master架构 五、node架构 一、Borg 架构 Borg 是谷歌内部的大规模集群管理系统，负责对谷歌内部很多核心服务的调度和管理。Borg的目的是让用户能够不必操心资源管理的问题，让他们专注于自己的核心业务，并且做到跨多个数据中心的资源利用率最大化。 Borg主要由BorgMaster、Borglet、borgcfg和Scheduler组成，如下图所示： BorgMaster是整个集群的大脑，负责维护整个集群的状态，并将数据持久化到Paxos存储中； Scheduer负责任务的调度，根据应用的特点将其调度到具体的机器上去； Borglet负责真正运行任务（在容器中）； borgcfg是Borg的命令行工具，用于跟Borg系统交互，一般通过一个配置文件来提交任务。 二、kubernetes 架构 Kubernetes借鉴了Borg的设计理念，比如Pod、Service、Labels和单Pod单IP等。Kubernetes的整体架构跟Borg非常像，如下图所示： Kubernetes主要由以下几个核心组件组成： etcd保存了整个集群的状态； apiserver提供了资源操作的唯一入口，并提供认证、授权、访问控制、API注册和发现等机制； controller manager负责维护集群的状态，比如故障检测、自动扩展、滚动更新等； scheduler负责资源的调度，按照预定的调度策略将Pod调度到相应的机器上； kubelet负责维护容器的生命周期，同时也负责Volume（CSI）和网络（CNI）的管理； 容器运行时负责镜像管理以及Pod和容器的真正运行（CRI）； kube-proxy负责为Service提供cluster内部的服务发现和负载均衡； 除了核心组件，还有一些推荐的Addons： kube-dns负责为整个集群提供DNS服务，1.10开始用core-dns代替 Ingress Controller为服务提供外网入口 Heapster提供资源监控 Dashboard提供GUI Federation提供跨可用区的集群 上图看起来较为抽象，来个简化版的： 三、k8s组件间的通信 下图非常清晰的展示了组件间的通信和相关的协议 四、master架构 五、node架构 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"arch/3-cni.html":{"url":"arch/3-cni.html","title":"cni","keywords":"","body":" CNI (Container Network Interface) Bridge IPAM DHCP host-local ptp IPVLAN MACVLAN Flannel Weave Net Contiv Calico OVN SR-IOV Romana OpenContrail Network Configuration Lists 端口映射示例 其他 Canal kuryr-kubernetes Cilium CNI-Genie 参考 https://github.com/containernetworking/cni https://github.com/containernetworking/plugins Container Networking Interface Specification CNI Extension conventions Kubernetes网络插件CNI学习整理 CNCF CNI系列之一：浅谈kubernetes的网络与CNI(以flannel为例) 容器CNI完全解读（一） kubernetes系列之十三：POD多网卡方案multus-cni之通过CRD给POD配置自由组合网络 Kubernetes的网络接口CNI及灵雀云的实践 借助 Calico，管窥 Kubernetes 网络策略 k8s技术预研12--kubernetes的常见开源网络组件 Kubernetes-基于flannel的集群网络 CNI (Container Network Interface) Container Network Interface (CNI) 最早是由CoreOS发起的容器网络规范，是Kubernetes网络插件的基础。其基本思想为：Container Runtime在创建容器时，先创建好network namespace，然后调用CNI插件为这个netns配置网络，其后再启动容器内的进程。现已加入CNCF，成为CNCF主推的网络模型。 CNI插件包括两部分： CNI Plugin负责给容器配置网络，它包括两个基本的接口 配置网络: AddNetwork(net NetworkConfig, rt RuntimeConf) (types.Result, error) 清理网络: DelNetwork(net NetworkConfig, rt RuntimeConf) error IPAM Plugin负责给容器分配IP地址，主要实现包括host-local和dhcp。 Kubernetes Pod 中的其他容器都是Pod所属pause容器的网络，创建过程为： kubelet 先创建pause容器生成network namespace 调用网络CNI driver CNI driver 根据配置调用具体的cni 插件 cni 插件给pause 容器配置网络 pod 中其他的容器都使用 pause 容器的网络 所有CNI插件均支持通过环境变量和标准输入传入参数： $ echo '{\"cniVersion\": \"0.3.1\",\"name\": \"mynet\",\"type\": \"macvlan\",\"bridge\": \"cni0\",\"isGateway\": true,\"ipMasq\": true,\"ipam\": {\"type\": \"host-local\",\"subnet\": \"10.244.1.0/24\",\"routes\": [{ \"dst\": \"0.0.0.0/0\" }]}}' | sudo CNI_COMMAND=ADD CNI_NETNS=/var/run/netns/a CNI_PATH=./bin CNI_IFNAME=eth0 CNI_CONTAINERID=a CNI_VERSION=0.3.1 ./bin/bridge $ echo '{\"cniVersion\": \"0.3.1\",\"type\":\"IGNORED\", \"name\": \"a\",\"ipam\": {\"type\": \"host-local\", \"subnet\":\"10.1.2.3/24\"}}' | sudo CNI_COMMAND=ADD CNI_NETNS=/var/run/netns/a CNI_PATH=./bin CNI_IFNAME=a CNI_CONTAINERID=a CNI_VERSION=0.3.1 ./bin/host-local 常见的CNI网络插件有 CNI Plugin Chains CNI还支持Plugin Chains，即指定一个插件列表，由Runtime依次执行每个插件。这对支持端口映射（portmapping）、虚拟机等非常有帮助。配置方法可以参考后面的端口映射示例。 Bridge Bridge是最简单的CNI网络插件，它首先在Host创建一个网桥，然后再通过veth pair连接该网桥到container netns。 注意：Bridge模式下，多主机网络通信需要额外配置主机路由，或使用overlay网络。可以借助Flannel或者Quagga动态路由等来自动配置。比如overlay情况下的网络结构为 配置示例 { \"cniVersion\": \"0.3.0\", \"name\": \"mynet\", \"type\": \"bridge\", \"bridge\": \"mynet0\", \"isDefaultGateway\": true, \"forceAddress\": false, \"ipMasq\": true, \"hairpinMode\": true, \"ipam\": { \"type\": \"host-local\", \"subnet\": \"10.10.0.0/16\" } } # export CNI_PATH=/opt/cni/bin # ip netns add ns # /opt/cni/bin/cnitool add mynet /var/run/netns/ns { \"interfaces\": [ { \"name\": \"mynet0\", \"mac\": \"0a:58:0a:0a:00:01\" }, { \"name\": \"vethc763e31a\", \"mac\": \"66:ad:63:b4:c6:de\" }, { \"name\": \"eth0\", \"mac\": \"0a:58:0a:0a:00:04\", \"sandbox\": \"/var/run/netns/ns\" } ], \"ips\": [ { \"version\": \"4\", \"interface\": 2, \"address\": \"10.10.0.4/16\", \"gateway\": \"10.10.0.1\" } ], \"routes\": [ { \"dst\": \"0.0.0.0/0\", \"gw\": \"10.10.0.1\" } ], \"dns\": {} } # ip netns exec ns ip addr 1: lo: mtu 65536 qdisc noop state DOWN group default qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 9: eth0@if8: mtu 1500 qdisc noqueue state UP group default link/ether 0a:58:0a:0a:00:04 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.10.0.4/16 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::8c78:6dff:fe19:f6bf/64 scope link tentative dadfailed valid_lft forever preferred_lft forever # ip netns exec ns ip route default via 10.10.0.1 dev eth0 10.10.0.0/16 dev eth0 proto kernel scope link src 10.10.0.4 IPAM DHCP DHCP插件是最主要的IPAM插件之一，用来通过DHCP方式给容器分配IP地址，在macvlan插件中也会用到DHCP插件。 在使用DHCP插件之前，需要先启动dhcp daemon: /opt/cni/bin/dhcp daemon & 然后配置网络使用dhcp作为IPAM插件 { ... \"ipam\": { \"type\": \"dhcp\", } } host-local host-local是最常用的CNI IPAM插件，用来给container分配IP地址。 IPv4: { \"ipam\": { \"type\": \"host-local\", \"subnet\": \"10.10.0.0/16\", \"rangeStart\": \"10.10.1.20\", \"rangeEnd\": \"10.10.3.50\", \"gateway\": \"10.10.0.254\", \"routes\": [ { \"dst\": \"0.0.0.0/0\" }, { \"dst\": \"192.168.0.0/16\", \"gw\": \"10.10.5.1\" } ], \"dataDir\": \"/var/my-orchestrator/container-ipam-state\" } } IPv6: { \"ipam\": { \"type\": \"host-local\", \"subnet\": \"3ffe:ffff:0:01ff::/64\", \"rangeStart\": \"3ffe:ffff:0:01ff::0010\", \"rangeEnd\": \"3ffe:ffff:0:01ff::0020\", \"routes\": [ { \"dst\": \"3ffe:ffff:0:01ff::1/64\" } ], \"resolvConf\": \"/etc/resolv.conf\" } } ptp ptp插件通过veth pair给容器和host创建点对点连接：veth pair一端在container netns内，另一端在host上。可以通过配置host端的IP和路由来让ptp连接的容器之前通信。 { \"name\": \"mynet\", \"type\": \"ptp\", \"ipam\": { \"type\": \"host-local\", \"subnet\": \"10.1.1.0/24\" }, \"dns\": { \"nameservers\": [ \"10.1.1.1\", \"8.8.8.8\" ] } } IPVLAN IPVLAN 和 MACVLAN 类似，都是从一个主机接口虚拟出多个虚拟网络接口。一个重要的区别就是所有的虚拟接口都有相同的 mac 地址，而拥有不同的 ip 地址。因为所有的虚拟接口要共享 mac 地址，所以有些需要注意的地方： DHCP 协议分配 ip 的时候一般会用 mac 地址作为机器的标识。这个情况下，客户端动态获取 ip 的时候需要配置唯一的 ClientID 字段，并且 DHCP server 也要正确配置使用该字段作为机器标识，而不是使用 mac 地址 IPVLAN支持两种模式： L2 模式：此时跟macvlan bridge 模式工作原理很相似，父接口作为交换机来转发子接口的数据。同一个网络的子接口可以通过父接口来转发数据，而如果想发送到其他网络，报文则会通过父接口的路由转发出去。 L3 模式：此时ipvlan 有点像路由器的功能，它在各个虚拟网络和主机网络之间进行不同网络报文的路由转发工作。只要父接口相同，即使虚拟机/容器不在同一个网络，也可以互相 ping 通对方，因为 ipvlan 会在中间做报文的转发工作。注意 L3 模式下的虚拟接口 不会接收到多播或者广播的报文（这个模式下，所有的网络都会发送给父接口，所有的 ARP 过程或者其他多播报文都是在底层的父接口完成的）。另外外部网络默认情况下是不知道 ipvlan 虚拟出来的网络的，如果不在外部路由器上配置好对应的路由规则，ipvlan 的网络是不能被外部直接访问的。 创建ipvlan的简单方法为 ip link add link type ipvlan mode { l2 | L3 } cni配置格式为 { \"name\": \"mynet\", \"type\": \"ipvlan\", \"master\": \"eth0\", \"ipam\": { \"type\": \"host-local\", \"subnet\": \"10.1.2.0/24\" } } 需要注意的是 ipvlan插件下，容器不能跟Host网络通信 主机接口（也就是master interface）不能同时作为ipvlan和macvlan的master接口 MACVLAN MACVLAN可以从一个主机接口虚拟出多个macvtap，且每个macvtap设备都拥有不同的mac地址（对应不同的linux字符设备）。MACVLAN支持四种模式 bridge模式：数据可以在同一master设备的子设备之间转发 vepa模式：VEPA 模式是对 802.1Qbg 标准中的 VEPA 机制的软件实现，MACVTAP 设备简单的将数据转发到master设备中，完成数据汇聚功能，通常需要外部交换机支持 Hairpin 模式才能正常工作 private模式：Private 模式和 VEPA 模式类似，区别是子 MACVTAP 之间相互隔离 passthrough模式：内核的 MACVLAN 数据处理逻辑被跳过，硬件决定数据如何处理，从而释放了 Host CPU 资源 创建macvlan的简单方法为 ip link add link name macvtap0 type macvtap cni配置格式为 { \"name\": \"mynet\", \"type\": \"macvlan\", \"master\": \"eth0\", \"ipam\": { \"type\": \"dhcp\" } } 需要注意的是 macvlan需要大量 mac 地址，每个虚拟接口都有自己的 mac 地址 无法和 802.11(wireless) 网络一起工作 主机接口（也就是master interface）不能同时作为ipvlan和macvlan的master接口 Flannel Flannel通过给每台宿主机分配一个子网的方式为容器提供虚拟网络，它基于Linux TUN/TAP，使用UDP封装IP包来创建overlay网络，并借助etcd维护网络的分配情况。 Weave Net Weave Net是一个多主机容器网络方案，支持去中心化的控制平面，各个host上的wRouter间通过建立Full Mesh的TCP链接，并通过Gossip来同步控制信息。这种方式省去了集中式的K/V Store，能够在一定程度上减低部署的复杂性，Weave将其称为“data centric”，而非RAFT或者Paxos的“algorithm centric”。 数据平面上，Weave通过UDP封装实现L2 Overlay，封装支持两种模式，一种是运行在user space的sleeve mode，另一种是运行在kernal space的 fastpath mode。Sleeve mode通过pcap设备在Linux bridge上截获数据包并由wRouter完成UDP封装，支持对L2 traffic进行加密，还支持Partial Connection，但是性能损失明显。Fastpath mode即通过OVS的odp封装VxLAN并完成转发，wRouter不直接参与转发，而是通过下发odp 流表的方式控制转发，这种方式可以明显地提升吞吐量，但是不支持加密等高级功能。 Contiv Contiv是思科开源的容器网络方案，主要提供基于Policy的网络管理，并与主流容器编排系统集成。Contiv最主要的优势是直接提供了多租户网络，并支持L2(VLAN), L3(BGP), Overlay (VXLAN)以及思科自家的ACI。 Calico Calico 是一个基于BGP的纯三层的数据中心网络方案（不需要Overlay），并且与OpenStack、Kubernetes、AWS、GCE等IaaS和容器平台都有良好的集成。 Calico在每一个计算节点利用Linux Kernel实现了一个高效的vRouter来负责数据转发，而每个vRouter通过BGP协议负责把自己上运行的workload的路由信息像整个Calico网络内传播——小规模部署可以直接互联，大规模下可通过指定的BGP route reflector来完成。 这样保证最终所有的workload之间的数据流量都是通过IP路由的方式完成互联的。Calico节点组网可以直接利用数据中心的网络结构（无论是L2或者L3），不需要额外的NAT，隧道或者Overlay Network。 此外，Calico基于iptables还提供了丰富而灵活的网络Policy，保证通过各个节点上的ACLs来提供Workload的多租户隔离、安全组以及其他可达性限制等功能。 OVN OVN (Open Virtual Network) 是OVS提供的原生虚拟化网络方案，旨在解决传统SDN架构（比如Neutron DVR）的性能问题。 OVN为Kubernetes提供了两种网络方案： Overaly: 通过ovs overlay连接容器 Underlay: 将VM内的容器连到VM所在的相同网络（开发中） 其中，容器网络的配置是通过OVN的CNI插件来实现。 SR-IOV Intel维护了一个SR-IOV的CNI插件，fork自hustcat/sriov-cni，并扩展了DPDK的支持。 项目主页见https://github.com/Intel-Corp/sriov-cni。 Romana Romana是Panic Networks在2016年提出的开源项目，旨在借鉴 route aggregation的思路来解决Overlay方案给网络带来的开销。 OpenContrail OpenContrail是Juniper推出的开源网络虚拟化平台，其商业版本为Contrail。其主要由控制器和vRouter组成： 控制器提供虚拟网络的配置、控制和分析功能 vRouter提供分布式路由，负责虚拟路由器、虚拟网络的建立以及数据转发 其中，vRouter支持三种模式 Kernel vRouter：类似于ovs内核模块 DPDK vRouter：类似于ovs-dpdk Netronome Agilio Solution (商业产品)：支持DPDK, SR-IOV and Express Virtio (XVIO) michaelhenkel/opencontrail-cni-plugin提供了一个OpenContrail的CNI插件。 Network Configuration Lists CNI SPEC 支持指定网络配置列表，包含多个网络插件，由 Runtime 依次执行。注意 ADD 操作，按顺序依次调用每个插件；而 DEL 操作调用顺序相反 ADD 操作，除最后一个插件，前面每个插件需要增加 prevResult 传递给其后的插件 第一个插件必须要包含 ipam 插件 端口映射示例 下面的例子展示了 bridge+portmap 插件的用法。 首先，配置 CNI 网络使用 bridge+portmap 插件： # cat /root/mynet.conflist { \"name\": \"mynet\", \"cniVersion\": \"0.3.0\", \"plugins\": [ { \"type\": \"bridge\", \"bridge\": \"mynet\", \"ipMasq\": true, \"isGateway\": true, \"ipam\": { \"type\": \"host-local\", \"subnet\": \"10.244.10.0/24\", \"routes\": [ {\"dst\": \"0.0.0.0/0\"} ] } }, { \"type\": \"portmap\", \"capabilities\": {\"portMappings\": true} } ] } 然后通过 CAP_ARGS 设置端口映射参数： # export CAP_ARGS='{ \"portMappings\": [ { \"hostPort\": 9090, \"containerPort\": 80, \"protocol\": \"tcp\", \"hostIP\": \"127.0.0.1\" } ] }' 测试添加网络接口： # ip netns add test # CNI_PATH=/opt/cni/bin NETCONFPATH=/root ./cnitool add mynet /var/run/netns/test { \"interfaces\": [ { \"name\": \"mynet\", \"mac\": \"0a:58:0a:f4:0a:01\" }, { \"name\": \"veth2cfb1d64\", \"mac\": \"4a:dc:1f:b7:56:b1\" }, { \"name\": \"eth0\", \"mac\": \"0a:58:0a:f4:0a:07\", \"sandbox\": \"/var/run/netns/test\" } ], \"ips\": [ { \"version\": \"4\", \"interface\": 2, \"address\": \"10.244.10.7/24\", \"gateway\": \"10.244.10.1\" } ], \"routes\": [ { \"dst\": \"0.0.0.0/0\" } ], \"dns\": {} } 可以从 iptables 规则中看到添加的规则： # iptables-save | grep 10.244.10.7 -A CNI-DN-be1eedf7a76853f303ebd -d 127.0.0.1/32 -p tcp -m tcp --dport 9090 -j DNAT --to-destination 10.244.10.7:80 -A CNI-SN-be1eedf7a76853f303ebd -s 127.0.0.1/32 -d 10.244.10.7/32 -p tcp -m tcp --dport 80 -j MASQUERADE 最后，清理网络接口： # CNI_PATH=/opt/cni/bin NETCONFPATH=/root ./cnitool del mynet /var/run/netns/test 其他 Canal Canal是Flannel和Calico联合发布的一个统一网络插件，提供CNI网络插件，并支持network policy。 kuryr-kubernetes kuryr-kubernetes是OpenStack推出的集成Neutron网络插件，主要包括Controller和CNI插件两部分，并且也提供基于Neutron LBaaS的Service集成。 Cilium Cilium是一个基于eBPF和XDP的高性能容器网络方案，提供了CNI和CNM插件。 项目主页为https://github.com/cilium/cilium。 CNI-Genie CNI-Genie是华为PaaS团队推出的同时支持多种网络插件（支持calico, canal, romana, weave等）的CNI插件。 项目主页为https://github.com/Huawei-PaaS/CNI-Genie。 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"arch/etcd介绍.html":{"url":"arch/etcd介绍.html","title":"etcd介绍","keywords":"","body":"参考： Etcd 架构与实现解析 Etcd解析 查看ETCD的状态 etcdctl --endpoints=https://172.18.53.221:2379,https://172.18.53.223:2379,https://172.18.53.224:2379 --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc/kubernetes/ssl/etcd.pem --key-file=/etc/kubernetes/ssl/etcd-key.pem ls /kubernetes/network/subnets 创建网络配置 /etc/kubernetes/bin/etcdctl --ca-file /etc/kubernetes/ssl/ca.pem --cert-file /etc/kubernetes/ssl/flanneld.pem --key-file /etc/kubernetes/ssl/flanneld-key.pem \\ --no-sync -C https://172.18.53.221:2379,https://172.18.53.223:2379,https://172.18.53.224:2379 \\ mk /kubernetes/network/config '{ \"Network\": \"10.2.0.0/16\", \"Backend\": { \"Type\": \"vxlan\", \"VNI\": 1 }}' Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"install/0-kubernetes的安装部署.html":{"url":"install/0-kubernetes的安装部署.html","title":"kubernetes的安装部署","keywords":"","body":"官方提供的几种Kubernetes部署方式 minikube Minikube是一个工具，可以在本地快速运行一个单点的Kubernetes，尝试Kubernetes或日常开发的用户使用。不能用于生产环境。 官方地址：https://kubernetes.io/docs/setup/minikube/ kubeadm Kubeadm也是一个工具，提供kubeadm init和kubeadm join，用于快速部署Kubernetes集群。 官方地址：https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm/ 二进制包 从官方下载发行版的二进制包，手动部署每个组件，组成Kubernetes集群。 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"install/0-二进制部署.html":{"url":"install/0-二进制部署.html","title":"二进制文件部署","keywords":"","body":" 二进制文件部署 1.初始化环境 2.部署CA证书 3.etcd集群搭建 4.master节点部署 5.部署kubectl工具 6.部署node节点 7.配置flannel网络 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"install/1-二进制部署-初始化环境.html":{"url":"install/1-二进制部署-初始化环境.html","title":"1.初始化环境","keywords":"","body":" 一、基础环境配置 1.配置ssh 2.安装docker（只需要在node节点安装） 3.准备部署的目录 4.准备软件包 k8s组件(本次安装以1.11.3为基线)有4个包： etcd flannel cni插件 cfssl的三个包 最后的准备的包应该如下： 5.解压kubernetes相应的几个包 一、基础环境配置 1.配置ssh 基础环境介绍： 主机名 ip 角色 master 172.18.53.221 master node1 172.18.53.223 node1 node2 172.18.53.224 node2 配置方法： 首先保证安装了包：openssh openssh-client 使用方法： ssh-keygen -t rsa ssh-copy-id master ssh-copy-id node1 ssh-copy-id node2 2.安装docker（只需要在node节点安装） 获取阿里的docker repo cd /etc/yum.repos.d/ wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 用yum进行安装 yum install -y docker-ce ... Complete! 如果中途出问题的解决方法 Error: Package: docker-ce-18.03.1.ce-1.el7.centos.x86_64 (docker-ce-edge) Requires: container-selinux >= 2.9 You could try using --skip-broken to work around the problem You could try running: rpm -Va --nofiles --nodigest ---------------------------------------------------------------------------------------------------------------- container-selinux >= 2.9 这个报错是container-selinux版本低或者是没安装的原因 yum 安装container-selinux 一般的yum源又找不到这个包 需要安装epel源 才能yum安装container-selinux，然后在安装docker-ce就可以了。 wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo yum install epel-release #阿里云上的epel源 然后yum install container-selinux 显示docker相关镜像的命令 yum list docker-ce.x86_64 --showduplicates | sort -r 3.准备部署的目录 mkdir -p /etc/kubernetes/{cfg,bin,ssl,log} 在这儿做一个约定：所有的文件存放路径如下： cfg：存放配置文件 bin：存放二进制文件 ssl：存放证书 log：存放日志 所有的二进制包放在目录 /usr/local/src/ 例如：cfssl和kubernetes的执行文件都放在 /etc/kubernetes/bin目录下 4.准备软件包 k8s组件(本次安装以1.11.3为基线)有4个包： https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.11.md [root@master k8s]# ls -l kube* -rw-r--r-- 1 root root 13908969 Sep 18 13:52 kubernetes-client-linux-amd64.tar.gz -rw-r--r-- 1 root root 99541161 Sep 18 13:52 kubernetes-node-linux-amd64.tar.gz -rw-r--r-- 1 root root 435868550 Sep 18 13:52 kubernetes-server-linux-amd64.tar.gz -rw-r--r-- 1 root root 1818410 Sep 18 13:52 kubernetes.tar.gz etcd https://github.com/etcd-io/etcd/releases flannel https://github.com/coreos/flannel/releases cni插件 cni-plugins-amd64-v0.7.1.tgz https://github.com/containernetworking/plugins/releases https://github.com/containernetworking/plugins/releases/download/v0.7.1/cni-plugins-amd64-v0.7.1.tgz cfssl的三个包 wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 最后的准备的包应该如下： 5.解压kubernetes相应的几个包 [root@master k8spkg]# mv * /usr/local/src/ [root@master k8spkg]# cd /usr/local/src/ [root@master src]# pwd /usr/local/src [root@master src]# ll total 594220 -rw-r--r-- 1 root root 6595195 Sep 18 16:54 cfssl-certinfo_linux-amd64 -rw-r--r-- 1 root root 2277873 Sep 18 16:54 cfssljson_linux-amd64 -rw-r--r-- 1 root root 10376657 Sep 18 16:54 cfssl_linux-amd64 -rw-r--r-- 1 root root 17108856 Sep 18 16:59 cni-plugins-amd64-v0.7.1.tgz -rw-r--r-- 1 root root 11254519 Sep 18 16:57 etcd-v3.3.9-linux-amd64.tar.gz -rw-r--r-- 1 root root 9706487 Sep 18 16:58 flannel-v0.10.0-linux-amd64.tar.gz -rw-r--r-- 1 root root 13908969 Sep 18 16:59 kubernetes-client-linux-amd64.tar.gz -rw-r--r-- 1 root root 99541161 Sep 18 16:59 kubernetes-node-linux-amd64.tar.gz -rw-r--r-- 1 root root 435868550 Sep 18 16:59 kubernetes-server-linux-amd64.tar.gz -rw-r--r-- 1 root root 1818410 Sep 18 16:59 kubernetes.tar.gz [root@master src]# tar -zxf kubernetes.tar.gz [root@master src]# tar -zxf kubernetes-node-linux-amd64.tar.gz [root@master src]# tar -zxf kubernetes-server-linux-amd64.tar.gz [root@master src]# tar -zxf kubernetes-client-linux-amd64.tar.gz 将执行文件copy到bin目录 [root@master src]# mv cfssl* /etc/kubernetes/bin [root@master src]# cd /etc/kubernetes/bin [root@master bin]# [root@master bin]# ls cfssl-certinfo_linux-amd64 cfssljson_linux-amd64 cfssl_linux-amd64 [root@master bin]# chmod +x * [root@master bin]# mv cfssl-certinfo_linux-amd64 cfssl-certinfo [root@master bin]# mv cfssljson_linux-amd64 cfssljson [root@master bin]# mv cfssl_linux-amd64 cfssl [root@master bin]# ll total 18808 -rwxr-xr-x 1 root root 10376657 Sep 18 16:54 cfssl -rwxr-xr-x 1 root root 6595195 Sep 18 16:54 cfssl-certinfo -rwxr-xr-x 1 root root 2277873 Sep 18 16:54 cfssljson Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"install/2-二进制部署-部署CA证书.html":{"url":"install/2-二进制部署-部署CA证书.html","title":"2.部署CA证书","keywords":"","body":" 一、创建认证中心(CA) 1.配置证书生成策略 2.创建用来生成 CA 证书签名请求（CSR）的 JSON 配置文件 3.生成CA证书和私钥(root 证书和私钥) 4.分发证书 一、创建认证中心(CA) CFSSL可以创建一个获取和操作证书的内部认证中心。 运行认证中心需要一个CA证书和相应的CA私钥。任何知道私钥的人都可以充当CA颁发证书。因此，私钥的保护至关重要。 1.配置证书生成策略 # cat ca-config.json { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"8760h\" } } } } 这个策略，有一个默认的配置，和一个profile，可以设置多个profile，这里的profile是kubernetes，也可以是其他用途的比如说etcd等。 默认策略，指定了证书的有效期是一年(8760h) kubernetes策略，指定了证书的用途 signing, 表示该证书可用于签名其它证书；生成的 ca.pem 证书中 CA=TRUE server auth：表示 client 可以用该 CA 对 server 提供的证书进行验证 client auth：表示 server 可以用该 CA 对 client 提供的证书进行验证 2.创建用来生成 CA 证书签名请求（CSR）的 JSON 配置文件 # cat ca-csr.json { \"CN\": \"kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" } ] } CN: Common Name，浏览器使用该字段验证网站是否合法，一般写的是域名。非常重要。浏览器使用该字段验证网站是否合法 C: Country， 国家 L: Locality，地区，城市 O: Organization Name，组织名称，公司名称 OU: Organization Unit Name，组织单位名称，公司部门 ST: State，州，省 3.生成CA证书和私钥(root 证书和私钥) ca证书：ca.pem 私钥：ca-key.pem 初始化CA cfssl gencert -initca ca-csr.json | cfssljson -bare ca这个命令会生成运行CA所必需的文件ca-key.pem（私钥）和ca.pem（证书），还会生成ca.csr（证书签名请求），用于交叉签名或重新签名。 [root@master temp]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca 2018/09/20 09:11:13 [INFO] generating a new CA key and certificate from CSR 2018/09/20 09:11:13 [INFO] generate received request 2018/09/20 09:11:13 [INFO] received CSR 2018/09/20 09:11:13 [INFO] generating key: rsa-2048 2018/09/20 09:11:13 [INFO] encoded CSR 2018/09/20 09:11:13 [INFO] signed certificate with serial number 699701764142099640624355744880845590731160955045 [root@master temp]# ls -lrt total 20 -rw-r--r-- 1 root root 290 Sep 19 09:43 ca-config.json -rw-r--r-- 1 root root 208 Sep 19 09:46 ca-csr.json -rw-r--r-- 1 root root 1359 Sep 20 09:11 ca.pem -rw------- 1 root root 1675 Sep 20 09:11 ca-key.pem -rw-r--r-- 1 root root 1001 Sep 20 09:11 ca.csr 注意： 使用现有的CA私钥，重新生成： ``` cfssl gencert -initca -ca-key key.pem ca-csr.json | cfssljson -bare ca ``` 使用现有的CA私钥和CA证书，重新生成： ``` cfssl gencert -renewca -ca cert.pem -ca-key key.pem ``` 3.1 查看cert(证书信息): cfssl certinfo -cert ca.pem 3.2 查看CSR(证书签名请求)信息： cfssl certinfo -csr ca.csr 4.分发证书 # cp ca.csr ca.pem ca-key.pem ca-config.json /etc/kubernetes/ssl SCP证书到 node1 和 node2 节点 # scp ca.pem node1:/etc/kubernetes/ssl # scp ca.pem node2:/etc/kubernetes/ssl Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"install/3-二进制部署-etcd集群搭建.html":{"url":"install/3-二进制部署-etcd集群搭建.html","title":"3.etcd集群搭建","keywords":"","body":" 搭建三节点的etcd 1.准备安装包 2.创建etcd签名请求： 3.生成etcd证书和私钥 4.分发etcd的证书 5.设置ETCD配置文件 5.1 配置方法1 5.2 配置方法2 6.配置完后启动etcd 7.查看状态 master(etcd-node1)的状态： node1(etcd-node2)的状态： node2(etcd-node3)的状态： 8.用命令查看集群的状态是否正常 搭建三节点的etcd 搭建步骤： 准备安装包 创建etcd签名请求： 生成etcd证书和私钥 分发etcd的证书 设置ETCD配置文件 配置完后启动etcd 查看状态集群的状态是否正常 1.准备安装包 [root@master ssl]# cd /usr/local/src/ [root@master src]# ll total 575416 -rw-r--r-- 1 root root 17108856 Sep 18 16:59 cni-plugins-amd64-v0.7.1.tgz -rw-r--r-- 1 root root 11254519 Sep 18 16:57 etcd-v3.3.9-linux-amd64.tar.gz -rw-r--r-- 1 root root 9706487 Sep 18 16:58 flannel-v0.10.0-linux-amd64.tar.gz drwxr-xr-x 9 root root 4096 Sep 10 02:44 kubernetes -rw-r--r-- 1 root root 13908969 Sep 18 16:59 kubernetes-client-linux-amd64.tar.gz -rw-r--r-- 1 root root 99541161 Sep 18 16:59 kubernetes-node-linux-amd64.tar.gz -rw-r--r-- 1 root root 435868550 Sep 18 16:59 kubernetes-server-linux-amd64.tar.gz -rw-r--r-- 1 root root 1818410 Sep 18 16:59 kubernetes.tar.gz [root@master src]# tar zxf etcd-v3.3.9-linux-amd64.tar.gz [root@master src]# cd etcd-v3.3.9-linux-amd64 [root@master etcd-v3.3.9-linux-amd64]# ls Documentation etcd etcdctl README-etcdctl.md README.md READMEv2-etcdctl.md [root@master etcd-v3.3.9-linux-amd64]# cp etcd etcdctl /etc/kubernetes/bin [root@master etcd-v3.3.9-linux-amd64]# scp etcd etcdctl node1:/etc/kubernetes/bin [root@master etcd-v3.3.9-linux-amd64]# scp etcd etcdctl node2:/etc/kubernetes/bin [root@master etcd-v3.3.9-linux-amd64]# 2.创建etcd签名请求： [root@master temp]# cat etcd-csr.json { \"CN\": \"etcd\", \"hosts\": [ \"127.0.0.1\", \"172.18.53.221\", \"172.18.53.223\", \"172.18.53.224\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" } ] } 3.生成etcd证书和私钥 [root@master temp]# cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/etc/kubernetes/ssl/ca-config.json \\ -profile=kubernetes etcd-csr.json | cfssljson -bare etcd 2018/09/20 09:21:30 [INFO] generate received request 2018/09/20 09:21:30 [INFO] received CSR 2018/09/20 09:21:30 [INFO] generating key: rsa-2048 2018/09/20 09:21:30 [INFO] encoded CSR 2018/09/20 09:21:30 [INFO] signed certificate with serial number 59419064252234230113380579545600310151937183620 2018/09/20 09:21:30 [WARNING] This certificate lacks a \"hosts\" field. This makes it unsuitable for websites. For more information see the Baseline Requirements for the Issuance and Management of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org); specifically, section 10.2.3 (\"Information Requirements\"). [root@master temp]# [root@master temp]# ls -lrt etcd* -rw-r--r-- 1 root root 287 Sep 20 09:20 etcd-csr.json -rw-r--r-- 1 root root 1436 Sep 20 09:21 etcd.pem -rw------- 1 root root 1675 Sep 20 09:21 etcd-key.pem -rw-r--r-- 1 root root 1062 Sep 20 09:21 etcd.csr 4.分发etcd的证书 [root@master temp]# cp etcd*.pem /etc/kubernetes/ssl [root@master temp]# scp etcd*.pem node1:/etc/kubernetes/ssl [root@master temp]# scp etcd*.pem node2:/etc/kubernetes/ssl [root@master temp]# 5.设置ETCD配置文件 设置etcd的配置有两种方式： 所有的配置都写在service文件中 配置文件写在/ect/kubernetes/cfg中，然后在service文件中加上配置的地址 下面我把两种方式的实现都写出来 5.1 配置方法1 先配置etcd.conf 注意：除了ETCD_INITIAL_CLUSTER不改变外，剩下的其他地方涉及到了ip地址的，都需要替换成相应node的ip地址。 另外，要注意ETCD_NAME要和下面的ETCD_INITIAL_CLUSTER中描述的名字对应上。 [root@master ssl]# cat /etc/kubernetes/cfg/etcd.conf #[member] ETCD_NAME=\"etcd-node1\" ETCD_DATA_DIR=\"/var/lib/etcd/default.etcd\" #ETCD_SNAPSHOT_COUNTER=\"10000\" #ETCD_HEARTBEAT_INTERVAL=\"100\" #ETCD_ELECTION_TIMEOUT=\"1000\" ETCD_LISTEN_PEER_URLS=\"https://172.18.53.221:2380\" ETCD_LISTEN_CLIENT_URLS=\"https://172.18.53.221:2379,https://127.0.0.1:2379\" #ETCD_MAX_SNAPSHOTS=\"5\" #ETCD_MAX_WALS=\"5\" #ETCD_CORS=\"\" #[cluster] ETCD_INITIAL_ADVERTISE_PEER_URLS=\"https://172.18.53.221:2380\" # if you use different ETCD_NAME (e.g. test), # set ETCD_INITIAL_CLUSTER value for this name, i.e. \"test=http://...\" ETCD_INITIAL_CLUSTER=\"etcd-node1=https://172.18.53.221:2380,etcd-node2=https://172.18.53.223:2380,etcd-node3=https://172.18.53.224:2380\" ETCD_INITIAL_CLUSTER_STATE=\"new\" ETCD_INITIAL_CLUSTER_TOKEN=\"k8s-etcd-cluster\" ETCD_ADVERTISE_CLIENT_URLS=\"https://172.18.53.221:2379\" #[security] CLIENT_CERT_AUTH=\"true\" ETCD_CA_FILE=\"/etc/kubernetes/ssl/ca.pem\" ETCD_CERT_FILE=\"/etc/kubernetes/ssl/etcd.pem\" ETCD_KEY_FILE=\"/etc/kubernetes/ssl/etcd-key.pem\" PEER_CLIENT_CERT_AUTH=\"true\" ETCD_PEER_CA_FILE=\"/etc/kubernetes/ssl/ca.pem\" ETCD_PEER_CERT_FILE=\"/etc/kubernetes/ssl/etcd.pem\" ETCD_PEER_KEY_FILE=\"/etc/kubernetes/ssl/etcd-key.pem\" 再在服务文件中将配置加上去 [root@master ssl]# cat /etc/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target [Service] Type=simple WorkingDirectory=/var/lib/etcd EnvironmentFile=-/etc/kubernetes/cfg/etcd.conf # set GOMAXPROCS to number of processors ExecStart=/bin/bash -c \"GOMAXPROCS=$(nproc) /etc/kubernetes/bin/etcd\" Type=notify [Install] WantedBy=multi-user.target [root@master ssl]# 5.2 配置方法2 注意：除了initial-cluster不改变外，剩下的其他地方涉及到了ip地址的，都需要替换成相应node的ip地址 [root@master ssl]# cat /usr/lib/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target Documentation=https://github.com/coreos [Service] Type=notify WorkingDirectory=/var/lib/etcd/ EnvironmentFile=-/etc/etcd/etcd.conf ExecStart=/etc/kubernetes/bin/etcd \\ --name=etcd-node1 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://172.18.53.221:2380 \\ --listen-peer-urls=https://172.18.53.221:2380 \\ --listen-client-urls=https://172.18.53.221:2379,https://127.0.0.1:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://172.18.53.221:2379 \\ --initial-cluster-token=etcd-cluster-0 \\ --initial-cluster=\"etcd-node1=https://172.18.53.221:2380,etcd-node2=https://172.18.53.223:2380,etcd-node3=https://172.18.53.224:2380\" \\ --initial-cluster-state=new \\ --data-dir=/var/lib/etcd Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target 6.配置完后启动etcd 注意，配置完第一个先不着急启动，等其他两个节点的配置完成后，三个节点同时启动etcd，否则第一个节点先启动时会一直夯在那儿。 [root@master ssl]# systemctl daemon-reload [[root@master ssl]# systemctl enable etcd 复制相应的配置信息到其他节点，记住一定要修改后才能用！ [[root@master ssl]# scp /etc/kubernetes/cfg/etcd.conf node1:/etc/kubernetes/cfg/ [[root@master ssl]# scp /etc/systemd/system/etcd.service node1:/etc/systemd/system/ [[root@master ssl]# scp /etc/kubernetes/cfg/etcd.conf node2:/etc/kubernetes/cfg/ [[root@master ssl]# scp /etc/systemd/system/etcd.service node2s:/etc/systemd/system/ 在所有节点上创建etcd存储目录并启动etcd [root@master ssl]# mkdir /var/lib/etcd [root@master ssl]# systemctl start etcd [root@master ssl]# systemctl status etcd 7.查看状态 master(etcd-node1)的状态： [root@master etcd]# systemctl status etcd ● etcd.service - Etcd Server Loaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: disabled) Active: active (running) since Thu 2018-09-20 11:16:26 CST; 9s ago Main PID: 1311 (etcd) CGroup: /system.slice/etcd.service └─1311 /etc/kubernetes/bin/etcd Sep 20 11:16:26 master etcd[1311]: set the initial cluster version to 3.0 Sep 20 11:16:26 master etcd[1311]: enabled capabilities for version 3.0 Sep 20 11:16:26 master etcd[1311]: updated the cluster version from 3.0 to 3.3 Sep 20 11:16:26 master etcd[1311]: enabled capabilities for version 3.3 Sep 20 11:16:26 master etcd[1311]: published {Name:etcd-node1 ClientURLs:[https://172.18.53.221:2379]} to cluster b4fed7ac46f8b765 Sep 20 11:16:26 master etcd[1311]: ready to serve client requests Sep 20 11:16:26 master systemd[1]: Started Etcd Server. Sep 20 11:16:26 master etcd[1311]: serving client requests on 127.0.0.1:2379 Sep 20 11:16:26 master etcd[1311]: ready to serve client requests Sep 20 11:16:26 master etcd[1311]: serving client requests on 172.18.53.221:2379 node1(etcd-node2)的状态： [root@node1 ~]# systemctl status etcd ● etcd.service - Etcd Server Loaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: disabled) Active: active (running) since Thu 2018-09-20 11:16:22 CST; 28s ago Main PID: 24275 (etcd) CGroup: /system.slice/etcd.service └─24275 /etc/kubernetes/bin/etcd Sep 20 11:16:27 node1 etcd[24275]: 68bdac33fe7fb4f1 received MsgVoteResp from 68bdac33fe7fb4f1 at term 188 Sep 20 11:16:27 node1 etcd[24275]: 68bdac33fe7fb4f1 [logterm: 2, index: 7] sent MsgVote request to 329981bcd604a91f at term 188 Sep 20 11:16:27 node1 etcd[24275]: 68bdac33fe7fb4f1 [logterm: 2, index: 7] sent MsgVote request to e6faa5d2f5954b4e at term 188 Sep 20 11:16:27 node1 etcd[24275]: 68bdac33fe7fb4f1 received MsgVoteResp from 329981bcd604a91f at term 188 Sep 20 11:16:27 node1 etcd[24275]: 68bdac33fe7fb4f1 [quorum:2] has received 2 MsgVoteResp votes and 0 vote rejections Sep 20 11:16:27 node1 etcd[24275]: 68bdac33fe7fb4f1 became leader at term 188 Sep 20 11:16:27 node1 etcd[24275]: raft.node: 68bdac33fe7fb4f1 elected leader 68bdac33fe7fb4f1 at term 188 Sep 20 11:16:27 node1 etcd[24275]: updating the cluster version from 3.0 to 3.3 Sep 20 11:16:27 node1 etcd[24275]: updated the cluster version from 3.0 to 3.3 Sep 20 11:16:27 node1 etcd[24275]: enabled capabilities for version 3.3 node2(etcd-node3)的状态： [root@node2 ~]# systemctl status etcd ● etcd.service - Etcd Server Loaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: disabled) Active: active (running) since Thu 2018-09-20 11:16:22 CST; 1min 0s ago Main PID: 24324 (etcd) CGroup: /system.slice/etcd.service └─24324 /etc/kubernetes/bin/etcd Sep 20 11:16:25 node2 etcd[24324]: health check for peer e6faa5d2f5954b4e could not connect: dial tcp 172.18.53.221:2380: connect: connection refused Sep 20 11:16:26 node2 etcd[24324]: 329981bcd604a91f [term: 186] received a MsgVote message with higher term from e6faa5d2f5954b4e [term: 187] Sep 20 11:16:26 node2 etcd[24324]: 329981bcd604a91f became follower at term 187 Sep 20 11:16:26 node2 etcd[24324]: 329981bcd604a91f [logterm: 2, index: 7, vote: 0] rejected MsgVote from e6faa5d2f5954b4e [logterm: 1, index: 3] at term 187 Sep 20 11:16:27 node2 etcd[24324]: 329981bcd604a91f [term: 187] received a MsgVote message with higher term from 68bdac33fe7fb4f1 [term: 188] Sep 20 11:16:27 node2 etcd[24324]: 329981bcd604a91f became follower at term 188 Sep 20 11:16:27 node2 etcd[24324]: 329981bcd604a91f [logterm: 2, index: 7, vote: 0] cast MsgVote for 68bdac33fe7fb4f1 [logterm: 2, index: 7] at term 188 Sep 20 11:16:27 node2 etcd[24324]: raft.node: 329981bcd604a91f elected leader 68bdac33fe7fb4f1 at term 188 Sep 20 11:16:27 node2 etcd[24324]: updated the cluster version from 3.0 to 3.3 Sep 20 11:16:27 node2 etcd[24324]: enabled capabilities for version 3.3 8.用命令查看集群的状态是否正常 [root@master etcd]# etcdctl --endpoints=https://172.18.53.221:2379,https://172.18.53.223:2379,https://172.18.53.224:2379 \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem cluster-health member 329981bcd604a91f is healthy: got healthy result from https://172.18.53.224:2379 member 68bdac33fe7fb4f1 is healthy: got healthy result from https://172.18.53.223:2379 member e6faa5d2f5954b4e is healthy: got healthy result from https://172.18.53.221:2379 cluster is healthy ``` Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"install/4-二进制部署-master节点部署.html":{"url":"install/4-二进制部署-master节点部署.html","title":"4.master节点部署","keywords":"","body":" 部署master节点 1.准备安装包 2.创建生成kubernetes的CSR的 JSON 配置文件 3.生成 kubernetes 证书和私钥 4.分发证书 5.配置 kube-apiserver 5.1 创建 kube-apiserver 使用的客户端 token 文件 5.2 创建基础用户名/密码认证配置(abac) 5.3 部署kube-apiserver服务 5.4 启动kube-apiserver服务 5.5 查看kube-apiserver服务状态 6.配置kube-controller-manager 6.1 部署kube-controller-manager服务 6.2 启动kube-controller-manager服务 6.3 查看kube-controller-manager服务状态 7.配置kube-scheduler 7.1 部署kube-scheduler服务 7.2 启动 kube-scheduler 服务 7.3 查看 kube-scheduler 状态 部署master节点 1.准备安装包 [root@master ~]# tar zxf kubernetes.tar.gz [root@master ~]# tar zxf kubernetes-server-linux-amd64.tar.gz [root@master ~]# tar zxf kubernetes-client-linux-amd64.tar.gz [root@master ~]# tar zxf kubernetes-node-linux-amd64.tar.gz [root@master ~]# cd /usr/local/src/kubernetes/server/bin [root@master bin]# ls -l total 1862792 -rwxr-xr-x 1 root root 59470358 Sep 10 02:44 apiextensions-apiserver -rwxr-xr-x 1 root root 138267349 Sep 10 02:44 cloud-controller-manager -rw-r--r-- 1 root root 8 Sep 10 02:44 cloud-controller-manager.docker_tag -rw-r--r-- 1 root root 139661312 Sep 10 02:44 cloud-controller-manager.tar -rwxr-xr-x 1 root root 227878608 Sep 10 02:44 hyperkube -rwxr-xr-x 1 root root 57395147 Sep 10 02:44 kubeadm -rwxr-xr-x 1 root root 58078146 Sep 10 02:44 kube-aggregator -rw-r--r-- 1 root root 8 Sep 10 02:44 kube-aggregator.docker_tag -rw-r--r-- 1 root root 59471872 Sep 10 02:44 kube-aggregator.tar -rwxr-xr-x 1 root root 185513792 Sep 10 02:44 kube-apiserver -rw-r--r-- 1 root root 8 Sep 10 02:44 kube-apiserver.docker_tag -rw-r--r-- 1 root root 186907648 Sep 10 02:44 kube-apiserver.tar -rwxr-xr-x 1 root root 154113026 Sep 10 02:44 kube-controller-manager -rw-r--r-- 1 root root 8 Sep 10 02:44 kube-controller-manager.docker_tag -rw-r--r-- 1 root root 155507200 Sep 10 02:44 kube-controller-manager.tar -rwxr-xr-x 1 root root 55414436 Sep 10 02:44 kubectl -rwxr-xr-x 1 root root 163041864 Sep 10 02:44 kubelet -rwxr-xr-x 1 root root 52068695 Sep 10 02:44 kube-proxy -rw-r--r-- 1 root root 8 Sep 10 02:44 kube-proxy.docker_tag -rw-r--r-- 1 root root 99644928 Sep 10 02:44 kube-proxy.tar -rwxr-xr-x 1 root root 55636875 Sep 10 02:44 kube-scheduler -rw-r--r-- 1 root root 8 Sep 10 02:44 kube-scheduler.docker_tag -rw-r--r-- 1 root root 57030656 Sep 10 02:44 kube-scheduler.tar -rwxr-xr-x 1 root root 2330265 Sep 10 02:44 mounter [root@master bin]# cp kube-apiserver kube-controller-manager kube-scheduler /etc/kubernetes/bin 2.创建生成kubernetes的CSR的 JSON 配置文件 内容和etcd-csr.json非常类似 [root@master temp]# cat kubernetes-csr.json { \"CN\": \"kubernetes\", \"hosts\": [ \"127.0.0.1\", \"172.18.53.221\", \"10.1.0.1\", \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" } ] } 3.生成 kubernetes 证书和私钥 [root@master temp]# cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/etc/kubernetes/ssl/ca-config.json \\ -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes 2018/09/20 13:51:56 [INFO] generate received request 2018/09/20 13:51:56 [INFO] received CSR 2018/09/20 13:51:56 [INFO] generating key: rsa-2048 2018/09/20 13:51:56 [INFO] encoded CSR 2018/09/20 13:51:56 [INFO] signed certificate with serial number 459304795069308189390350956282652310628094139983 2018/09/20 13:51:56 [WARNING] This certificate lacks a \"hosts\" field. This makes it unsuitable for websites. For more information see the Baseline Requirements for the Issuance and Management of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org); specifically, section 10.2.3 (\"Information Requirements\"). [root@master temp]# [root@master temp]# ls -l kube* -rw-r--r-- 1 root root 1245 Sep 20 13:51 kubernetes.csr -rw-r--r-- 1 root root 435 Sep 20 13:49 kubernetes-csr.json -rw------- 1 root root 1679 Sep 20 13:51 kubernetes-key.pem -rw-r--r-- 1 root root 1610 Sep 20 13:51 kubernetes.pem 4.分发证书 [root@master temp]# cp kubernetes*.pem .. [root@master temp]# scp kubernetes*.pem node1:/etc/kubernetes/ssl [root@master temp]# scp kubernetes*.pem node2:/etc/kubernetes/ssl [root@master temp]# 5.配置 kube-apiserver 5.1 创建 kube-apiserver 使用的客户端 token 文件 [root@master temp]# head -c 16 /dev/urandom | od -An -t x | tr -d ' ' cdf6d36d697dae799728ac52531a43a8 [root@master temp]# vim /etc/kubernetes/ssl/bootstrap-token.csv [root@master temp]# cat /etc/kubernetes/ssl/bootstrap-token.csv cdf6d36d697dae799728ac52531a43a8,kubelet-bootstrap,10001,\"system:kubelet-bootstrap\" [root@master temp]# 5.2 创建基础用户名/密码认证配置(abac) [root@master temp]# vim /etc/kubernetes/ssl/basic-auth.csv [root@master temp]# cat /etc/kubernetes/ssl/basic-auth.csv admin123,admin,1 readonly123,readonly,2 注意格式是csv的，顺序是：[密码，用户名，序号] 5.3 部署kube-apiserver服务 [root@master temp]# cat /usr/lib/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/kubernetes/kubernetes After=network.target [Service] ExecStart=/etc/kubernetes/bin/kube-apiserver \\ --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction \\ --bind-address=172.18.53.221 \\ --insecure-bind-address=127.0.0.1 \\ --authorization-mode=Node,RBAC \\ --runtime-config=rbac.authorization.k8s.io/v1 \\ --kubelet-https=true \\ --anonymous-auth=false \\ --basic-auth-file=/etc/kubernetes/ssl/basic-auth.csv \\ --enable-bootstrap-token-auth \\ --token-auth-file=/etc/kubernetes/ssl/bootstrap-token.csv \\ --service-cluster-ip-range=10.1.0.0/16 \\ --service-node-port-range=20000-40000 \\ --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \\ --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\ --client-ca-file=/etc/kubernetes/ssl/ca.pem \\ --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --etcd-cafile=/etc/kubernetes/ssl/ca.pem \\ --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem \\ --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem \\ --etcd-servers=https://172.18.53.221:2379,https://172.18.53.223:2379,https://172.18.53.224:2379 \\ --enable-swagger-ui=true \\ --allow-privileged=true \\ --audit-log-maxage=30 \\ --audit-log-maxbackup=3 \\ --audit-log-maxsize=100 \\ --audit-log-path=/etc/kubernetes/log/api-audit.log \\ --event-ttl=1h \\ --v=2 \\ --logtostderr=false \\ --log-dir=/etc/kubernetes/log Restart=on-failure RestartSec=5 Type=notify LimitNOFILE=65536 [Install] WantedBy=multi-user.target 注意：目前 k8s 支持授权插件： ABAC RBAC Webhook Node 仅v1.7版本以上支持Node授权，配合NodeRestriction准入控制来限制kubelet仅可访问node、endpoint、pod、service以及secret、configmap、PV和PVC等相关的资源，配置方法为 --authorization-mode=Node,RBAC --admission-control=...,NodeRestriction,... 注意，kubelet认证需要使用system:nodes组，并使用用户名system:node:。 5.4 启动kube-apiserver服务 [root@master temp]# systemctl daemon-reload [root@master temp]# systemctl enable kube-apiserver [root@master temp]# systemctl start kube-apiserver 5.5 查看kube-apiserver服务状态 [root@master temp]# systemctl status kube-apiserver ● kube-apiserver.service - Kubernetes API Server Loaded: loaded (/usr/lib/systemd/system/kube-apiserver.service; enabled; vendor preset: disabled) Active: active (running) since Thu 2018-09-20 14:05:52 CST; 7s ago Docs: https://github.com/kubernetes/kubernetes Main PID: 1601 (kube-apiserver) CGroup: /system.slice/kube-apiserver.service └─1601 /etc/kubernetes/bin/kube-apiserver --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction --bind-address=17... Sep 20 14:05:43 master systemd[1]: Starting Kubernetes API Server... Sep 20 14:05:43 master kube-apiserver[1601]: Flag --admission-control has been deprecated, Use --enable-admission-plugins or --disable-admission-plugins instead. Will be remo...ure version. Sep 20 14:05:43 master kube-apiserver[1601]: Flag --insecure-bind-address has been deprecated, This flag will be removed in a future version. Sep 20 14:05:47 master kube-apiserver[1601]: [restful] 2018/09/20 14:05:47 log.go:33: [restful/swagger] listing is available at https://172.18.53.221:6443/swaggerapi Sep 20 14:05:47 master kube-apiserver[1601]: [restful] 2018/09/20 14:05:47 log.go:33: [restful/swagger] https://172.18.53.221:6443/swaggerui/ is mapped to folder /swagger-ui/ Sep 20 14:05:49 master kube-apiserver[1601]: [restful] 2018/09/20 14:05:49 log.go:33: [restful/swagger] listing is available at https://172.18.53.221:6443/swaggerapi Sep 20 14:05:49 master kube-apiserver[1601]: [restful] 2018/09/20 14:05:49 log.go:33: [restful/swagger] https://172.18.53.221:6443/swaggerui/ is mapped to folder /swagger-ui/ Sep 20 14:05:52 master systemd[1]: Started Kubernetes API Server. Hint: Some lines were ellipsized, use -l to show in full. 6.配置kube-controller-manager 6.1 部署kube-controller-manager服务 [root@master temp]# cat /usr/lib/systemd/system/kube-controller-manager.service [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/etc/kubernetes/bin/kube-controller-manager \\ --address=127.0.0.1 \\ --master=http://127.0.0.1:8080 \\ --allocate-node-cidrs=true \\ --service-cluster-ip-range=10.1.0.0/16 \\ --cluster-cidr=10.2.0.0/16 \\ --cluster-name=kubernetes \\ --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \\ --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --root-ca-file=/etc/kubernetes/ssl/ca.pem \\ --leader-elect=true \\ --v=2 \\ --logtostderr=false \\ --log-dir=/etc/kubernetes/log Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target 6.2 启动kube-controller-manager服务 [root@master temp]# systemctl daemon-reload [root@master temp]# systemctl enable kube-controller-manager [root@master temp]# systemctl start kube-controller-manager 6.3 查看kube-controller-manager服务状态 [root@master temp]# systemctl status kube-controller-manager ● kube-controller-manager.service - Kubernetes Controller Manager Loaded: loaded (/usr/lib/systemd/system/kube-controller-manager.service; enabled; vendor preset: disabled) Active: active (running) since Thu 2018-09-20 14:09:54 CST; 6s ago Docs: https://github.com/kubernetes/kubernetes Main PID: 1654 (kube-controller) CGroup: /system.slice/kube-controller-manager.service └─1654 /etc/kubernetes/bin/kube-controller-manager --address=127.0.0.1 --master=http://127.0.0.1:8080 --allocate-node-cidrs=true --service-cluster-ip-range=10.1.0.0/16 --clust... Sep 20 14:09:54 master systemd[1]: Started Kubernetes Controller Manager. Sep 20 14:09:54 master systemd[1]: Starting Kubernetes Controller Manager... 7.配置kube-scheduler 7.1 部署kube-scheduler服务 [root@master temp]# cat /usr/lib/systemd/system/kube-scheduler.service [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/etc/kubernetes/bin/kube-scheduler \\ --address=127.0.0.1 \\ --master=http://127.0.0.1:8080 \\ --leader-elect=true \\ --v=2 \\ --logtostderr=false \\ --log-dir=/etc/kubernetes/log Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target 7.2 启动 kube-scheduler 服务 [root@master temp]# systemctl daemon-reload [root@master temp]# systemctl enable kube-scheduler [root@master temp]# systemctl start kube-scheduler 7.3 查看 kube-scheduler 状态 [root@master temp]# systemctl status kube-scheduler ● kube-scheduler.service - Kubernetes Scheduler Loaded: loaded (/usr/lib/systemd/system/kube-scheduler.service; enabled; vendor preset: disabled) Active: active (running) since Thu 2018-09-20 14:12:01 CST; 4s ago Docs: https://github.com/kubernetes/kubernetes Main PID: 1711 (kube-scheduler) CGroup: /system.slice/kube-scheduler.service └─1711 /etc/kubernetes/bin/kube-scheduler --address=127.0.0.1 --master=http://127.0.0.1:8080 --leader-elect=true --v=2 --logtostderr=false --log-dir=/etc/kubernetes/log Sep 20 14:12:01 master systemd[1]: Started Kubernetes Scheduler. Sep 20 14:12:01 master systemd[1]: Starting Kubernetes Scheduler... Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"install/5-二进制部署-部署kubectl工具.html":{"url":"install/5-二进制部署-部署kubectl工具.html","title":"5.部署kubectl工具","keywords":"","body":" 部署kubectl工具 1.准备二进制文件 2.创建 admin 证书签名请求 3.生成admin证书 4.分发证书 5.配置kubectl的使用的config 5.1 设置集群参数 5.2 设置客户端认证参数 5.3 设置上下文参数 5.4 设置默认上下文 6.使用kubectl查看当前的资源情况 部署kubectl工具 1.准备二进制文件 [root@master temp]# cd /usr/local/src/kubernetes/client/bin/ [root@master bin]# cp kubectl /etc/kubernetes/bin/ [root@master bin]# scp kubectl node1:/etc/kubernetes/bin/ [root@master bin]# scp kubectl node2:/etc/kubernetes/bin/ 2.创建 admin 证书签名请求 [root@master temp]# cat admin-csr.json { \"CN\": \"admin\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"system:masters\", \"OU\": \"System\" } ] } 3.生成admin证书 [root@master temp]# cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/etc/kubernetes/ssl/ca-config.json \\ -profile=kubernetes admin-csr.json | cfssljson -bare admin 2018/09/20 14:28:30 [INFO] generate received request 2018/09/20 14:28:30 [INFO] received CSR 2018/09/20 14:28:30 [INFO] generating key: rsa-2048 2018/09/20 14:28:31 [INFO] encoded CSR 2018/09/20 14:28:31 [INFO] signed certificate with serial number 56918806213694388959411509627148507078688254192 2018/09/20 14:28:31 [WARNING] This certificate lacks a \"hosts\" field. This makes it unsuitable for websites. For more information see the Baseline Requirements for the Issuance and Management of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org); specifically, section 10.2.3 (\"Information Requirements\"). [root@master temp]# ls -l admin* -rw-r--r-- 1 root root 1009 Sep 20 14:28 admin.csr -rw-r--r-- 1 root root 229 Sep 20 14:26 admin-csr.json -rw------- 1 root root 1675 Sep 20 14:28 admin-key.pem -rw-r--r-- 1 root root 1399 Sep 20 14:28 admin.pem 4.分发证书 只想给master节点使用，那就不需要再scp到node节点上去。 [root@master temp]# cp admin*.pem .. 5.配置kubectl的使用的config 因为rbac认证的原因，默认会内置一些角色，可以查看admin-crs.json中的\"O\": \"system:masters\" 5.1 设置集群参数 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://172.18.53.221:6443 5.2 设置客户端认证参数 kubectl config set-credentials admin \\ --client-certificate=/etc/kubernetes/ssl/admin.pem \\ --embed-certs=true \\ --client-key=/etc/kubernetes/ssl/admin-key.pem 5.3 设置上下文参数 kubectl config set-context kubernetes \\ --cluster=kubernetes \\ --user=admin 5.4 设置默认上下文 kubectl config use-context kubernetes 以上所有的操作的最终目的是为了生成 ~/.kube/config文件，如果node节点想使用kubectl的话，就需要将相应的admin*.pem拷贝到/etc/kubernetes/ssl目录，以及拷贝一份 ~/.kube/config文件 6.使用kubectl查看当前的资源情况 [root@master ssl]# kubectl get cs NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-1 Healthy {\"health\":\"true\"} etcd-0 Healthy {\"health\":\"true\"} etcd-2 Healthy {\"health\":\"true\"} Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"install/6-二进制部署-部署node节点.html":{"url":"install/6-二进制部署-部署node节点.html","title":"6.部署node节点","keywords":"","body":" 一、准备工作 1. 准备node节点的kubelet和kube-proxy二进制包 2.创建角色绑定 3.创建 用于kubelet的kubeconfig 文件 二、部署kubelet 1.设置CNI支持 2.创建kubelet目录 3.创建kubelet服务 4.启动kubelet服务 5.查看kubelet服务状态 6.在master节点查看csr请求 7.在master节点通过kubelet 的 TLS 证书请求 三、配置kube-proxy 1.安装kube-proxy使用LVS需要的包 2.创建 kube-proxy 证书请求的json 3.生成kube-proxy证书 4.分发证书 5.创建kube-proxy配置文件 6.创建kube-proxy的工作目录 7.创建kube-proxy服务配置 8.启动kube-proxy服务 9.查看kube-proxy服务状态 10.检查LVS状态 一、准备工作 1. 准备node节点的kubelet和kube-proxy二进制包 [root@master bin]# pwd /usr/local/src/kubernetes/server/bin [root@master bin]# ll total 1862792 -rwxr-xr-x 1 root root 59470358 Sep 10 02:44 apiextensions-apiserver -rwxr-xr-x 1 root root 138267349 Sep 10 02:44 cloud-controller-manager -rw-r--r-- 1 root root 8 Sep 10 02:44 cloud-controller-manager.docker_tag -rw-r--r-- 1 root root 139661312 Sep 10 02:44 cloud-controller-manager.tar -rwxr-xr-x 1 root root 227878608 Sep 10 02:44 hyperkube -rwxr-xr-x 1 root root 57395147 Sep 10 02:44 kubeadm -rwxr-xr-x 1 root root 58078146 Sep 10 02:44 kube-aggregator -rw-r--r-- 1 root root 8 Sep 10 02:44 kube-aggregator.docker_tag -rw-r--r-- 1 root root 59471872 Sep 10 02:44 kube-aggregator.tar -rwxr-xr-x 1 root root 185513792 Sep 10 02:44 kube-apiserver -rw-r--r-- 1 root root 8 Sep 10 02:44 kube-apiserver.docker_tag -rw-r--r-- 1 root root 186907648 Sep 10 02:44 kube-apiserver.tar -rwxr-xr-x 1 root root 154113026 Sep 10 02:44 kube-controller-manager -rw-r--r-- 1 root root 8 Sep 10 02:44 kube-controller-manager.docker_tag -rw-r--r-- 1 root root 155507200 Sep 10 02:44 kube-controller-manager.tar -rwxr-xr-x 1 root root 55414436 Sep 10 02:44 kubectl -rwxr-xr-x 1 root root 163041864 Sep 10 02:44 kubelet -rwxr-xr-x 1 root root 52068695 Sep 10 02:44 kube-proxy -rw-r--r-- 1 root root 8 Sep 10 02:44 kube-proxy.docker_tag -rw-r--r-- 1 root root 99644928 Sep 10 02:44 kube-proxy.tar -rwxr-xr-x 1 root root 55636875 Sep 10 02:44 kube-scheduler -rw-r--r-- 1 root root 8 Sep 10 02:44 kube-scheduler.docker_tag -rw-r--r-- 1 root root 57030656 Sep 10 02:44 kube-scheduler.tar -rwxr-xr-x 1 root root 2330265 Sep 10 02:44 mounter [root@master bin]# scp kubelet kube-proxy node1:/etc/kubernetes/bin [root@master bin]# scp kubelet kube-proxy node2:/etc/kubernetes/bin [root@master bin]# 2.创建角色绑定 [root@master bin]# kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap clusterrolebinding.rbac.authorization.k8s.io/kubelet-bootstrap created 3.创建 用于kubelet的kubeconfig 文件 3.1 设置集群参数 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://172.18.53.221:6443 \\ --kubeconfig=bootstrap.kubeconfig 3.2 设置客户端认证参数 kubectl config set-credentials kubelet-bootstrap \\ --token=cdf6d36d697dae799728ac52531a43a8 \\ --kubeconfig=bootstrap.kubeconfig 3.3 设置上下文参数 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=bootstrap.kubeconfig 3.4 选择默认上下文 kubectl config use-context default --kubeconfig=bootstrap.kubeconfig 3.5 将当前目录生成的 bootstrap.kubeconfig 分发到node节点上 [root@master bin]# cp bootstrap.kubeconfig /etc/kubernetes/cfg [root@master bin]# scp /etc/kubernetes/cfg/bootstrap.kubeconfig node1:/etc/kubernetes/cfg [root@master bin]# scp /etc/kubernetes/cfg/bootstrap.kubeconfig node2:/etc/kubernetes/cfg 二、部署kubelet 1.设置CNI支持 [root@node1 ~]# mkdir -p /etc/cni/net.d [root@node1 ~]# vim /etc/cni/net.d/10-default.conf [root@node1 ~]# cat /etc/cni/net.d/10-default.conf { \"name\": \"flannel\", \"type\": \"flannel\", \"delegate\": { \"bridge\": \"docker0\", \"isDefaultGateway\": true, \"mtu\": 1400 } } 2.创建kubelet目录 [root@node1 ~]# mkdir /var/lib/kubelet 3.创建kubelet服务 创建kubelet服务配置 [root@node1 ~]# vim /usr/lib/systemd/system/kubelet.service [root@node1 ~]# cat /usr/lib/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/kubernetes/kubernetes After=docker.service Requires=docker.service [Service] WorkingDirectory=/var/lib/kubelet ExecStart=/etc/kubernetes/bin/kubelet \\ --address=172.18.53.223 \\ --hostname-override=172.18.53.223 \\ --pod-infra-container-image=mirrorgooglecontainers/pause-amd64:3.0 \\ --experimental-bootstrap-kubeconfig=/etc/kubernetes/cfg/bootstrap.kubeconfig \\ --kubeconfig=/etc/kubernetes/cfg/kubelet.kubeconfig \\ --cert-dir=/etc/kubernetes/ssl \\ --network-plugin=cni \\ --cni-conf-dir=/etc/cni/net.d \\ --cni-bin-dir=/etc/kubernetes/bin/cni \\ --cluster-dns=10.1.0.2 \\ --cluster-domain=cluster.local. \\ --hairpin-mode=hairpin-veth \\ --allow-privileged=true \\ --fail-swap-on=false \\ --v=2 \\ --logtostderr=false \\ --log-dir=/etc/kubernetes/log Restart=on-failure RestartSec=5 4.启动kubelet服务 [root@node1 ~]# systemctl daemon-reload [root@node1 ~]# systemctl enable kubelet [root@node1 ~]# systemctl start kubelet 5.查看kubelet服务状态 [root@node1 ~]# systemctl status kubelet ● kubelet.service - Kubernetes Kubelet Loaded: loaded (/usr/lib/systemd/system/kubelet.service; static; vendor preset: disabled) Active: active (running) since Thu 2018-09-20 15:54:03 CST; 4s ago Docs: https://github.com/kubernetes/kubernetes Main PID: 24798 (kubelet) Memory: 14.2M CGroup: /system.slice/kubelet.service └─24798 /etc/kubernetes/bin/kubelet --address=172.18.53.223 --hostname-override=172.18.53.223 --pod-infra-container-image=mirrorgooglecontainers/pause-amd64:3.0 --experimental... Sep 20 15:54:03 node1 systemd[1]: Started Kubernetes Kubelet. Sep 20 15:54:03 node1 systemd[1]: Starting Kubernetes Kubelet... Sep 20 15:54:03 node1 kubelet[24798]: Flag --address has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See http... information. Sep 20 15:54:03 node1 kubelet[24798]: Flag --experimental-bootstrap-kubeconfig has been deprecated, Use --bootstrap-kubeconfig Sep 20 15:54:03 node1 kubelet[24798]: Flag --cluster-dns has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See ... information. Sep 20 15:54:03 node1 kubelet[24798]: Flag --cluster-domain has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. S... information. Sep 20 15:54:03 node1 kubelet[24798]: Flag --hairpin-mode has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See... information. Sep 20 15:54:03 node1 kubelet[24798]: Flag --allow-privileged has been deprecated, will be removed in a future version Sep 20 15:54:03 node1 kubelet[24798]: Flag --fail-swap-on has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See... information. Hint: Some lines were ellipsized, use -l to show in full. 以上步骤从 设置CNI支持开始到 查看kubelet服务状态，需要在node2节点也做一次 6.在master节点查看csr请求 [root@master bin]# kubectl get csr NAME AGE REQUESTOR CONDITION node-csr-2LtbtsOORfUJd6ZGNZyVjmbHO0eyu619JxbRyfXySuA 14s kubelet-bootstrap Pending node-csr-7YxIy4ylo9KNGWDzs-oduDGfQP9kULlaXaL8B1E4X34 16s kubelet-bootstrap Pending 7.在master节点通过kubelet 的 TLS 证书请求 [root@master bin]# kubectl get csr|grep 'Pending' | awk 'NR>0{print $1}'| xargs kubectl certificate approve certificatesigningrequest.certificates.k8s.io/node-csr-2LtbtsOORfUJd6ZGNZyVjmbHO0eyu619JxbRyfXySuA approved certificatesigningrequest.certificates.k8s.io/node-csr-7YxIy4ylo9KNGWDzs-oduDGfQP9kULlaXaL8B1E4X34 approved 再次查询状态已经是approved的了 [root@master bin]# kubectl get csr NAME AGE REQUESTOR CONDITION node-csr-2LtbtsOORfUJd6ZGNZyVjmbHO0eyu619JxbRyfXySuA 58s kubelet-bootstrap Approved,Issued node-csr-7YxIy4ylo9KNGWDzs-oduDGfQP9kULlaXaL8B1E4X34 1m kubelet-bootstrap Approved,Issued 这个时候查询get node也可以看到结果了 [root@master bin]# kubectl get node NAME STATUS ROLES AGE VERSION 172.18.53.223 Ready 19s v1.11.3 172.18.53.224 Ready 19s v1.11.3 三、配置kube-proxy 1.安装kube-proxy使用LVS需要的包 注意：如果不使用lvs的话，就会使用iptables，这个是1.10版本的新特性 yum install -y ipvsadm ipset conntrack 2.创建 kube-proxy 证书请求的json [root@master temp]# cat kube-proxy-csr.json { \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" } ] } 3.生成kube-proxy证书 [root@master temp]# cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/etc/kubernetes/ssl/ca-config.json \\ -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy 2018/09/20 16:08:15 [INFO] generate received request 2018/09/20 16:08:15 [INFO] received CSR 2018/09/20 16:08:15 [INFO] generating key: rsa-2048 2018/09/20 16:08:16 [INFO] encoded CSR 2018/09/20 16:08:16 [INFO] signed certificate with serial number 372779783324557452959035572248079920613862154571 2018/09/20 16:08:16 [WARNING] This certificate lacks a \"hosts\" field. This makes it unsuitable for websites. For more information see the Baseline Requirements for the Issuance and Management of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org); specifically, section 10.2.3 (\"Information Requirements\"). [root@master temp]# ls -l kube-pro*.pem -rw------- 1 root root 1675 Sep 20 16:08 kube-proxy-key.pem -rw-r--r-- 1 root root 1403 Sep 20 16:08 kube-proxy.pem 4.分发证书 [root@master temp]# scp kube-proxy*.pem node1:/etc/kubernetes/ssl/ [root@master temp]# scp kube-proxy*.pem node2:/etc/kubernetes/ssl/ 5.创建kube-proxy配置文件 5.1 创建集群信息 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://172.18.53.221:6443 \\ --kubeconfig=kube-proxy.kubeconfig 5.2 配置认证信息 kubectl config set-credentials kube-proxy \\ --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \\ --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig 5.3设置上下文环境 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig 5.4配置默认的上下文 kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig 5.5分发kube-proxy.kubeconfig到两个node节点 [root@master temp]# cp kube-proxy.kubeconfig /etc/kubernetes/cfg [root@master temp]# scp /etc/kubernetes/cfg/kube-proxy.kubeconfig node1:/etc/kubernetes/cfg/kube-proxy.kubeconfig [root@master temp]# scp /etc/kubernetes/cfg/kube-proxy.kubeconfig node2:/etc/kubernetes/cfg/kube-proxy.kubeconfig 6.创建kube-proxy的工作目录 mkdir /var/lib/kube-proxy 7.创建kube-proxy服务配置 [root@node1 ~]# cat /usr/lib/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube-Proxy Server Documentation=https://github.com/kubernetes/kubernetes After=network.target [Service] WorkingDirectory=/var/lib/kube-proxy ExecStart=/etc/kubernetes/bin/kube-proxy \\ --bind-address=172.18.53.223 \\ --hostname-override=172.18.53.223 \\ --kubeconfig=/etc/kubernetes/cfg/kube-proxy.kubeconfig \\ --masquerade-all \\ --feature-gates=SupportIPVSProxyMode=true \\ --proxy-mode=ipvs \\ --ipvs-min-sync-period=5s \\ --ipvs-sync-period=5s \\ --ipvs-scheduler=rr \\ --logtostderr=true \\ --v=2 \\ --logtostderr=false \\ --log-dir=/etc/kubernetes/log Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target 8.启动kube-proxy服务 [root@node1 ~]# systemctl daemon-reload [root@node1 ~]# systemctl enable kube-proxy Created symlink from /etc/systemd/system/multi-user.target.wants/kube-proxy.service to /usr/lib/systemd/system/kube-proxy.service. [root@node1 ~]# systemctl start kube-proxy 9.查看kube-proxy服务状态 [root@node1 ~]# systemctl status kube-proxy ● kube-proxy.service - Kubernetes Kube-Proxy Server Loaded: loaded (/usr/lib/systemd/system/kube-proxy.service; enabled; vendor preset: disabled) Active: active (running) since Thu 2018-09-20 16:27:38 CST; 11s ago Docs: https://github.com/kubernetes/kubernetes Main PID: 25355 (kube-proxy) Memory: 8.0M CGroup: /system.slice/kube-proxy.service ‣ 25355 /etc/kubernetes/bin/kube-proxy --bind-address=172.18.53.223 --hostname-override=172.18.53.223 --kubeconfig=/etc/kubernetes/cfg/kube-proxy.kubeconfig --masquerade-all -... Sep 20 16:27:38 node1 systemd[1]: Started Kubernetes Kube-Proxy Server. Sep 20 16:27:38 node1 systemd[1]: Starting Kubernetes Kube-Proxy Server... 10.检查LVS状态 [root@node1 ~]# ipvsadm -L -n IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -> RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.1.0.1:443 rr -> 172.18.53.221:6443 Masq 1 0 0 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"install/7-二进制部署-配置flannel网络.html":{"url":"install/7-二进制部署-配置flannel网络.html","title":"7.配置flannel网络","keywords":"","body":" 1.创建flannel的csr请求json 2.生成证书 3.分发证书 4.下载Flannel软件包 5.分发二进制文件 和 脚本到各自节点 6.配置Flannel.cfg 7.分发Flannel.cfg 8.配置Flannel的service文件 9.分发flannel的service配置文件 10.Flannel CNI集成 10.1 配置cni的位置 10.2 创建Etcd的key 10.3 查看服务状态 10.4 配置Docker使用Flannel 11. 将配置复制到节点2 12. 在两个节点分别重启Docker flannel在每个node都要安装！ 1.创建flannel的csr请求json [root@master temp]# cat flanneld-csr.json { \"CN\": \"flanneld\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" } ] } 2.生成证书 [root@master temp]# cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/etc/kubernetes/ssl/ca-config.json \\ -profile=kubernetes flanneld-csr.json | cfssljson -bare flanneld 2018/09/20 16:59:59 [INFO] generate received request 2018/09/20 16:59:59 [INFO] received CSR 2018/09/20 16:59:59 [INFO] generating key: rsa-2048 2018/09/20 16:59:59 [INFO] encoded CSR 2018/09/20 16:59:59 [INFO] signed certificate with serial number 219405115889238011180949290870567268914187527455 2018/09/20 16:59:59 [WARNING] This certificate lacks a \"hosts\" field. This makes it unsuitable for websites. For more information see the Baseline Requirements for the Issuance and Management of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org); specifically, section 10.2.3 (\"Information Requirements\"). [root@master temp]# ll flann* -rw-r--r-- 1 root root 997 Sep 20 16:59 flanneld.csr -rw-r--r-- 1 root root 221 Sep 20 16:40 flanneld-csr.json -rw------- 1 root root 1679 Sep 20 16:59 flanneld-key.pem -rw-r--r-- 1 root root 1391 Sep 20 16:59 flanneld.pem 3.分发证书 [root@master temp]# cp flanneld*.pem .. [root@master temp]# scp flanneld*.pem node1:/etc/kubernetes/ssl/ [root@master temp]# scp flanneld*.pem node2:/etc/kubernetes/ssl/ 4.下载Flannel软件包 [root@master temp]# cd /usr/local/src [root@master src]# tar zxf flannel-v0.10.0-linux-amd64.tar.gz [root@master src]# ls cni-plugins-amd64-v0.7.1.tgz flanneld kubernetes-client-linux-amd64.tar.gz kubernetes.tar.gz etcd-v3.3.9-linux-amd64 flannel-v0.10.0-linux-amd64.tar.gz kubernetes-node-linux-amd64.tar.gz mk-docker-opts.sh etcd-v3.3.9-linux-amd64.tar.gz kubernetes kubernetes-server-linux-amd64.tar.gz README.md 5.分发二进制文件 和 脚本到各自节点 分发 二进制文件 flanneld 脚本mk-docker-opts.sh [root@master src]# cp flanneld mk-docker-opts.sh /etc/kubernetes/bin [root@master src]# scp flanneld mk-docker-opts.sh node1:/etc/kubernetes/bin [root@master src]# scp flanneld mk-docker-opts.sh node2:/etc/kubernetes/bin 分发 脚本remove-docker0.sh [root@master src]# cd /usr/local/src/kubernetes/cluster/centos/node/bin/ [root@master bin]# cp remove-docker0.sh /etc/kubernetes/bin/ [root@master bin]# scp remove-docker0.sh node1:/etc/kubernetes/bin/ [root@master bin]# scp remove-docker0.sh node2:/etc/kubernetes/bin/ 6.配置Flannel.cfg [root@master cfg]# cat flannel.cfg FLANNEL_ETCD=\"-etcd-endpoints=https://172.18.53.221:2379,https://172.18.53.223:2379,https://172.18.53.224:2379\" FLANNEL_ETCD_KEY=\"-etcd-prefix=/kubernetes/network\" FLANNEL_ETCD_CAFILE=\"--etcd-cafile=/etc/kubernetes/ssl/ca.pem\" FLANNEL_ETCD_CERTFILE=\"--etcd-certfile=/etc/kubernetes/ssl/flanneld.pem\" FLANNEL_ETCD_KEYFILE=\"--etcd-keyfile=/etc/kubernetes/ssl/flanneld-key.pem\" 7.分发Flannel.cfg [root@master cfg]# scp flannel.cfg node1:/etc/kubernetes/cfg/ [root@master cfg]# scp flannel.cfg node2:/etc/kubernetes/cfg/ 8.配置Flannel的service文件 [root@master cfg]# cat /usr/lib/systemd/system/flannel.service [Unit] Description=Flanneld overlay address etcd agent After=network.target Before=docker.service [Service] EnvironmentFile=-/etc/kubernetes/cfg/flannel.cfg ExecStartPre=/etc/kubernetes/bin/remove-docker0.sh ExecStart=/etc/kubernetes/bin/flanneld ${FLANNEL_ETCD} ${FLANNEL_ETCD_KEY} ${FLANNEL_ETCD_CAFILE} ${FLANNEL_ETCD_CERTFILE} ${FLANNEL_ETCD_KEYFILE} ExecStartPost=/etc/kubernetes/bin/mk-docker-opts.sh -d /run/flannel/docker Type=notify [Install] WantedBy=multi-user.target RequiredBy=docker.service 9.分发flannel的service配置文件 [root@master cfg]# scp /usr/lib/systemd/system/flannel.service node1:/usr/lib/systemd/system/flannel.service [root@master cfg]# scp /usr/lib/systemd/system/flannel.service node2:/usr/lib/systemd/system/flannel.service 10.Flannel CNI集成 10.1 配置cni的位置 [root@master cfg]# mkdir /etc/kubernetes/bin/cni [root@master cfg]# tar zxf /usr/local/src/cni-plugins-amd64-v0.7.1.tgz -C /etc/kubernetes/bin/cni [root@master cfg]# scp -r /etc/kubernetes/bin/cni/* node1:/etc/kubernetes/bin/cni/ [root@master cfg]# scp -r /etc/kubernetes/bin/cni/* node2:/etc/kubernetes/bin/cni/ 10.2 创建Etcd的key /etc/kubernetes/bin/etcdctl --ca-file /etc/kubernetes/ssl/ca.pem --cert-file /etc/kubernetes/ssl/flanneld.pem --key-file /etc/kubernetes/ssl/flanneld-key.pem \\ --no-sync -C https://172.18.53.221:2379,https://172.18.53.223:2379,https://172.18.53.224:2379 \\ mk /kubernetes/network/config '{ \"Network\": \"10.2.0.0/16\", \"Backend\": { \"Type\": \"vxlan\", \"VNI\": 1 }}' >/dev/null 2>&1 [root@master bin]# systemctl daemon-reload [root@master bin]# systemctl enable flannel [root@master bin]# chmod +x /etc/kubernetes/bin/* [root@master bin]# systemctl start flannel 10.3 查看服务状态 [root@master bin]# systemctl status flannel ● flannel.service - Flanneld overlay address etcd agent Loaded: loaded (/usr/lib/systemd/system/flannel.service; enabled; vendor preset: disabled) Active: active (running) since Thu 2018-09-20 17:34:10 CST; 2min 0s ago Process: 2433 ExecStartPost=/etc/kubernetes/bin/mk-docker-opts.sh -d /run/flannel/docker (code=exited, status=0/SUCCESS) Process: 2417 ExecStartPre=/etc/kubernetes/bin/remove-docker0.sh (code=exited, status=0/SUCCESS) Main PID: 2424 (flanneld) Tasks: 8 Memory: 9.1M CGroup: /system.slice/flannel.service └─2424 /etc/kubernetes/bin/flanneld -etcd-endpoints=https://172.18.53.221:2379,https://172.18.53.223:2379,https://172.18.53.224:2379 -etcd-prefix=/kubernetes/network --etcd-ca... Sep 20 17:34:10 master flanneld[2424]: I0920 17:34:10.771155 2424 main.go:300] Wrote subnet file to /run/flannel/subnet.env Sep 20 17:34:10 master flanneld[2424]: I0920 17:34:10.771166 2424 main.go:304] Running backend. Sep 20 17:34:10 master flanneld[2424]: I0920 17:34:10.771448 2424 vxlan_network.go:60] watching for new subnet leases Sep 20 17:34:10 master flanneld[2424]: I0920 17:34:10.778270 2424 main.go:396] Waiting for 22h59m59.991532601s to renew lease Sep 20 17:34:10 master systemd[1]: Started Flanneld overlay address etcd agent. Sep 20 17:34:10 master flanneld[2424]: I0920 17:34:10.821672 2424 iptables.go:115] Some iptables rules are missing; deleting and recreating rules Sep 20 17:34:10 master flanneld[2424]: I0920 17:34:10.821692 2424 iptables.go:137] Deleting iptables rule: -s 10.2.0.0/16 -j ACCEPT Sep 20 17:34:10 master flanneld[2424]: I0920 17:34:10.822675 2424 iptables.go:137] Deleting iptables rule: -d 10.2.0.0/16 -j ACCEPT Sep 20 17:34:10 master flanneld[2424]: I0920 17:34:10.823622 2424 iptables.go:125] Adding iptables rule: -s 10.2.0.0/16 -j ACCEPT Sep 20 17:34:10 master flanneld[2424]: I0920 17:34:10.825542 2424 iptables.go:125] Adding iptables rule: -d 10.2.0.0/16 -j ACCEPT 10.4 配置Docker使用Flannel [root@node1 cni]# vim /usr/lib/systemd/system/docker.service [Unit] #在Unit下面修改After和增加Requires After=network-online.target firewalld.service flannel.service Wants=network-online.target Requires=flannel.service [Service] #增加EnvironmentFile=-/run/flannel/docker Type=notify EnvironmentFile=-/run/flannel/docker ExecStart=/usr/bin/dockerd $DOCKER_OPTS Requires=flannel.service的意思是需要先把flannle启动之后，docker才能启动。 docker会使用环境变量中的配置文件， [root@master src]# cat /run/flannel/docker DOCKER_OPT_BIP=\"--bip=10.2.58.1/24\" DOCKER_OPT_IPMASQ=\"--ip-masq=true\" DOCKER_OPT_MTU=\"--mtu=1450\" DOCKER_OPTS=\" --bip=10.2.58.1/24 --ip-masq=true --mtu=1450\" 所以docker启动的时候，用命令ExecStart=/usr/bin/dockerd $DOCKER_OPTS就可以使得docker用flannel的网络了(bip) 11. 将配置复制到节点2 root@node1 cni]# scp /usr/lib/systemd/system/docker.service node2:/usr/lib/systemd/system/docker.service 12. 在两个节点分别重启Docker [root@node1 cni]# systemctl daemon-reload [root@node1 cni]# systemctl restart docker Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"install/8-命令行参考.html":{"url":"install/8-命令行参考.html","title":"命令行参考","keywords":"","body":"https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands# Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"install/9-CA的基础概念.html":{"url":"install/9-CA的基础概念.html","title":"CA的基础概念","keywords":"","body":" 一、基本概念 1.CA的基础术语 2.证书的编码格式 3.证书签名请求CSR 4.数字签名 5.数字证书和公钥 二、CFSSL工具 1.CFSSL介绍 2.安装cfssl 3.cfssl工具命令介绍： 4.cfssl常用命令： 一、基本概念 1.CA的基础术语 CA(Certification Authority)证书，指的是权威机构给我们颁发的证书。 密钥就是用来加解密用的文件或者字符串。密钥在非对称加密的领域里，指的是私钥和公钥，他们总是成对出现，其主要作用是加密和解密。常用的加密强度是2048bit。 RSA即非对称加密算法。非对称加密有两个不一样的密码，一个叫私钥，另一个叫公钥，用其中一个加密的数据只能用另一个密码解开，用自己的都解不了，也就是说用公钥加密的数据只能由私钥解开。 2.证书的编码格式 PEM(Privacy Enhanced Mail)，通常用于数字证书认证机构（Certificate Authorities，CA），扩展名为.pem, .crt, .cer, 和 .key。内容为Base64编码的ASCII码文件，有类似\"-----BEGIN CERTIFICATE-----\" 和 \"-----END CERTIFICATE-----\"的头尾标记。服务器认证证书，中级认证证书和私钥都可以储存为PEM格式（认证证书其实就是公钥）。Apache和nginx等类似的服务器使用PEM格式证书。 DER(Distinguished Encoding Rules)，与PEM不同之处在于其使用二进制而不是Base64编码的ASCII。扩展名为.der，但也经常使用.cer用作扩展名，所有类型的认证证书和私钥都可以存储为DER格式。Java使其典型使用平台。 3.证书签名请求CSR CSR(Certificate Signing Request)，它是向CA机构申请数字×××书时使用的请求文件。在生成请求文件前，我们需要准备一对对称密钥。私钥信息自己保存，请求中会附上公钥信息以及国家，城市，域名，Email等信息，CSR中还会附上签名信息。当我们准备好CSR文件后就可以提交给CA机构，等待他们给我们签名，签好名后我们会收到crt文件，即证书。 注意：CSR并不是证书。而是向权威证书颁发机构获得签名证书的申请。 把CSR交给权威证书颁发机构,权威证书颁发机构对此进行签名,完成。保留好CSR,当权威证书颁发机构颁发的证书过期的时候,你还可以用同样的CSR来申请新的证书,key保持不变。 4.数字签名 数字签名就是\"非对称加密+摘要算法\"，其目的不是为了加密，而是用来防止他人篡改数据。 其核心思想是：比如A要给B发送数据，A先用摘要算法得到数据的指纹，然后用A的私钥加密指纹，加密后的指纹就是A的签名，B收到数据和A的签名后，也用同样的摘要算法计算指纹，然后用A公开的公钥解密签名，比较两个指纹，如果相同，说明数据没有被篡改，确实是A发过来的数据。假设C想改A发给B的数据来欺骗B，因为篡改数据后指纹会变，要想跟A的签名里面的指纹一致，就得改签名，但由于没有A的私钥，所以改不了，如果C用自己的私钥生成一个新的签名，B收到数据后用A的公钥根本就解不开。 常用的摘要算法有MD5、SHA1、SHA256。 使用私钥对需要传输的文本的摘要进行加密，得到的密文即被称为该次传输过程的签名。 5.数字证书和公钥 数字证书则是由证书认证机构（CA）对证书申请者真实身份验证之后，用CA的根证书对申请人的一些基本信息以及申请人的公钥进行签名（相当于加盖发证书机 构的公章）后形成的一个数字文件。 实际上，数字证书就是经过CA认证过的公钥，除了公钥，还有其他的信息，比如Email，国家，城市，域名等。 二、CFSSL工具 1.CFSSL介绍 项目地址： https://github.com/cloudflare/cfssl 下载地址： https://pkg.cfssl.org/ 参考链接： https://blog.cloudflare.com/how-to-build-your-own-public-key-infrastructure/ CFSSL是CloudFlare开源的一款PKI/TLS工具。 CFSSL 包含一个命令行工具 和一个用于 签名，验证并且捆绑TLS证书的 HTTP API 服务。 使用Go语言编写。 CFSSL包括： 一组用于生成自定义 TLS PKI 的工具 cfssl程序，是CFSSL的命令行工具 multirootca程序是可以使用多个签名密钥的证书颁发机构服务器 mkbundle程序用于构建证书池 cfssljson程序，从cfssl和multirootca程序获取JSON输出，并将证书，密钥，CSR和bundle写入磁盘 PKI借助数字证书和公钥加密技术提供可信任的网络身份。通常，证书就是一个包含如下身份信息的文件： 证书所有组织的信息 公钥 证书颁发组织的信息 证书颁发组织授予的权限，如证书有效期、适用的主机名、用途等 使用证书颁发组织私钥创建的数字签名 2.安装cfssl wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 chmod +x cfssl_linux-amd64 cfssljson_linux-amd64 cfssl-certinfo_linux-amd64 mv cfssl_linux-amd64 /usr/local/bin/cfssl mv cfssljson_linux-amd64 /usr/local/bin/cfssljson mv cfssl-certinfo_linux-amd64 /usr/bin/cfssl-certinfo 当然也可以将cfssk放在指定的目录，在环境变量中配置好加入PATH就行 3.cfssl工具命令介绍： bundle: 创建包含客户端证书的证书包 genkey: 生成一个key(私钥)和CSR(证书签名请求) scan: 扫描主机问题 revoke: 吊销证书 certinfo: 输出给定证书的证书信息， 跟cfssl-certinfo 工具作用一样 gencrl: 生成新的证书吊销列表 selfsign: 生成一个新的自签名密钥和 签名证书 print-defaults: 打印默认配置，这个默认配置可以用作模板 serve: 启动一个HTTP API服务 gencert: 生成新的key(密钥)和签名证书 -ca：指明ca的证书 -ca-key：指明ca的私钥文件 -config：指明请求证书的json文件 -profile：与-config中的profile对应，是指根据config中的profile段来生成证书的相关信息 ocspdump ocspsign info: 获取有关远程签名者的信息 sign: 签名一个客户端证书，通过给定的CA和CA密钥，和主机名 ocsprefresh ocspserve 4.cfssl常用命令： cfssl gencert -initca ca-csr.json | cfssljson -bare ca ## 初始化ca cfssl gencert -initca -ca-key key.pem ca-csr.json | cfssljson -bare ca ## 使用现有私钥, 重新生成 cfssl certinfo -cert ca.pem cfssl certinfo -csr ca.csr Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"install/10-kubeadm安装k8s集群.html":{"url":"install/10-kubeadm安装k8s集群.html","title":"kubeadm安装k8s集群","keywords":"","body":" kubeadm安装k8s集群 主机准备工作 修改主机名 关闭防火墙 禁用SELINUX 关闭 swap 安装docker 获取repo源(这个地方选用阿里的) 用yum进行安装 安装完成后启动 安装kubelet/kubeadm/kubectl 查看kubeadm init所需要的镜像 初始化master节点 添加节点node kubeadm安装k8s集群 每个节点都要安装kubelet kubeadm 主机准备工作 修改主机名 # cat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 172.26.243.111 k8s-node1 172.26.243.110 k8s-master 关闭防火墙 如果各个主机启用了防火墙，需要开放Kubernetes各个组件所需要的端口，可以查看Installing kubeadm中的”Check required ports”一节。 这里简单起见在各节点禁用防火墙： systemctl stop firewalld.service #停止firewall systemctl disable firewalld.service #禁止firewall开机启动 firewall-cmd --state #查看防火墙状态 禁用SELINUX setenforce 0 vi /etc/selinux/config #SELINUX修改为disabled SELINUX=disabled 关闭 swap swapoff -a 安装docker 获取repo源(这个地方选用阿里的) cd /etc/yum.repos.d/ wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 用yum进行安装 yum install docker-ce -y 安装完成后启动 systemctl enable docker && systemctl start docker 安装kubelet/kubeadm/kubectl kubeadm: the command to bootstrap the cluster. kubelet: the component that runs on all of the machines in your cluster and does things like starting pods and containers. kubectl: the command line util to talk to your cluster. 获取k8s的镜像 cat /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF 安装kubelet/kubeadm/kubectl # yum install -y kubelet kubeadm kubectl Installed: kubeadm.x86_64 0:1.14.1-0 kubectl.x86_64 0:1.14.1-0 kubelet.x86_64 0:1.14.1-0 Dependency Installed: conntrack-tools.x86_64 0:1.4.4-4.el7 cri-tools.x86_64 0:1.12.0-0 kubernetes-cni.x86_64 0:0.7.5-0 libnetfilter_cthelper.x86_64 0:1.0.0-9.el7 libnetfilter_cttimeout.x86_64 0:1.0.0-6.el7 libnetfilter_queue.x86_64 0:1.0.2-2.el7_2 socat.x86_64 0:1.7.3.2-2.el7 启动kubelet systemctl enable kubelet && systemctl start kubelet 查看kubeadm init所需要的镜像 [root@k8s-master ~]# kubeadm config images list I0425 15:55:58.500052 23338 version.go:96] could not fetch a Kubernetes version from the internet: unable to get URL \"https://dl.k8s.io/release/stable-1.txt\": Get https://dl.k8s.io/releas e/stable-1.txt: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)I0425 15:55:58.500133 23338 version.go:97] falling back to the local client version: v1.14.1 k8s.gcr.io/kube-apiserver:v1.14.1 k8s.gcr.io/kube-controller-manager:v1.14.1 k8s.gcr.io/kube-scheduler:v1.14.1 k8s.gcr.io/kube-proxy:v1.14.1 k8s.gcr.io/pause:3.1 k8s.gcr.io/etcd:3.3.10 k8s.gcr.io/coredns:1.3.1 初始化master节点 kubeadm init --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers --pod-network-cidr=10.1.0.0/16 --service-cidr=10.2.0.0/16 --apiserver-advertise-address=172.26.243.110 注意几个选项： 参考： https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/#what-s-next 执行完这一步有提示： Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 172.26.243.110:6443 --token 0i1ato.5kroy56ed2vkr5lx \\ --discovery-token-ca-cert-hash sha256:7f9fda6c143d5cf9034616311581e43f7f1083574be032c759be10e06d783ed3 根据上面的提示创建config文件,并安装网络插件 网络插件可见: https://kubernetes.io/docs/concepts/cluster-administration/addons/ 我在这儿选择的是安装flannel 地址:https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel.yml 可以把这个yaml文件download下来再去执行create # kubectl create -f kube-flannel.yaml podsecuritypolicy.extensions/psp.flannel.unprivileged created clusterrole.rbac.authorization.k8s.io/flannel created clusterrolebinding.rbac.authorization.k8s.io/flannel created serviceaccount/flannel created configmap/kube-flannel-cfg created daemonset.extensions/kube-flannel-ds-amd64 created daemonset.extensions/kube-flannel-ds-arm64 created daemonset.extensions/kube-flannel-ds-arm created daemonset.extensions/kube-flannel-ds-ppc64le created daemonset.extensions/kube-flannel-ds-s390x created 查看下pod的状态 [root@k8s-master ~]# kubectl get pod --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-d5947d4b-c2ptm 0/1 Pending 0 12m kube-system coredns-d5947d4b-v6brp 0/1 Pending 0 12m kube-system etcd-k8s-master 1/1 Running 0 11m kube-system kube-apiserver-k8s-master 1/1 Running 0 11m kube-system kube-controller-manager-k8s-master 1/1 Running 0 11m kube-system kube-flannel-ds-amd64-mqvf6 0/1 Init:0/1 0 3m31s kube-system kube-proxy-mxmhw 1/1 Running 0 12m kube-system kube-scheduler-k8s-master 1/1 Running 0 11m [root@k8s-master ~]# kubectl describe pod kube-flannel-ds-amd64-mqvf6 -n kube-system Name: kube-flannel-ds-amd64-mqvf6 Namespace: kube-system Priority: 0 PriorityClassName: Node: k8s-master/172.26.243.110 Start Time: Thu, 25 Apr 2019 16:04:57 +0800 Labels: app=flannel controller-revision-hash=8676477c4 pod-template-generation=1 tier=node Annotations: Status: Pending IP: 172.26.243.110 Controlled By: DaemonSet/kube-flannel-ds-amd64 Init Containers: install-cni: Container ID: Image: quay.io/coreos/flannel:v0.11.0-amd64 Image ID: Port: Host Port: Command: cp Args: -f /etc/kube-flannel/cni-conf.json /etc/cni/net.d/10-flannel.conflist State: Waiting Reason: PodInitializing Ready: False Restart Count: 0 Environment: Mounts: /etc/cni/net.d from cni (rw) /etc/kube-flannel/ from flannel-cfg (rw) /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-p8ccb (ro) Containers: kube-flannel: Container ID: Image: quay.io/coreos/flannel:v0.11.0-amd64 Image ID: Port: Host Port: Command: /opt/bin/flanneld Args: --ip-masq --kube-subnet-mgr State: Waiting Reason: PodInitializing Ready: False Restart Count: 0 Limits: cpu: 100m memory: 50Mi Requests: cpu: 100m memory: 50Mi Environment: POD_NAME: kube-flannel-ds-amd64-mqvf6 (v1:metadata.name) POD_NAMESPACE: kube-system (v1:metadata.namespace) Mounts: /etc/kube-flannel/ from flannel-cfg (rw) /run/flannel from run (rw) /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-p8ccb (ro) Conditions: Type Status Initialized False Ready False ContainersReady False PodScheduled True Volumes: run: Type: HostPath (bare host directory volume) Path: /run/flannel HostPathType: cni: Type: HostPath (bare host directory volume) Path: /etc/cni/net.d HostPathType: flannel-cfg: Type: ConfigMap (a volume populated by a ConfigMap) Name: kube-flannel-cfg Optional: false flannel-token-p8ccb: Type: Secret (a volume populated by a Secret) SecretName: flannel-token-p8ccb Optional: false QoS Class: Guaranteed Node-Selectors: beta.kubernetes.io/arch=amd64 Tolerations: :NoSchedule node.kubernetes.io/disk-pressure:NoSchedule node.kubernetes.io/memory-pressure:NoSchedule node.kubernetes.io/network-unavailable:NoSchedule node.kubernetes.io/not-ready:NoExecute node.kubernetes.io/pid-pressure:NoSchedule node.kubernetes.io/unreachable:NoExecute node.kubernetes.io/unschedulable:NoSchedule Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 3m46s default-scheduler Successfully assigned kube-system/kube-flannel-ds-amd64-mqvf6 to k8s-master Normal Pulling 3m46s kubelet, k8s-master Pulling image \"quay.io/coreos/flannel:v0.11.0-amd64\" 发现用了的镜像是quay.io/coreos/flannel:v0.11.0-amd64,因为墙的原因,可以考虑先下载这个镜像load到节点去,再来执行创建flannel插件的步骤。 参考: https://kubernetes.io/zh/docs/setup/independent/install-kubeadm/ https://kubernetes.io/docs/setup/independent/install-kubeadm/ 添加节点node 先把flannel插件对应的镜像load到node节点上 [root@k8s-node1 tmp]# docker load ] 4.672MB/4.672MB 5d3f68f6da8f: Loading layer [==================================================>] 9.526MB/9.526MB 9b48060f404d: Loading layer [==================================================>] 5.912MB/5.912MB 3f3a4ce2b719: Loading layer [==================================================>] 35.25MB/35.25MB 9ce0bb155166: Loading layer [==================================================>] 5.12kB/5.12kB Loaded image: quay.io/coreos/flannel:v0.11.0-amd64 [root@k8s-node1 tmp]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE quay.io/coreos/flannel v0.11.0-amd64 ff281650a721 2 months ago 52.6MB 根据init master后的输出来添加节点 [root@k8s-node1 tmp]# kubeadm join 172.26.243.110:6443 --token 0i1ato.5kroy56ed2vkr5lx \\ > --discovery-token-ca-cert-hash sha256:7f9fda6c143d5cf9034616311581e43f7f1083574be032c759be10e06d783ed3 [preflight] Running pre-flight checks [WARNING Service-Docker]: docker service is not enabled, please run 'systemctl enable docker.service' [WARNING IsDockerSystemdCheck]: detected \"cgroupfs\" as the Docker cgroup driver. The recommended driver is \"systemd\". Please follow the guide at https://kubernetes.io/docs/setup/cri / [WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service' [preflight] Reading configuration from the cluster... [preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [kubelet-start] Downloading configuration for the kubelet from the \"kubelet-config-1.14\" ConfigMap in the kube-system namespace [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\" [kubelet-start] Activating the kubelet service [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap... This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run 'kubectl get nodes' on the control-plane to see this node join the cluster. [root@k8s-node1 tmp]# [root@k8s-node1 tmp]# kubectl get nodes The connection to the server localhost:8080 was refused - did you specify the right host or port? [root@k8s-node1 tmp]# [root@k8s-node1 tmp]# [root@k8s-node1 tmp]# mkdir -p $HOME/.kube [root@k8s-node1 tmp]# scp -i k8s-master:/etc/kubernetes/admin.conf $HOME/.kube/config usage: scp [-12346BCpqrv] [-c cipher] [-F ssh_config] [-i identity_file] [-l limit] [-o ssh_option] [-P port] [-S program] [[user@]host1:]file1 ... [[user@]host2:]file2 [root@k8s-node1 tmp]# scp -i k8s-master:/etc/kubernetes/admin.conf $HOME/.kube/config^C [root@k8s-node1 tmp]# [root@k8s-node1 tmp]# scp k8s-master:/etc/kubernetes/admin.conf $HOME/.kube/config The authenticity of host 'k8s-master (172.26.243.110)' can't be established. ECDSA key fingerprint is SHA256:ABQQSCI0KxtjuCL5a3nNRf+bdNmoPLxYXUJVRtlqf3w. ECDSA key fingerprint is MD5:dd:cb:ed:d5:d6:cd:50:ec:c2:bb:65:6f:d7:3f:6f:4a. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added 'k8s-master,172.26.243.110' (ECDSA) to the list of known hosts. root@k8s-master's password: admin.conf 100% 5454 12.1MB/s 00:00 [root@k8s-node1 tmp]# [root@k8s-node1 tmp]# chown $(id -u):$(id -g) $HOME/.kube/config [root@k8s-node1 tmp]# ls Aegis- flannel.tar systemd-private-603a83b0fba9490f8c053d023082ac11-chronyd.service-8KhMeD [root@k8s-node1 tmp]# kubectl get node NAME STATUS ROLES AGE VERSION k8s-master Ready master 25m v1.14.1 k8s-node1 Ready 119s v1.14.1 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"install/11-kubeadm-join的选项含义.html":{"url":"install/11-kubeadm-join的选项含义.html","title":"kubeadm-join的选项含义","keywords":"","body":" 概述 kubeadm join一般有3个选项 选项--token 选项--discovery-token-ca-cert-hash 在忘记token的情况下,node节点加入k8s集群的步骤 概述 在k8s1.14之后,kubeadm安装已经快要成为事实的标准,在之前参加的CKA考试中,发现部署的环境以及是使用kubeadm部署的了，所以对kubeadm的使用也必需要有所了解了！ 在安装完master节点后，最后会出现提示的信息，告知节点怎么添加，如下 kubeadm join 172.26.243.110:6443 --token 5e01an.651vofaq7w7b7h3a \\ --discovery-token-ca-cert-hash sha256:4ab1554087ab674b0ec716a4eafc28211fb8e4d5338ffa33f896e2984e929054 kubeadm join一般有3个选项 172.26.243.110:6443 这个是apiserver的地址，需要按照实际的情况来填 token: 一般为24小时有效的令牌可以使用添加节点的信息(也可以创建永不过期的token) discovery-token-ca-cert-hash : ca证书sha256编码 选项--token token的查看 kubeadm token list token的创建 kubeadm token create 永久token的创建 # kubeadm token create --ttl 0 b9y0pe.t854qjr9kqk8vt6r # kubeadm token list | grep forever b9y0pe.t854qjr9kqk8vt6r authentication,signing system:bootstrappers:kubeadm:default-node-token 选项--discovery-token-ca-cert-hash ca证书sha256编码的获取方式: openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //' 在忘记token的情况下,node节点加入k8s集群的步骤 1.获取apiserver的信息 ${apiserver:port} 2.创建或者获取一个token ${token} 3.获取ca证书的sha256编码 ${token-ca-cert-hash} 4.组装命令kubeadm join ${apiserver:port} --token ${token} --discovery-token-ca-cert-hash ${token-ca-cert-hash} Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"addons/0-kubernetes-addons.html":{"url":"addons/0-kubernetes-addons.html","title":"kubernetes-addons","keywords":"","body":"k8s除了自身的组件外，下面的这些附加组件也非常有用，推荐安装: CoreDNS Dashboard Heapster Ingress Helm harbor Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"addons/2-dashboard.html":{"url":"addons/2-dashboard.html","title":"dashboard","keywords":"","body":"安装 要想使用 Dashboard，首先我们需要安装它，而 Dashboard 的安装其实也很简单。不过对于国内用户需要注意的是需要解决网络问题，或替换镜像地址等。 对于已经解决网络问题的用户： 访问Dashboard # kubectl cluster-info Kubernetes master is running at https://192.168.3.27:6443 CoreDNS is running at https://192.168.3.27:6443/api/v1/namespaces/kube-system/services/coredns:dns/proxy kubernetes-dashboard is running at https://192.168.3.27:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy monitoring-grafana is running at https://192.168.3.27:6443/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. 看这一行: kubernetes-dashboard is running at https://192.168.3.27:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy 用户名:admin 密码：admin 选择Token令牌模式登录。 获取Token kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}') 登录后进入,可以看到: 概况: 集群: workload: 服务发现与负载均衡: 配置与存储: PS:如果要换成英文显示,只需要设置浏览器的默认语言为英文即可 解决网络问题遇到的访问错误 通过浏览器进行dashboard的访问，跳转的时候报错： Error: 'dial tcp 10.2.16.52:8443: getsockopt: connection timed out' 检查集群的相关信息：depoloyment,service,pod,endpoint都是正常的,如下： [root@master ~]# kubectl get deploy,pod,svc,ep -n kube-system -o wide | grep dash deployment.extensions/kubernetes-dashboard 1 1 1 1 46d kubernetes-dashboard mirrorgooglecontainers/kubernetes-dashboard-amd64 :v1.8.3 k8s-app=kubernetes-dashboard pod/kubernetes-dashboard-66c9d98865-wwst6 1/1 Running 0 46d 10.2.16.52 192.168.3.3 service/kubernetes-dashboard NodePort 10.1.232.248 443:34939/TCP 46d k8s-app=kubernetes-dashboard endpoints/kubernetes-dashboard 10.2.16.52:8443 46d 接着检查关键的几个进程，最后发现是因为master节点的flannel没有启动起来，当启动起来后就可以正常进行访问了。同时，kubectl top node 这个命令也可以正常的执行了。 总结dashboard不能访问的排查步骤 1.需要检查apiserver的地址设置的是否正确（重启apiserver和kubenets）,然后就是flannel是否配置启动 2.配置Kubernetes网络，在master和nodes上都需要安装flannel 检查master和node上配置文件是否一致 3.检查iptables -L -n ，检查node节点上的FORWARD 查看转发是否是drop，如果是drop，则开启 4.如果是更高版本的网络，会用ipvs来代替iptables，那么就应该去查看ipvsadm的信息 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"addons/3-harbor.html":{"url":"addons/3-harbor.html","title":"harbor","keywords":"","body":" 一、harbor的安装 1. 下载安装docker-compose 2. 生成harbor的pem相关文件 3.设置docker证书 4. 配置harcfg文件（修改密码，域名，协议[https]等） 5.执行install.sh进行安装 6. 进行验证 二、harbor使用 1.harbor的业务架构 2.api访问的方式 3.harbor的生命周期管理 三、harbor与k8s结合 1.镜像创建的步骤 2.公有仓库的镜像可以直接拉取 3.私有仓库镜的拉取 harbor的地址: https://github.com/vmware/harbor/releases 一、harbor的安装 安装harbor的步骤： 下载安装docker-compose 生成harbor的pem相关文件 设置docker证书 配置harcfg文件（修改密码，域名，协议[https]等） 执行install.sh 进行安装 进行验证 接下来，我将准备把harbor部署到192.168.3.6这个节点上。 1. 下载安装docker-compose pip install docker-compose wget https://github.com/docker/compose/releases/download/1.24.0/docker-compose-Linux-x86_64 chmod a+x docker-compose-Linux-x86_64 mv docker-compose-Linux-x86_64 /etc/kubernetes/bin/docker-compose 2. 生成harbor的pem相关文件 [root@k8s-master temp]# cat harbor-csr.jso { \"CN\": \"harbor\", \"hosts\": [ \"127.0.0.1\", \"192.168.3.6\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" } ] } cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/etc/kubernetes/ssl/ca-config.json \\ -profile=kubernetes harbor-csr.json | cfssljson -bare harbor [root@k8s-master temp]# ls -l ha* -rw-r--r-- 1 root root 1050 Oct 15 14:51 harbor.csr -rw-r--r-- 1 root root 258 Oct 15 14:49 harbor-csr.json -rw------- 1 root root 1679 Oct 15 14:51 harbor-key.pem -rw-r--r-- 1 root root 1419 Oct 15 14:51 harbor.pem 3.设置docker证书 这一步是在配置ssl访问才会有的，把ca.pem放在这个目录中 如果如下目录不存在，请创建，如果有域名请按此格式依次创建 mkdir -p /etc/docker/certs.d/192.168.3.6 # mkdir -p /etc/docker/certs.d/[IP2] # mkdir -p /etc/docker/certs.d/[example1.com] 如果端口为443，则不需要指定。如果为自定义端口，请指定端口 # /etc/docker/certs.d/yourdomain.com:port # 将ca根证书依次复制到上述创建的目录中 cp /etckubernetes/ssl/ca.pem /etc/docker/certs.d/192.168.3.6/ 4. 配置harcfg文件（修改密码，域名，协议[https]等） [root@k8s-master harbor]# pwd /usr/local/src/harbor [root@k8s-master harbor]# ll total 854960 drwxr-xr-x 3 root root 23 Oct 15 14:53 common -rw-r--r-- 1 root root 1185 May 2 23:34 docker-compose.clair.yml -rw-r--r-- 1 root root 1725 May 2 23:34 docker-compose.notary.yml -rw-r--r-- 1 root root 3596 May 2 23:34 docker-compose.yml drwxr-xr-x 3 root root 156 May 2 23:34 ha -rw-r--r-- 1 root root 6687 May 2 23:34 harbor.cfg -rw-r--r-- 1 root root 875401338 May 2 23:36 harbor.v1.5.0.tar.gz -rwxr-xr-x 1 root root 5773 May 2 23:34 install.sh -rw-r--r-- 1 root root 10771 May 2 23:34 LICENSE -rw-r--r-- 1 root root 482 May 2 23:34 NOTICE -rwxr-xr-x 1 root root 27379 May 2 23:34 prepare 修改harbor.cfg中的配置信息，主要是修改密码，域名，协议[https]等 如果要改默认的端口80，那么要改2个地方： harbor.cfg中的hostname=192.168.x.x:8888 docker-compose.yml 对应的ports prepare5.执行install.sh进行安装 [root@k8s-master harbor]# ./install.sh [Step 0]: checking installation environment ... Note: docker version: 18.06.1 Note: docker-compose version: 1.23.0 [Step 1]: loading Harbor images ... ... [Step 2]: preparing environment ... ... [Step 3]: checking existing instance of Harbor ... ... [Step 4]: starting Harbor ... Creating network \"harbor_harbor\" with the default driver Creating harbor-log ... done Creating registry ... done Creating redis ... done Creating harbor-adminserver ... done Creating harbor-db ... done Creating harbor-ui ... done Creating harbor-jobservice ... done Creating nginx ... done ✔ ----Harbor has been installed and started successfully.---- Now you should be able to visit the admin portal at http://192.168.3.6 . For more details, please visit https://github.com/vmware/harbor . 6. 进行验证 首先进行login登录验证 发现有报错，如下： [root@k8s-master mysql]# docker login 192.168.3.6 Username: admin Password: Error response from daemon: Get https://192.168.3.6/v2/: dial tcp 192.168.3 .6:443: connect: connection refused 原因是安装的时候使用的是http协议，所以需要改成https协议 先关闭harbor [root@k8s-master harbor]# docker-compose down -v vi harbor.cfg 修改ui_url_protocol为https #set ui_url_protocol ui_url_protocol = https 修改完后prepare，再启动harbor [root@k8s-master harbor]# vim harbor.cfg [root@k8s-master harbor]# ./prepare [root@k8s-master harbor]# docker-compose up -d Creating network \"harbor_harbor\" with the default driver Creating harbor-log ... done Creating redis ... done Creating harbor-adminserver ... done Creating harbor-db ... done Creating registry ... done Creating harbor-ui ... done Creating nginx ... done Creating harbor-jobservice ... done 再次登录，成功： [root@k8s-master /]# docker login 192.168.3.6 Username: admin Password: WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded 注意： 登录成功后，认证信息自动保存到 ~/.docker/config.json 文件 [root@k8s-master 192.168.3.6]# cat ~/.docker/config.json { \"auths\": { \"192.168.3.6\": { \"auth\": \"YWRtaW46U3RhcioyMDE0\" } }, \"HttpHeaders\": { \"User-Agent\": \"Docker-Client/18.06.1-ce (linux)\" } } 在其他节点同样登录，失败了！！！ [root@k8s-node3 ~]# docker login 192.168.3.6 Authenticating with existing credentials... Login did not succeed, error: Error response from daemon: Get https://192.168.3.6/v2/: x509: certificate signed by unknown authority Username (admin): admin Password: Error response from daemon: Get https://192.168.3.6/v2/: x509: certificate signed by unknown authority 这个应该是CA证书的问题，网上找了找方法没有解决到。 最终还是使用http协议作为非安全的方式进行访问 ：（ ，修改： [root@k8s-node3~]# vim /etc/docker/daemon.json { \"registry-mirrors\": [\"https://re0o947o.mirror.aliyuncs.com\"], \"insecure-registries\":[\"192.168.3.6\"] } 使用ssl的方式留在后面来研究。。。。。。。。。。。。。。。。 通过web端进行登录： 二、harbor使用 1.harbor的业务架构 harbor的业务架构：用户->仓库，其中仓库又分为公共仓库和私有仓库 私有仓库需要登录才能有访问的权限 公共仓库直接可以进行访问 2.api访问的方式 http： curl -u \"admin:Star*2014\" -X GET -H \"Content-Type: application/json\" \"http://192.168.3.6/api/projects/k8spublic\" https： curl -k --cert /etc/kubernetes/ssl/harbor.pem --key /etc/kubernetes/ssl/harbor-key.pem -X GET -H \"Content-Type: application/json\" \"https://192.168.3.6/api/projects/k8spublic\" 3.harbor的生命周期管理 可以使用docker-compose来管理Harbor的生命周期。在harbor的安装目录下，执行相关命令。 docker-compose ps 查看 docker-compose stop 停止 docker-compose start 启动 docker-compose down 删除，利用./install.sh可以重新安装 三、harbor与k8s结合 在docker环境下直接使用,分为三步: docker login docker tag docker push 在k8s中使用harbor分为两种，一种是使用公共仓库的镜像，一种是使用私有仓库的镜像。 1.镜像创建的步骤 创建用户 创建项目 将镜像打标签 docker tag 192.168.3.6/k8spublic/nginx 192.168.3.6/k8sprivate/nginx 将镜像push到仓库中去 [root@k8s-master ~]# docker push 192.168.3.6/k8sprivate/nginx 拉取镜像对于公共仓库和私有仓库就有区别了： 2.公有仓库的镜像可以直接拉取 参考一个yaml文件的例子： apiVersion: extensions/v1beta1 kind: Deployment metadata: labels: run: mynginx name: mynginx namespace: default spec: replicas: 3 selector: matchLabels: run: mynginx template: metadata: labels: run: mynginx spec: containers: - image: 192.168.3.6/k8spublic/nginx imagePullPolicy: Always name: mynginx resources: requests: cpu: 100m terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 可以看到镜像是192.168.3.6/k8spublic/nginx，而这个镜像是属于共有仓库的，所以创建pod的时候可以直接被拉取下来。如果镜像是非共有的，那么创建pod的时候，会报错： [root@k8s-master doc]# kubectl get pod NAME READY STATUS RESTARTS AGE mynginx-7f9dc98fd8-49jlk 0/1 ImagePullBackOff 0 5s mynginx-7f9dc98fd8-lhkqm 0/1 ImagePullBackOff 0 5s mynginx-7f9dc98fd8-pvzjv 0/1 ImagePullBackOff 0 5s troublshooting一下： Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 18s default-scheduler Successfully assigned default/mynginx-7f9dc98fd8-49jlk to k8s-node3 Normal BackOff 12s (x2 over 13s) kubelet, k8s-node3 Back-off pulling image \"192.168.3.6/k8sprivate/nginx\" Warning Failed 12s (x2 over 13s) kubelet, k8s-node3 Error: ImagePullBackOff Normal Pulling 1s (x2 over 15s) kubelet, k8s-node3 pulling image \"192.168.3.6/k8sprivate/nginx\" Warning Failed 1s (x2 over 15s) kubelet, k8s-node3 Failed to pull image \"192.168.3.6/k8sprivate/nginx\": rpc error: code = Unknown desc = Error response from daemon: pull access denied for 192.168.3.6/k8sprivate/nginx, repository does not exist or may require 'docker login' Warning Failed 1s (x2 over 15s) kubelet, k8s-node3 Error: ErrImagePull 会有提示报错: pull access denied for 192.168.3.6/k8sprivate/nginx, repository does not exist or may require 'docker login' Warning Failed,这个说明需要先login 3.私有仓库镜的拉取 所以私有仓库镜像在k8s中的正确的拉取方式是： 3.1 首先获取用户名和密码信息，并以base64的方式进行转码 [root@k8s-master doc]# cat /root/.docker/config.json | base64 -w 0 ewoJImF1dGhzIjogewoJCSIxOTIuMTY4LjMuNiI6IHsKCQkJImF1dGgiOiAiWVdSdGFXNDZVM1JoY2lveU1ERTAiCgkJfQoJfSwKCSJIdHRwSGVhZGVycyI6IHsKCQkiVXNlci1BZ2VudCI6ICJEb2NrZXItQ2xpZW50LzE4LjA2LjEtY2UgKGxpbnV4KSIKCX0KfQ== 3.2 然后将harbor的用户名和密码以secret的方式创建 apiVersion: v1 kind: Secret metadata: name: harborsecret data: .dockerconfigjson: ewoJImF1dGhzIjogewoJCSIxOTIuMTY4LjMuNiI6IHsKCQkJImF1dGgiOiAiWVdSdGFXNDZVM1JoY2lveU1ERTAiCgkJfQoJfSwKCSJIdHRwSGVhZGVycyI6IHsKCQkiVXNlci1BZ2VudCI6ICJEb2NrZXItQ2xpZW50LzE4LjA2LjEtY2UgKGxpbnV4KSIKCX0KfQ== type: kubernetes.io/dockerconfigjson 3.3 在拉取的时候加上imagePullSecrets [root@k8s-master doc]# cat deploy-secret.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: labels: run: mynginx name: mynginx namespace: default spec: replicas: 3 selector: matchLabels: run: mynginx template: metadata: labels: run: mynginx spec: containers: - image: 192.168.3.6/k8spublic/nginx imagePullPolicy: Always name: mynginx resources: requests: cpu: 100m terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 imagePullSecrets: - name: harborsecret 注意最后的imagePullSecrets: - name: harborsecret 特别注意,如果harbor配置的是http模式,那么所有节点的/etc/docker/daemon.json配置中都要把harbor的地址加上去。在docker1.13后，默认的模式https! 否则会报类似的错误: Failed to pull image \"192.168.3.6:8888/private/nfs-client-provisioner:latest\": rpc error: code = Unknown desc = Error response f rom daemon: Get https://192.168.3.6:8888/v2/: http: server gave HTTP response to HTTPS client Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"pod/pod概述.html":{"url":"pod/pod概述.html","title":"pod概述","keywords":"","body":" 理解Pod pod的生命周期 Pod中如何管理多个容器 网络 存储 使用Pod 理解Pod Pod是Kubernetes的里可部署的和管理的最小单元，一个或多个容器构成一个Pod，通常Pod里的容器运行相同的应用。Pod包含的容器都运行在同一个宿主机上，看作一个统一管理单元。 一个Pod代表着集群中运行的一个进程 Pod中封装着应用的容器（也可以是好几个容器），存储、独立的网络IP(一个pod中的多个container共享网络,localhost)，管理容器如何运行的策略选项。 Pod代表着部署的一个单位：kubernetes中应用的一个实例，可能由一个或者多个容器组合在一起共享资源。 每个Pod中都有一个pause容器，pause容器做为Pod的网络接入点，Pod中其他的容器会使用容器映射模式启动并接入到这个pause容器。属于同一个Pod的所有容器共享网络的namespace。 Docker是kubernetes中最常用的容器运行时，但是Pod也支持其他容器运行时,如RKT 在Kubrenetes集群中Pod有如下两种使用方式： 一个Pod中运行一个容器 这种使用方式中最常见的用法，可以把Pod想象成是单个容器的封装，kuberentes管理的是Pod而不是直接管理容器。 在一个Pod中同时运行多个容器 一个Pod中也可以同时封装几个需要紧密耦合互相协作的容器，它们之间共享资源 这些在同一个Pod中的容器可以互相协作成为一个service单位容器共享文件，另一个“sidecar”容器来更新这些文件。Pod将这些容器的存储资源作为一个实体来管理。 同时要理解sidecar这个k8s非常有高度的架构，也是后面istio的基石 另外，和主容器同时存在的我们往往会忽略的一个容器叫init container，这个在后面再具体阐述 pod的生命周期 Pod的生命周期，通过模板定义Pod，然后分配到一个Node上运行，在Pod所包含的容器运行结束后Pod也结束。 在整个过程中，Pod的状态： 挂起 ︰ Pod已被提交到Master，但一个或多个容器镜像尚未创建。包括调度和下载镜像，可能需要一段时间。 运行 ︰ Pod已绑定到的节点，和所有容器镜像已创建完成。至少一个容器是仍在运行，或正在启动或重新启动。 成功 ︰ Pod的所有容器已经成功的终止，并不会重新启动。 失败 ︰ Pod的所有容器已经都终止，至少一个容器已都终止失败 （以非零退出状态退出）。 未知 ︰ 出于某种原因的Pod状态无法获得，通常由于在与主机的Pod通信错误。 Pod中如何管理多个容器 Pod中可以共享两种资源：网络和存储。 网络 每个Pod都会被分配一个唯一的IP地址。Pod中的所有容器共享网络空间，包括IP地址和端口。Pod内部的容器可以使用 localhost 互相通信。Pod中的容器与外界通信时，必须分配共享网络资源（例如使用宿主机的端口映射）。 存储 可以Pod指定多个共享的Volume。Pod中的所有容器都可以访问共享的volume。Volume也可以用来持久化Pod中的存储资源，以防容器重启后文件丢失。 使用Pod 我们很少在k8s集群中直接使用pod, 直接使用pod的这种方式我们又称这种pod叫做静态pod。 静态 Pod 直接由特定节点上的kubelet进程来管理，不通过 master 节点上的apiserver。无法与我们常用的控制器Deployment或者DaemonSet进行关联，它由kubelet进程自己来监控，当pod崩溃时重启该pod，kubelete也无法对他们进行健康检查。静态 pod 始终绑定在某一个kubelet，并且始终运行在同一个节点上。 kubelet会自动为每一个静态 pod 在 Kubernetes 的 apiserver 上创建一个镜像 Pod（Mirror Pod），因此我们可以在 apiserver 中查询到该 pod，但是不能通过 apiserver 进行控制（例如不能删除）。 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"pod/静态pod.html":{"url":"pod/静态pod.html","title":"静态pod","keywords":"","body":"static pod 由kubelet来进行启动，apiserver是不能对其管理的。这样就可以解决一个蛋生鸡还是鸡生蛋的问题，也让组件容器化变为可能。 配置方法 在kubelet的配置文件加上相应的pod-manifest-path=/opt/kubernetes/manifest选项 [root@node1 manifest]# cat /usr/lib/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=docker.service Requires=docker.service [Service] WorkingDirectory=/var/lib/kubelet ExecStart=/opt/kubernetes/bin/kubelet \\ --address=192.168.3.28 \\ --hostname-override=192.168.3.28 \\ --pod-infra-container-image=mirrorgooglecontainers/pause-amd64:3.0 \\ --experimental-bootstrap-kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig \\ --kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig \\ --cert-dir=/opt/kubernetes/ssl \\ --network-plugin=cni \\ --cni-conf-dir=/etc/cni/net.d \\ --cni-bin-dir=/opt/kubernetes/bin/cni \\ --cluster-dns=10.1.0.2 \\ --cluster-domain=cluster.local. \\ --hairpin-mode hairpin-veth \\ --allow-privileged=true \\ --fail-swap-on=false \\ --logtostderr=true \\ --v=2 \\ --log-dir=/opt/kubernetes/log \\ --pod-manifest-path=/opt/kubernetes/manifest Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target [root@node1 manifest]# ps -ef | grep kubelet root 31443 1 14 17:47 ? 00:00:02 /opt/kubernetes/bin/kubelet --address=192.168.3.28 --hostname-override=192.168.3.28 --pod-infra-container-image=mirrorgooglecontainers/pause- amd64:3.0 --experimental-bootstrap-kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig --kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig --cert-dir=/opt/kubernetes/ssl --network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/kubernetes/bin/cni --cluster-dns=10.1.0.2 --cluster-domain=cluster.local. --hairpin-mode hairpin-veth --allow-privileged=true --fail-swap-on=false --logtostderr=true --v=2 --log-dir=/opt/kubernetes/log --pod-manifest-path=/opt/kubernetes/manifest 然后在/opt/kubernetes/manifest加上相应的pod的yaml文件即可 [root@node1 manifest]# ls static-web.yaml [root@node1 manifest]# cat static-web.yaml apiVersion: v1 kind: Pod metadata: name: static-web labels: role: myrole spec: containers: - name: web image: nginx ports: - name: web containerPort: 80 protocol: TCP 过一下,就可以在pod的列表中看到 [root@node1 manifest]# kubectl get pod | grep static static-web-192.168.3.28 1/1 Running 0 17m Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"pod/pod-hook.html":{"url":"pod/pod-hook.html","title":"pod中的钩子","keywords":"","body":"pod中的钩子 参考: https://kubernetes.io/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/ 2种钩子函数: PostStart：这个钩子在容器创建后立即执行。但是，并不能保证钩子将在容器ENTRYPOINT之前运行，因为没有参数传递给处理程序。主要用于资源部署、环境准备等。不过需要注意的是如果钩子花费太长时间以至于不能运行或者挂起， 容器将不能达到running状态。 PreStop：这个钩子在容器终止之前立即被调用。它是阻塞的，意味着它是同步的，所以它必须在删除容器的调用发出之前完成。主要用于优雅关闭应用程序、通知其他系统等。如果钩子在执行期间挂起， Pod阶段将停留在running状态并且永不会达到failed状态。 使用例子： 这个例子在pod启动之前先输出一个语句到/usr/share/message；在关闭pod之前先优雅的干掉nginx apiVersion: v1 kind: Pod metadata: name: lifecycle-demo spec: containers: - name: lifecycle-demo-container image: nginx lifecycle: postStart: exec: command: [\"/bin/sh\", \"-c\", \"echo Hello from the postStart handler > /usr/share/message\"] preStop: exec: command: [\"/bin/sh\",\"-c\",\"nginx -s quit; while killall -0 nginx; do sleep 1; done\"] 下面这个例子是因为退出的时候，容器销毁了，看不到preStop的效果，所以可以输出一个语句到主机上，作为参考： apiVersion: v1 kind: Pod metadata: name: hook-demo2 labels: app: hook spec: containers: - name: hook-demo2 image: nginx ports: - name: webport containerPort: 80 volumeMounts: - name: message mountPath: /usr/share/ lifecycle: preStop: exec: command: ['/bin/sh', '-c', 'echo Hello from the preStop Handler > /usr/share/message'] volumes: - name: message hostPath: path: /tmp 注意：另外钩子调用的日志没有暴露个给 Pod 的 event，所以只能通过describe来获取相关的信息，如果有错误将可以看到FailedPostStartHook或FailedPreStopHook这样的 event Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"pod/init-pod.html":{"url":"pod/init-pod.html","title":"init container","keywords":"","body":" init容器的使用 init container 概述 应用场景 示例 init容器的使用 init container 概述 Init Container就是用来做初始化工作的容器，可以是一个或者多个，如果有多个的话，这些容器会按定义的顺序依次执行，只有所有的Init Container执行完后，主容器才会被启动。我们知道一个Pod里面的所有容器是共享数据卷和网络命名空间的，所以Init Container里面产生的数据可以被主容器使用到的。 init container / main container 的生命周期 从上面这张图可以直观的看到PostStart和PreStop包括liveness和readiness是属于主容器的生命周期范围内的，而Init Container是独立于主容器之外的，当然它们都属于Pod的生命周期范畴之内的。 应用场景 等待其他服务Ready：这个可以用来解决服务之间的依赖问题，比如我们有一个 Web 服务，该服务又依赖于另外一个数据库服务，但是在我们启动这个 Web 服务的时候我们并不能保证依赖的这个数据库服务就已经启动起来了，所以可能会出现一段时间内 Web 服务连接数据库异常。要解决这个问题的话我们就可以在 Web 服务的 Pod 中使用一个 InitContainer，在这个初始化容器中去检查数据库是否已经准备好了，准备好了过后初始化容器就结束退出，然后我们的主容器 Web 服务被启动起来，这个时候去连接数据库就不会有问题了。 做初始化配置：比如集群里检测所有已经存在的成员节点，为主容器准备好集群的配置信息，这样主容器起来后就能用这个配置信息加入集群。 其它场景：如将 pod 注册到一个中央数据库、配置中心等。 示例 apiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp spec: containers: - name: myapp-container image: busybox:1.28 command: ['sh', '-c', 'echo The app is running! && sleep 3600'] initContainers: - name: init-myservice image: busybox:1.28 command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;'] - name: init-mydb image: busybox:1.28 command: ['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;'] kubectl describe -f myapp.yaml Name: myapp-pod Namespace: default [...] Labels: app=myapp Status: Pending [...] Init Containers: init-myservice: [...] State: Running [...] init-mydb: [...] State: Waiting Reason: PodInitializing Ready: False [...] Containers: myapp-container: [...] State: Waiting Reason: PodInitializing Ready: False [...] Events: FirstSeen LastSeen Count From SubObjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 16s 16s 1 {default-scheduler } Normal Scheduled Successfully assigned myapp-pod to 172.17.4.201 16s 16s 1 {kubelet 172.17.4.201} spec.initContainers{init-myservice} Normal Pulling pulling image \"busybox\" 13s 13s 1 {kubelet 172.17.4.201} spec.initContainers{init-myservice} Normal Pulled Successfully pulled image \"busybox\" 13s 13s 1 {kubelet 172.17.4.201} spec.initContainers{init-myservice} Normal Created Created container with docker id 5ced34a04634; Security:[seccomp=unconfined] 13s 13s 1 {kubelet 172.17.4.201} spec.initContainers{init-myservice} Normal Started Started container with docker id 5ced34a04634 示例2: apiVersion: v1 kind: Pod metadata: name: init-demo spec: containers: - name: nginx image: nginx ports: - containerPort: 80 volumeMounts: - name: workdir mountPath: /usr/share/nginx/html initContainers: - name: install image: busybox command: - wget - \"-O\" - \"/work-dir/index.html\" - http://www.baidu.com volumeMounts: - name: workdir mountPath: \"/work-dir\" volumes: - name: workdir emptyDir: {} Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"pod/pod的健康检查.html":{"url":"pod/pod的健康检查.html","title":"pod的健康检查","keywords":"","body":" container中有2种探针 存活探针 - liveness 可读性探针 - readiness 探针的使用方式 参数说明 示例 存活探针示例 可读性探针示例 实际的一个treafik的探针示例 参考: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/ container中有2种探针 liveness:存活探针 readiness:可读性探针 存活探针 - liveness kubelet 通过使用 liveness probe 来确定你的应用程序是否正在运行，通俗点将就是是否还活着。一般来说，如果你的程序一旦崩溃了， Kubernetes 就会立刻知道这个程序已经终止了，然后就会重启这个程序。而我们的 liveness probe 的目的就是来捕获到当前应用程序还没有终止，还没有崩溃，如果出现了这些情况，那么就重启处于该状态下的容器，使应用程序在存在 bug 的情况下依然能够继续运行下去。 可读性探针 - readiness kubelet 使用 readiness probe 来确定容器是否已经就绪可以接收流量过来了。这个探针通俗点讲就是说是否准备好了，现在可以开始工作了。只有当 Pod 中的容器都处于就绪状态的时候 kubelet 才会认定该 Pod 处于就绪状态，因为一个 Pod 下面可能会有多个容器。当然 Pod 如果处于非就绪状态，那么我们就会将他从我们的工作队列(实际上就是我们后面需要重点学习的 Service)中移除出来，这样我们的流量就不会被路由到这个 Pod 里面来了。 探针的使用方式 exec：执行一段命令 http：检测某个 http 请求 tcpSocket：使用此配置， kubelet 将尝试在指定端口上打开容器的套接字。如果可以建立连接，容器被认为是健康的，如果不能就认为是失败的。实际上就是检查端口 参数说明 periodSeconds: kubelet需要每隔多少秒执行一次xxx probe initialDelaySeconds : 指定kubelet在该执行第一次探测之前需要等待的秒数 timeoutSeconds：探测超时时间，默认1秒，最小1秒。 successThreshold：探测失败后，最少连续探测成功多少次才被认定为成功。默认是 1，但是如果是liveness则必须是 1。最小值是 1。 failureThreshold：探测成功后，最少连续探测失败多少次才被认定为失败。默认是 3，最小值是 1。 -示例 存活探针示例 使用执行命令的方式 apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-exec spec: containers: - name: liveness image: k8s.gcr.io/busybox args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5 使用http请求的方式 apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-http spec: containers: - name: liveness image: k8s.gcr.io/liveness args: - /server livenessProbe: httpGet: path: /healthz port: 8080 httpHeaders: - name: Custom-Header value: Awesome initialDelaySeconds: 3 periodSeconds: 3 在k8s.gcr.io/liveness这个镜像中部署了一个go的程序, http.HandleFunc(\"/healthz\", func(w http.ResponseWriter, r *http.Request) { duration := time.Now().Sub(started) if duration.Seconds() > 10 { w.WriteHeader(500) w.Write([]byte(fmt.Sprintf(\"error: %v\", duration.Seconds()))) } else { w.WriteHeader(200) w.Write([]byte(\"ok\")) } }) 在这个例子中,container启动10秒内,执行了/healthz 这个命令后,都是返回的200,对于程序来说,我们认为200-400之间的值都是正常的,而当超过10s后,就返回500了,这个时候存活探针就认为不正常了. 使用tcp socket的方式 apiVersion: v1 kind: Pod metadata: name: goproxy labels: app: goproxy spec: containers: - name: goproxy image: k8s.gcr.io/goproxy:0.1 ports: - containerPort: 8080 readinessProbe: tcpSocket: port: 8080 initialDelaySeconds: 5 periodSeconds: 10 livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 15 periodSeconds: 20 TCP 检查的配置与 HTTP 检查非常相似，只是将httpGet替换成了tcpSocket。 该探针会去连接容器的8080端，如果连接成功，则该 Pod 将被标记为就绪状态。然后Kubelet将每隔10秒钟执行一次该检查。 可读性探针示例 从上面的YAML文件可以看出readiness probe的配置跟liveness probe很像，基本上一致的。唯一的不同是使用readinessProbe而不是livenessProbe。两者如果同时使用的话就可以确保流量不会到达还未准备好的容器，准备好过后，如果应用程序出现了错误，则会重新启动容器。 实际的一个treafik的探针示例 test的deployment apiVersion: v1 kind: Deployment apiVersion: extensions/v1beta1 metadata: name: test namespace: default labels: test: alpine spec: replicas: 1 selector: matchLabels: test: alpine template: metadata: labels: test: alpine name: test spec: containers: - image: mritd/alpine:3.4 name: alpine resources: limits: cpu: 200m memory: 30Mi requests: cpu: 100m memory: 20Mi ports: - name: http containerPort: 80 args: command: - \"bash\" - \"-c\" - \"echo ok > /tmp/health;sleep 120;rm -f /tmp/health\" livenessProbe: exec: command: - cat - /tmp/health initialDelaySeconds: 20 test的svc apiVersion: v1 kind: Service metadata: name: test labels: name: test spec: ports: - port: 8123 targetPort: 80 selector: name: test test的ingress apiVersion: extensions/v1beta1 kind: Ingress metadata: name: test spec: rules: - host: test.com http: paths: - path: / backend: serviceName: test servicePort: 8123 全部创建好以后，进入 traefik ui 界面，可以观察到每隔 2 分钟健康检查失败后，kubernetes 重建 pod，同时 traefik 会从后端列表中移除这个 pod 其他更多请参考官方文档 https://docs.traefik.io/ Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"pod/pod中容器的启动顺序.html":{"url":"pod/pod中容器的启动顺序.html","title":"pod的启动顺序","keywords":"","body":" 经典图来诠释 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"pod/k8s中的limits.html":{"url":"pod/k8s中的limits.html","title":"k8s中的LimitRange","keywords":"","body":"k8s中的 LimitRange LimitRange(简称limits)基于namespace的资源管理，包括pod和container的最小、最大和default、defaultrequests等。 一旦创建limits，以后创建资源时，K8S将该limits资源限制条件默认/强制给pod，创建后发现不符合规则，将暂停创建pod。 在创建资源时，用户可以为pod自定义资源管理限制，在创建时会去检查和匹配limits值，发现不匹配将在创建时报错。创建后，该pod的资源使用遵守自定义规则，而不会遵守namespace的limits限制。 例子 apiVersion: v1 kind: LimitRange metadata: namespace: default name: qmxyfzx-limitrange labels: project: qmxyfzx app: limitrange version: v1 spec: limits: - max: cpu: 1 memory: 1Gi min: cpu: 0.05 memory: 64Mi type: Pod #注意pod只能这么多参数 - default: cpu: 0.2 memory: 200Mi defaultRequest: cpu: 0.01 memory: 16Mi max: cpu: 0.25 memory: 256Mi min: cpu: 0.005 memory: 8Mi #container只能这么多参数 type: Container 参数说明 pod可以有的参数 max：表示pod中所有容器资源的Limit值和的上限，也就是整个pod资源的最大Limit，如果pod定义中的Limit值大于LimitRange中的值，则pod无法成功创建。 min：表示pod中所有容器资源请求总和的下限，也就是所有容器request的资源总和不能小于min中的值，否则pod无法成功创建。 maxLimitRequestRatio：表示pod中所有容器资源请求的Limit值和request值比值的上限，例如该pod中cpu的Limit值为3，而request为0.5，此时比值为6，创建pod将会失败。 container可以有的参数 在container的部分，max、min和maxLimitRequestRatio的含义和pod中的类似，只不过是针对单个的容器而言。下面说明几个情况：如果container设置了max， pod中的容器必须设置limit，如果未设置，则使用defaultlimt的值，如果defaultlimit也没有设置，则无法成功创建 如果设置了container的min，创建容器的时候必须设置request的值，如果没有设置，则使用defaultrequest，如果没有defaultrequest，则默认等于容器的limit值，如果limit也没有，启动就会报错 defaultlimit ：默认时，限制使用的资源是：CPU 0.2个核，内存200M defaultRequest：默认时，最低保证可以使用的资源是： CPU 0.01个核，内存16M 使用规则 1、每个namespace应该只有一个limits； 2、limits值设置： 每容器（type： container） max>=default>=defaultRequest>min 每pod（type： pod） max>=min 整个 容器的max*容器数 3、创建资源时，pod自定义资源限制的规则：自定义的单个request>=limits的容器的defaultrequets 自定义的request的总和>=limits的pod的min 自定义的单个limit 使用建议 只使用limits的pod或者container中的一种，尽量不使用同时使用，特别在pod中有多容器需求的情况下。 尽量使用max，尽量不同时使用max和min 由于limits会针对该namespace下的所有新建的pods，所以在该namespace下应该运行哪些资源需求相同的业务 在复杂的limits配置下，不要在创建资源时使用自定义配置。 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"pod/k8s中的resource-quotas.html":{"url":"pod/k8s中的resource-quotas.html","title":"k8s中的resource-quotas","keywords":"","body":"在一个多用户、多团队的k8s集群上，通常会遇到一个问题，如何在不同团队之间取得资源的公平，即，不会因为某个流氓团队占据了所有资源，从而导致其他团队无法使用k8s。 k8s的解决方法是，通过RBAC将不同团队（or 项目）限制在不同的namespace下，通过resourceQuota来限制该namespace能够使用的资源。资源分为以下三种。 计算资源配额：cpu，memory 存储资源配置：requests.storage（真～总量），pvc，某storage class下的限制（例如针对fast类型的sc的限制） 对象数量配置：cm，service，pod的个数。 官方文档请见下面的Ref: https://kubernetes.io/docs/tasks/administer-cluster/quota-memory-cpu-namespace/#clean-up 下面举个例子: 首先，集群管理员需要为团队创建一个namespace kubectl create namespace yfzx 然后，将下面的quota apply到该namespace。 apiVersion: v1 kind: ResourceQuota metadata: name: compute-resources spec: hard: limits.cpu: \"16\" limits.memory: 8Gi persistentvolumeclaims: \"8\" pods: \"16\" requests.cpu: \"8\" requests.memory: 4Gi requests.storage: 30Gi 创建pod时指定相应的requests apiVersion: v1 kind: Pod metadata: name: quota-mem-cpu-demo spec: containers: - name: quota-mem-cpu-demo-ctr image: nginx resources: limits: memory: \"16Gi\" cpu: \"800m\" requests: memory: \"4Gi\" cpu: \"500m\" 注意，当开启了resource quota时，用户创建pod，必须指定cpu、内存的 requests or limits ，否则创建失败。resourceQuota搭配 limitRanges口感更佳：limitRange可以配置创建Pod的默认limit/request。 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"pod/容器的资源使用限制.html":{"url":"pod/容器的资源使用限制.html","title":"pod的资源请求与限制","keywords":"","body":"什么是pod的资源请求与资源限制？ 当创建一个Pod的时候，需要为运行在Pod中的容器请求CPU和RAM资源，还可以设置CPU和RAM资源的限制。 请求CPU和RAM资源，在配置文件里面使用resources:rquests字段 设置CPU和RAM限制，在配置文件里面使用resources:limits字段 单位说明 CPU资源以cpus为单位。允许小数值,可以用后缀m来表示mili。例如100m cpu等同于100 milicpu，意思是0.1cpu。 RAM资源以bytes为单位。可以将RAM表示为纯整数或具有这些后缀之一的定点整数： E, P, T, G, M, K, Ei, Pi, Ti, Gi, Mi, Ki。 申请资源：表示容器使用的最小CPU/内存的值，即工作负载能够运行起来工作所需要的最小资源 （申请资源过多，会占用集群过多资源，导致资源不足，工作负载部署不起来） 限制资源：表示容器使用的最大CPU/内存的值，即工作负载处理最大业务流量所需要的最大资源（申请资源过少，会导致工作负载运行中因为资源不足而异常） 使用说明 如果节点上具有足够的CPU和RAM资源可用于所有容器要求的CPU和RAM总和，k8s将把Pod调度在上面。 同样当容器运行在节点上时，k8s 不允许容器消耗的CPU和RAM资源超出指定的容器的限制。 如果容器超出RAM限制，pod将结束。如果CPU超出限制，它将成为CPU节流的候选者。 例子： Pod请求250mili cpu和64 mebibytes RAM，同时设置上线为1cpu和128 mebibytes RAM apiVersion: v1 kind: Pod metadata: name: cpu-ram-demo spec: containers: - name: cpu-ram-demo-container image: gcr.io/google-samples/node-hello:1.0 resources: requests: memory: \"64Mi\" cpu: \"250m\" limits: memory: \"128Mi\" cpu: \"1\" 是否必须设置工作负载的值？ 必须设置。 如果不设置，会带来如下影响： 工作负载的监控数据不准确； 会因为未配置资源限制的工作负载（内存泄漏等）占用太多的资源，而导致其他工作负载使用不到资源或者节点资源耗尽而异常 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"pod/pod的HPA特性.html":{"url":"pod/pod的HPA特性.html","title":"pod的HPA特性","keywords":"","body":" 一、压测工具ab介绍及安装 1.介绍 2.安装 3.ab的使用 4.示例： 二、hpa介绍 三、基础测试环境配置 1.首先在k8s中安装heapster 2.创建deployment 3.创建服务，并暴露服务 4.访问测试一下 四、对CPU限制的HPA演示 1.创建hpa 2.使用ab对pod进行压力测试 3.继续监控hpa 4.当测试完成后 一、压测工具ab介绍及安装 1.介绍 ab(apache benchmark) —— apache自带的一个测试工具，一般把apache压力测试称为AB测试。 ab is a tool for benchmarking your Apache Hypertext Transfer Protocol (HTTP) server. It is designed to give you an impression of how your current Apache installation performs. This especially shows you how many requests per second your Apache installation is capable of serving. 文档地址：http://httpd.apache.org/docs/2.4/programs/ab.html 2.安装 [root@k8s-master ~]# yum -y install httpd-tools [root@k8s-master ~]# ab -V This is ApacheBench, Version 2.3 Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ 3.ab的使用 参数说明： Usage: ab [options] [http[s]://]hostname[:port]/path Options are: -n requests Number of requests to perform -c concurrency Number of multiple requests to make at a time -t timelimit Seconds to max. to spend on benchmarking This implies -n 50000 -s timeout Seconds to max. wait for each response Default is 30 seconds -b windowsize Size of TCP send/receive buffer, in bytes -B address Address to bind to when making outgoing connections -p postfile File containing data to POST. Remember also to set -T -u putfile File containing data to PUT. Remember also to set -T -T content-type Content-type header to use for POST/PUT data, eg. 'application/x-www-form-urlencoded' Default is 'text/plain' -v verbosity How much troubleshooting info to print -w Print out results in HTML tables -i Use HEAD instead of GET -x attributes String to insert as table attributes -y attributes String to insert as tr attributes -z attributes String to insert as td or th attributes -C attribute Add cookie, eg. 'Apache=1234'. (repeatable) -H attribute Add Arbitrary header line, eg. 'Accept-Encoding: gzip' Inserted after all normal header lines. (repeatable) -A attribute Add Basic WWW Authentication, the attributes are a colon separated username and password. -P attribute Add Basic Proxy Authentication, the attributes are a colon separated username and password. -X proxy:port Proxyserver and port number to use -V Print version number and exit -k Use HTTP KeepAlive feature -d Do not show percentiles served table. -S Do not show confidence estimators and warnings. -q Do not show progress when doing more than 150 requests -g filename Output collected data to gnuplot format file. -e filename Output CSV file with percentages served -r Don't exit on socket receive errors. -h Display usage information (this message) -Z ciphersuite Specify SSL/TLS cipher suite (See openssl ciphers) -f protocol Specify SSL/TLS protocol (SSL3, TLS1, TLS1.1, TLS1.2 or ALL) 常用参数 -n ：总共的请求执行数，缺省是1； -c： 并发数，缺省是1； -t：测试所进行的总时间，秒为单位，缺省50000s -p：POST时的数据文件 -w: 以HTML表的格式输出结果 4.示例： ab -n 1000 -c 100 -w http://localhost/index.php >> /tmp/1.html 上面的测试用例表示100并发的情况下，共测试访问index.php脚本1000次，并将测试结果保存到/tmp/1.html文件中 分析测试结果，可知在该100并发访问的情况下，共测试访问1000次，失败了xxx次。进而可知该脚本在此环境是否可以满足100并发访问的要求，然后通过调节并发数并重新检查失败次数，可得出在多少并发下系统可正常工作。 结果详解： Server Software: Apache/2.4.18 服务器软件版本 Server Hostname: www.06a.com 请求的URL Server Port: 80 请求的端口号 Document Path: / 请求的服务器的路径 Document Length: 19590 bytes 页面长度 单位是字节 Concurrency Level: 200 并发数 Time taken for tests: 124.509 seconds 一共使用了124s Complete requests: 1000 请求的次数 Failed requests: 9 失败的请求 (Connect: 0, Receive: 0, Length: 9, Exceptions: 0) Total transferred: 19669661 bytes 总共传输的字节数 http头信息 HTML transferred: 19472463 bytes 实际页面传递的字节数 Requests per second: 8.03 [#/sec] (mean) 每秒多少个请求 Time per request: 24901.805 [ms] (mean) 平均每个用户等待多长时间 Time per request: 124.509 [ms] (mean, across all concurrent requests) 服务器平均用多长时间处理 Transfer rate: 154.28 [Kbytes/sec] received 每秒获取多少数据 Connection Times (ms) min mean[+/-sd] median max Connect: 0 1 3.0 0 62 Processing: 4679 17276 7877.7 15587 64050 Waiting: 4675 17273 7877.1 15586 64050 Total: 4679 17277 7877.8 15588 64051 Percentage of the requests served within a certain time (ms) 50% 15588 50%的用户的请求15588ms内返回 66% 21097 75% 24071 80% 25294 90% 27939 95% 29550 98% 32122 99% 34885 100% 64051 (longest request) 二、hpa介绍 HPA全称Horizontal Pod Autoscaling，即pod的水平自动扩展。 自动扩展主要分为两种，其一为水平扩展，针对于实例数目的增减；其二为垂直扩展，即单个实例可以使用的资源的增减。HPA属于前者。 HPA是Kubernetes中实现POD水平自动伸缩的功能。 云计算具有水平弹性的特性，这个是云计算区别于传统IT技术架构的主要特性。对于Kubernetes中的POD集群来说，HPA可以实现很多自动化功能，比如当POD中业务负载上升的时候，可以创建新的POD来保证业务系统稳定运行，当POD中业务负载下降的时候，可以销毁POD来提高资源利用率。 HPA控制器默认每隔30秒就会运行一次。 如果要修改间隔时间，可以设置horizontal-pod-autoscaler-sync-period参数。 HPA的操作对象是RC、RS或Deployment对应的Pod 根据观察到的CPU等实际使用量与用户的期望值进行比对，做出是否需要增减实例数量的决策。 hpa的发展历程 在Kubernetes v1.1中首次引入了hpa特性。hpa第一个版本基于观察到的CPU利用率，后续版本支持基于内存使用。 在Kubernetes 1.6中引入了一个新的API自定义指标API，它允许HPA访问任意指标。 Kubernetes 1.7引入了聚合层，允许第三方应用程序通过注册为API附加组件来扩展Kubernetes API。自定义指标API以及聚合层使得像Prometheus这样的监控系统可以向HPA控制器公开特定于应用程序的指标。 HPA原理如下图： 三、基础测试环境配置 1.首先在k8s中安装heapster 2.创建deployment [root@k8s-master ~]# kubectl run nginx --image=nginx --requests=cpu=100m deployment.apps/nginx created [root@k8s-master ~]# kubectl get pod [root@k8s-master ~]# kubectl get pod -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE nginx-5bc844797f-htssr 1/1 Running 0 57s 10.2.52.5 k8s-node3 3.创建服务，并暴露服务 [root@k8s-master ~]# kubectl expose deployment nginx --target-port=80 --port=80 --type=NodePort service/nginx exposed [root@k8s-master ~]# kubectl get svc,pod | grep nginx service/nginx NodePort 10.1.73.58 80:33625/TCP 7s pod/nginx-5bc844797f-htssr 1/1 Running 0 1m 4.访问测试一下 [root@k8s-master ~]# curl k8s-node3:33625 Welcome to nginx! body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } Welcome to nginx! If you see this page, the nginx web server is successfully installed and working. Further configuration is required. For online documentation and support please refer to nginx.org. Commercial support is available at nginx.com. Thank you for using nginx. 四、对CPU限制的HPA演示 1.创建hpa 设置最小replicas为1，最大为10，设置cpu的使用率不超过10% [root@k8s-master ~]# kubectl autoscale deployment nginx --max=10 --min=1 --cpu-percent=10 horizontalpodautoscaler.autoscaling/nginx autoscaled [root@k8s-master ~]# kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE nginx Deployment/nginx /10% 1 10 0 4s 注意TARGETS这一列的值为 /10%，troubleshooting一下 [root@k8s-master heapster]# kubectl describe hpa nginx Name: nginx Namespace: default Labels: Annotations: CreationTimestamp: Wed, 10 Oct 2018 10:12:15 +0800 Reference: Deployment/nginx Metrics: ( current / target ) resource cpu on pods (as a percentage of request): 0% (0) / 10% Min replicas: 1 Max replicas: 10 Deployment pods: 1 current / 1 desired Conditions: Type Status Reason Message ---- ------ ------ ------- AbleToScale True ReadyForNewScale the last scale time was sufficiently old as to warrant a new scale ScalingActive True ValidMetricFound the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request) ScalingLimited True TooFewReplicas the desired replica count is increasing faster than the maximum scale rate Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedComputeMetricsReplicas 29m (x13 over 35m) horizontal-pod-autoscaler failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from r esource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io) Warning FailedGetResourceMetric 6m (x61 over 35m) horizontal-pod-autoscaler unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io) 参考：https://github.com/kubernetes/kubernetes/issues/57673 这个是因为controller没有加上--horizontal-pod-autoscaler-use-rest-clients=false导致的，给kube-controller-manager.service加上这一个选项后，重启启动apiserver 再次查询hpa，显示正常了： [root@k8s-master ~]# kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE nginx Deployment/nginx 0%/10% 1 10 1 6h 2.使用ab对pod进行压力测试 [root@k8s-master ~]# ab -c 1000 -n 1000000 http://192.168.3.22:33625/ This is ApacheBench, Version 2.3 Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking 192.168.3.22 (be patient) ... ... 3.继续监控hpa 可以看到，当cpu的压力超过10%后，每过3分钟replicas会进行一次扩展来分担压力 [root@k8s-master ~]# kubectl get hpa --watch NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE nginx Deployment/nginx 0%/10% 1 10 1 6h nginx Deployment/nginx 30%/10% 1 10 1 6h nginx Deployment/nginx 30%/10% 1 10 3 6h nginx Deployment/nginx 70%/10% 1 10 3 6h 4.当测试完成后 [root@k8s-master ~]# ab -c 1000 -n 1000000 http://192.168.3.22:33625/ This is ApacheBench, Version 2.3 Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking 192.168.3.22 (be patient) Completed 100000 requests Completed 200000 requests Completed 300000 requests Completed 400000 requests Completed 500000 requests Completed 600000 requests Completed 700000 requests Completed 800000 requests Completed 900000 requests Completed 1000000 requests Finished 1000000 requests Server Software: nginx/1.15.5 Server Hostname: 192.168.3.22 Server Port: 33625 Document Path: / Document Length: 612 bytes Concurrency Level: 1000 Time taken for tests: 992.860 seconds Complete requests: 1000000 Failed requests: 0 Write errors: 0 Total transferred: 845000000 bytes HTML transferred: 612000000 bytes Requests per second: 1007.19 [#/sec] (mean) Time per request: 992.860 [ms] (mean) Time per request: 0.993 [ms] (mean, across all concurrent requests) Transfer rate: 831.13 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 1 990 111.4 1003 3010 Processing: 0 3 23.5 1 2883 Waiting: 0 2 23.3 1 2883 Total: 1 992 104.8 1004 4321 Percentage of the requests served within a certain time (ms) 50% 1004 66% 1004 75% 1005 80% 1005 90% 1005 95% 1006 98% 1006 99% 1008 100% 4321 (longest request) HPA的变化： 可以看到，当cpu的压力超过10%后，每过3分钟replicas会进行一次扩展来分担压力。 在ab测试的过程，replicas已经被扩到最大的10个，但是cpu的使用率还是超过了10%，所以replicas的数量没有降下去 当ab测试完成后，cpu的使用率开始下降，这个时候replcas的数量也慢慢从10降为7，最后降为1。[root@k8s-master ~]# kubectl get hpa --watch NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE nginx Deployment/nginx 0%/10% 1 10 1 6h nginx Deployment/nginx 30%/10% 1 10 1 6h nginx Deployment/nginx 30%/10% 1 10 3 6h nginx Deployment/nginx 70%/10% 1 10 3 6h nginx Deployment/nginx 70%/10% 1 10 3 6h nginx Deployment/nginx 40%/10% 1 10 3 6h nginx Deployment/nginx 40%/10% 1 10 3 6h nginx Deployment/nginx 40%/10% 1 10 3 6h nginx Deployment/nginx 40%/10% 1 10 6 6h nginx Deployment/nginx 32%/10% 1 10 6 6h nginx Deployment/nginx 32%/10% 1 10 6 6h nginx Deployment/nginx 22%/10% 1 10 6 6h nginx Deployment/nginx 22%/10% 1 10 6 6h nginx Deployment/nginx 22%/10% 1 10 6 6h nginx Deployment/nginx 22%/10% 1 10 10 6h nginx Deployment/nginx 19%/10% 1 10 10 6h nginx Deployment/nginx 19%/10% 1 10 10 6h nginx Deployment/nginx 14%/10% 1 10 10 6h nginx Deployment/nginx 14%/10% 1 10 10 6h nginx Deployment/nginx 14%/10% 1 10 10 6h nginx Deployment/nginx 14%/10% 1 10 10 6h nginx Deployment/nginx 14%/10% 1 10 10 6h nginx Deployment/nginx 7%/10% 1 10 10 6h nginx Deployment/nginx 7%/10% 1 10 7 6h nginx Deployment/nginx 0%/10% 1 10 7 6h nginx Deployment/nginx 0%/10% 1 10 7 6h nginx Deployment/nginx 0%/10% 1 10 7 6h nginx Deployment/nginx 0%/10% 1 10 7 6h nginx Deployment/nginx 0%/10% 1 10 7 6h nginx Deployment/nginx 0%/10% 1 10 7 6h nginx Deployment/nginx 0%/10% 1 10 7 6h nginx Deployment/nginx 0%/10% 1 10 1 6h nginx Deployment/nginx 0%/10% 1 10 1 6h Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"pod/容器安全.html":{"url":"pod/容器安全.html","title":"容器安全","keywords":"","body":" 容器安全 Security Context 安全上下文 Container-level Security Context Pod-level Security Context Pod Security Policies（PSP） SELinux Sysctls AppArmor Seccomp 参考文献 容器安全 Kubernetes提供了多种机制来限制容器的行为，减少容器攻击面，保证系统安全性。 Security Context：限制容器的行为，包括Capabilities、ReadOnlyRootFilesystem、Privileged、RunAsNonRoot、RunAsUser以及SELinuxOptions等 Pod Security Policy：集群级的Pod安全策略，自动为集群内的Pod和Volume设置Security Context Sysctls：允许容器设置内核参数，分为安全Sysctls和非安全Sysctls AppArmor：限制应用的访问权限 Seccomp：Secure computing mode的缩写，限制容器应用可执行的系统调用 Security Context 安全上下文 Security Context的目的是限制不可信容器的行为，保护系统和其他容器不受其影响。 Kubernetes提供了三种配置Security Context的方法： Container-level Security Context：仅应用到指定的容器 Pod-level Security Context：应用到Pod内所有容器以及Volume Pod Security Policies（PSP）：应用到集群内部所有Pod以及Volume Container-level Security Context Container-level Security Context仅应用到指定的容器上，并且不会影响Volume。比如设置容器运行在特权模式： apiVersion: v1 kind: Pod metadata: name: hello-world spec: containers: - name: hello-world-container # The container definition # ... securityContext: privileged: true Pod-level Security Context Pod-level Security Context应用到Pod内所有容器，并且还会影响Volume（包括fsGroup和selinuxOptions）。 apiVersion: v1 kind: Pod metadata: name: hello-world spec: containers: # specification of the pod's containers # ... securityContext: fsGroup: 1234 supplementalGroups: [5678] seLinuxOptions: level: \"s0:c123,c456\" Pod Security Policies（PSP） Pod Security Policies（PSP）是集群级的Pod安全策略，自动为集群内的Pod和Volume设置Security Context。 使用PSP需要API Server开启extensions/v1beta1/podsecuritypolicy，并且配置PodSecurityPolicy admission控制器。 支持的控制项 控制项 | 说明 privileged | 运行特权容器 defaultAddCapabilities | 可添加到容器的Capabilities requiredDropCapabilities |会从容器中删除的Capabilities volumes| 控制容器可以使用哪些volume hostNetwork| host网络 hostPorts |允许的host端口列表 hostPID| 使用host PID namespace hostIPC| 使用host IPC namespace seLinux | SELinux Context runAsUser| user ID supplementalGroups |允许的补充用户组 fsGroup| volume FSGroup readOnlyRootFilesystem |只读根文件系统 示例 限制容器的host端口范围为8000-8080： apiVersion: extensions/v1beta1 kind: PodSecurityPolicy metadata: name: permissive spec: seLinux: rule: RunAsAny supplementalGroups: rule: RunAsAny runAsUser: rule: RunAsAny fsGroup: rule: RunAsAny hostPorts: - min: 8000 max: 8080 volumes: - '*' SELinux SELinux (Security-Enhanced Linux) 是一种强制访问控制（mandatory access control）的实现。它的作法是以最小权限原则（principle of least privilege）为基础，在Linux核心中使用Linux安全模块（Linux Security Modules）。SELinux主要由美国国家安全局开发，并于2000年12月22日发行给开放源代码的开发社区。 可以通过runcon来为进程设置安全策略，ls和ps的-Z参数可以查看文件或进程的安全策略。 开启与关闭SELinux 修改/etc/selinux/config文件方法： 开启：SELINUX=enforcing 关闭：SELINUX=disabled 通过命令临时修改： 开启：setenforce 1 关闭：setenforce 0 查询SELinux状态：getenforce 示例 apiVersion: v1 kind: Pod metadata: name: hello-world spec: containers: - image: gcr.io/google_containers/busybox:1.24 name: test-container command: - sleep - \"6000\" volumeMounts: - mountPath: /mounted_volume name: test-volume restartPolicy: Never hostPID: false hostIPC: false securityContext: seLinuxOptions: level: \"s0:c2,c3\" volumes: - name: test-volume emptyDir: {} 这会自动给docker容器生成如下的HostConfig.Binds: /var/lib/kubelet/pods/f734678c-95de-11e6-89b0-42010a8c0002/volumes/kubernetes.io~empty-dir/test-volume:/mounted_volume:Z /var/lib/kubelet/pods/f734678c-95de-11e6-89b0-42010a8c0002/volumes/kubernetes.io~secret/default-token-88xxa:/var/run/secrets/kubernetes.io/serviceaccount:ro,Z /var/lib/kubelet/pods/f734678c-95de-11e6-89b0-42010a8c0002/etc-hosts:/etc/hosts 对应的volume也都会正确设置SELinux： $ ls -Z /var/lib/kubelet/pods/f734678c-95de-11e6-89b0-42010a8c0002/volumes drwxr-xr-x. root root unconfined_u:object_r:svirt_sandbox_file_t:s0:c2,c3 kubernetes.io~empty-dir drwxr-xr-x. root root unconfined_u:object_r:svirt_sandbox_file_t:s0:c2,c3 kubernetes.io~secret 关于SELinux的介绍可以参考： 图文教程：SELinux政策实施详解 这篇文章以漫画的形式来讲解 , 深入浅出 Sysctls sysctl接口允许允许管理员在系统运行的状态下动态修改内核参数。 设置的参数在/proc/sys下可以看到。 其配置参数包含了多个Linux的多个子系统，比如： kernel(以kernel.开头的参数) networking（以net.开头的参数） virtual memory （以vm.开头的参数） MDADM （以dev.开头的参数） namespaced 和 node-level 的sysctl参数： namespaced级别的sysctl参数运行每个pod各自配置而不冲突。 要在pod的语境里(编排文件中)使用， 必须namespaced级别的sysctl, 这些sysctl参数包括： kernel.shm*, kernel.msg*, kernel.sem, fs.mqueue.*, net.*. 对于node-level级别的sysctl参数，比如vm.开头，dev.开头的参数， 如果要进行设置，需要管理员在node节点上配置（比如写node的/etc/sysctl.conf文件），或者用DaemonSet启动一个有系统特权的容器来进行设置。 Sysctls允许容器设置内核参数，分为安全Sysctls和非安全Sysctls 安全Sysctls：即设置后不影响其他Pod的内核选项，只作用在容器namespace中，默认开启。包括以下几种 kernel.shm_rmid_forced ： 这个参数启用后，会销毁未被使用的共享内存， 不要轻易开启 net.ipv4.ip_local_port_range： 表示用于向外连接的端口范围。缺省情况下很小：32768到61000 net.ipv4.tcp_syncookies： 表示开启SYN Cookies。当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击，默认为0，表示关闭 非安全Sysctls：即设置好有可能影响其他Pod和Node上其他服务的内核选项，默认禁止。如果使用，需要管理员在配置kubelet时开启，如kubelet --experimental-allowed-unsafe-sysctls 'kernel.msg*,net.ipv4.route.min_pmtu' Sysctls还在alpha阶段，需要通过Pod annotation设置，如：apiVersion: v1 kind: Pod metadata: name: sysctl-example annotations: security.alpha.kubernetes.io/sysctls: kernel.shm_rmid_forced=1 security.alpha.kubernetes.io/unsafe-sysctls: net.ipv4.route.min_pmtu=1000,kernel.msgmax=1 2 3 spec: ... 关于sysctl的介绍可以参考sysctl.conf学习和调优 和 设置Linux内核参数 /etc/sysctl.conf AppArmor AppArmor(Application Armor)是Linux内核的一个安全模块，允许系统管理员将每个程序与一个安全配置文件关联，从而限制程序的功能。通过它你可以指定程序可以读、写或运行哪些文件，是否可以打开网络端口等。作为对传统Unix的自主访问控制模块的补充，AppArmor提供了强制访问控制机制。 在使用AppArmor之前需要注意 Kubernetes版本>=v1.4 apiserver和kubelet已开启AppArmor特性，--feature-gates=AppArmor=true 已开启apparmor内核模块，通过cat /sys/module/apparmor/parameters/enabled查看 仅支持docker container runtime AppArmor profile已经加载到内核，通过cat /sys/kernel/security/apparmor/profiles查看 AppArmor还在alpha阶段，需要通过Pod annotation container.apparmor.security.beta.kubernetes.io/来设置。可选的值包括 runtime/default: 使用Container Runtime的默认配置 localhost/: 使用已加载到内核的AppArmor profile $ sudo apparmor_parser -q profile k8s-apparmor-example-deny-write flags=(attach_disconnected) { #include file, # Deny all file writes. deny /** w, } EOF' $ kubectl create -f /dev/stdin Seccomp Seccomp是Secure computing mode的缩写，它是Linux内核提供的一个操作，用于限制一个进程可以执行的系统调用．Seccomp需要有一个配置文件来指明容器进程允许和禁止执行的系统调用。 在Kubernetes中，需要将seccomp配置文件放到/var/lib/kubelet/seccomp目录中（可以通过kubelet选项--seccomp-profile-root修改）。比如禁止chmod的格式为 $ cat /var/lib/kubelet/seccomp/chmod.json { \"defaultAction\": \"SCMP_ACT_ALLOW\", \"syscalls\": [ { \"name\": \"chmod\", \"action\": \"SCMP_ACT_ERRNO\" } ] } Seccomp还在alpha阶段，需要通过Pod annotation设置，包括： security.alpha.kubernetes.io/seccomp/pod：应用到该Pod的所有容器 security.alpha.kubernetes.io/seccomp/container/：应用到指定容器 而value有三个选项： runtime/default: 使用Container Runtime的默认配置 unconfined: 允许所有系统调用 localhost/: 使用Node本地安装的seccomp，需要放到/var/lib/kubelet/seccomp目录中 比如使用刚才创建的seccomp配置： apiVersion: v1 kind: Pod metadata: name: trustworthy-pod annotations: seccomp.security.alpha.kubernetes.io/pod: localhost/chmod spec: containers: - name: trustworthy-container image: sotrustworthy:latest 参考文献 容器安全 Kubernetes集群安全配置案例 轻松了解Kubernetes认证功能 Kubernetes技术分析之安全 Kubernetes双向TLS配置-Centos7 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"controller/rc与rs.html":{"url":"controller/rc与rs.html","title":"使用rc与rs","keywords":"","body":" Replication Controller Replication Controller 是否手动创建Pod，如果想要创建同一个容器的多份拷贝，需要一个个分别创建出来么，能否将Pods划到逻辑组里？ Replication Controller确保任意时间都有指定数量的Pod“副本”在运行。如果为某个Pod创建了Replication Controller并且指定3个副本，它会创建3个Pod，并且持续监控它们。如果某个Pod不响应，那么Replication Controller会替换它，保持总数为3.如下面的动画所示： 如果之前不响应的Pod恢复了，现在就有4个Pod了，那么Replication Controller会将其中一个终止保持总数为3。如果在运行中将副本总数改为5，Replication Controller会立刻启动2个新Pod，保证总数为5。还可以按照这样的方式缩小Pod，这个特性在执行滚动升级时很有用。 当创建Replication Controller时，需要指定两个东西： Pod模板：用来创建Pod副本的模板 Label：Replication Controller需要监控的Pod的标签。 Replication Controller核心作用是确保在任何时候集群中一个RC所关联的Pod都保持一定数量的副本处于正常运行状态。如果该Pod的副本数量太多，则Replication Controller会销毁一些Pod副本；反之Replication Controller会添加副本，直到Pod的副本数量达到预设的副本数量。 最好不要越过RC直接创建Pod，因为Replication Controller会通过RC管理Pod副本，实现自动创建、补足、替换、删除Pod副本，这样就能提高应用的容灾能力，减少由于节点崩溃等意外状况造成的损失。即使应用程序只有一个Pod副本，也强烈建议使用RC来定义Pod。 pod通过RC创建：当Pod通过RC创建后，即使修改RC的模板定义，也不会影响到已经创建的Pod。此外Pod可以通过修改标签来实现脱离RC的管控，该方法可以用于将Pod从集群中迁移、数据修复等调试。对于被迁移的Pod副本，RC会自动创建一个新副本替换被迁移的副本。需要注意的是，通过kubectl删除RC时会一起删掉RC所创建的Pod副本，但是通过REST API删除时，需要将replicas设置为0，等到Pod删除后再删除RC。 重新调度：如前面所说，不论是想运行1个副本还是1000个副本，Replication Controller都能确保指定数量的副本存在于集群中，即使发生节点故障或Pod副本被终止运行等意外情况。 伸缩：修改Replication Controller的replicas的属性值，可以非常容易的实现扩大或缩小副本的数量。例如，通过下列命令可以实现手工修改名为foo的RC副本数量为3： kubectl scale –replicas=3 rc/foo 滚动更新：副本控制器被设计成通过逐个替换Pod的方式来辅助服务的滚动更新。推荐的方法是创建一个新的只有一个副本的RC，若新的RC副本数量加1，则旧的RC副本数量减1，直到这个旧的RC副本数量为0，然后删除旧的RC。这样即使在滚动更新的过程中发生了不可预测的错误，Pod集合的更新也都在可控范围之内。在理想情况下，滚动更新控制器需要将准备就绪的应用考虑在内，保证在集群中任何时刻都有足够数量的可用的Pod（https://github.com/kubernetes/kubernetes/issues/1353） Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"controller/使用deployment.html":{"url":"controller/使用deployment.html","title":"使用deployment","keywords":"","body":"在kubernetes集群中部署mysql主从 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"controller/使用job.html":{"url":"controller/使用job.html","title":"使用job","keywords":"","body":"直接上例子 apiVersion: batch/v1 kind: Job metadata: name: job-demo spec: template: metadata: name: job-demo spec: containers: - name: hello-world image: alpine command: [\"echo\", \"Hello World!\"] restartPolicy: Never Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"controller/使用cronjob.html":{"url":"controller/使用cronjob.html","title":"使用crontab","keywords":"","body":"直接上例子 apiVersion: batch/v1beta1 kind: CronJob metadata: name: cronjob-demo spec: schedule: \"*/1 * * * *\" jobTemplate: spec: template: spec: containers: - name: hello-word image: alpine command: [\"echo\", \"CronJob\"] restartPolicy: OnFailure Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"service/服务暴露发现.html":{"url":"service/服务暴露发现.html","title":"服务介绍","keywords":"","body":" Kubernetes 服务暴露介绍 LoadBlancer Service NodePort Service Ingress 反向代理负载均衡器 Ingress Controller Ingress ingress工作流程 Kubernetes 服务暴露介绍 从 kubernetes 1.2 版本开始，kubernetes提供了 Ingress 对象来实现对外暴露服务；到目前为止 kubernetes 总共有以下几种暴露服务的方式: LoadBlancer Service NodePort Service External IPs Ingress LoadBlancer Service LoadBlancer Service 是 kubernetes 深度结合云平台的一个组件；当使用 LoadBlancer Service 暴露服务时，实际上是通过向底层云平台申请创建一个负载均衡器来向外暴露服务；目前 LoadBlancer Service 支持的云平台已经相对完善，比如国外的 GCE、DigitalOcean，国内的 阿里云，私有云 Openstack 等等，由于 LoadBlancer Service 深度结合了云平台，所以只能在一些云平台上来使用 NodePort Service NodePort Service 顾名思义，实质上就是通过在集群的每个 node 上暴露一个端口，然后将这个端口映射到某个具体的 service 来实现的，虽然每个 node 的端口有很多(0~65535)，但是由于安全性和易用性(服务多了就乱了，还有端口冲突问题)实际使用可能并不多 Ingress Ingress 这个东西是 1.2 后才出现的，通过 Ingress 用户可以实现使用 nginx 等开源的反向代理负载均衡器实现对外暴露服务。 使用 Ingress 时一般会有三个组件： 反向代理负载均衡器 Ingress Controller Ingress 反向代理负载均衡器 反向代理负载均衡器很简单，说白了就是 nginx、apache 什么的；在集群中反向代理负载均衡器可以自由部署，可以使用 Replication Controller、Deployment、DaemonSet 等等，不过个人喜欢以 DaemonSet 的方式部署，感觉比较方便 Ingress Controller Ingress Controller 实质上可以理解为是个监视器，Ingress Controller 通过不断地跟 kubernetes API 打交道，实时的感知后端 service、pod 等变化，比如新增和减少 pod，service 增加与减少等；当得到这些变化信息后，Ingress Controller 再结合下文的 Ingress 生成配置，然后更新反向代理负载均衡器，并刷新其配置，达到服务发现的作用 Ingress Ingress 简单理解就是个规则定义；比如说某个域名对应某个 service，即当某个域名的请求进来时转发给某个 service;这个规则将与 Ingress Controller 结合，然后 Ingress Controller 将其动态写入到负载均衡器配置中，从而实现整体的服务发现和负载均衡 ingress工作流程 从上图中可以很清晰的看到，实际上请求进来还是被负载均衡器拦截，比如 nginx，然后 Ingress Controller 通过跟 Ingress 交互得知某个域名对应哪个 service，再通过跟 kubernetes API 交互得知 service 地址等信息；综合以后生成配置文件实时写入负载均衡器，然后负载均衡器 reload 该规则便可实现服务发现，即动态映射 了解了以上内容以后，这也就很好的说明了我为什么喜欢把负载均衡器部署为 Daemon Set；因为无论如何请求首先是被负载均衡器拦截的，所以在每个 node 上都部署一下，同时 hostport 方式监听 80 端口；那么就解决了其他方式部署不确定 负载均衡器在哪的问题，同时访问每个 node 的 80 都能正确解析请求；如果前端再 放个 nginx 就又实现了一层负载均衡 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"service/内部服务发现.html":{"url":"service/内部服务发现.html","title":"集群内部服务发现","keywords":"","body":" 内部服务发现 每个pod内部自己生成的环境变量 使用init container 的方式给pod赋予变量 使用 coredns/kubedns 服务发现 CoreDNS/KubeDNS的原理 自定义域和上下游域名服务器 为每个 Pod 设置 DNS 策略 说说无头服务 内部服务发现 每个pod内部自己生成的环境变量 每个pod创建后，它内部会根据k8s的规则自己生成相应的环境变量，如下： 查看名为nginxtest的pod [root@master ~]# kubectl get pod -o wide | grep nginxtest nginxtest-7b46d4c47f-dhjxp 1/1 Running 0 60d 10.2.35.57 192.168.3.28 nginxtest-7b46d4c47f-mvrg6 1/1 Running 0 60d 10.2.35.58 192.168.3.28 nginxtest-7b46d4c47f-nbgff 1/1 Running 0 60d 10.2.16.54 192.168.3.3 nginxtest-7b46d4c47f-z8h6m 1/1 Running 0 60d 10.2.16.55 192.168.3.3 随便进入其中一个pod [root@master ~]# kubectl exec -it nginxtest-7b46d4c47f-dhjxp /bin/bash root@nginxtest-7b46d4c47f-dhjxp:/# 显示这个pod的环境变量 root@nginxtest-7b46d4c47f-dhjxp:/# env DNFS_PORT_22_TCP_PROTO=tcp DEVPORTAL_WEB_PORT_80_TCP_ADDR=10.1.172.163 FOO_PORT_32323_TCP=tcp://10.1.226.80:32323 FOO_SERVICE_PORT=32323 FOO_SERVICE_HOST=10.1.226.80 DNFS_PORT_22_TCP=tcp://10.1.62.34:22 JENKINS_SERVICE_PORT_HTTP=80 FOO_PORT_32323_TCP_ADDR=10.1.226.80 JENKINS_PORT_80_TCP_PORT=80 HOSTNAME=nginxtest-7b46d4c47f-dhjxp NJS_VERSION=1.15.8.0.2.7-1~stretch DNFS_SERVICE_PORT_WEB=80 JENKINS_PORT_50000_TCP=tcp://10.1.210.74:50000 JENKINS_SERVICE_PORT_AGENT=50000 NGINX_VERSION=1.15.8-1~stretch FOO_PORT=tcp://10.1.226.80:32323 JENKINS_SERVICE_HOST=10.1.210.74 NGINX_PORT_80_TCP=tcp://10.1.180.137:80 KUBERNETES_PORT_443_TCP_PROTO=tcp KUBERNETES_PORT_443_TCP_ADDR=10.1.0.1 NGINX_PORT=tcp://10.1.180.137:80 DNFS_PORT_22_TCP_ADDR=10.1.62.34 DNFS_PORT_80_TCP_PORT=80 JENKINS_PORT_50000_TCP_PROTO=tcp KUBERNETES_PORT=tcp://10.1.0.1:443 FOO_PORT_32323_TCP_PROTO=tcp PWD=/ HOME=/root DNFS_SERVICE_PORT_ADMIN=22 NGINX_SERVICE_PORT=80 KUBERNETES_SERVICE_PORT_HTTPS=443 KUBERNETES_PORT_443_TCP_PORT=443 FOO_PORT_32323_TCP_PORT=32323 DEVPORTAL_WEB_PORT_80_TCP_PORT=80 DEVPORTAL_WEB_PORT_80_TCP=tcp://10.1.172.163:80 NGINX_PORT_80_TCP_ADDR=10.1.180.137 JENKINS_PORT_80_TCP_ADDR=10.1.210.74 DNFS_SERVICE_HOST=10.1.62.34 DEVPORTAL_WEB_PORT=tcp://10.1.172.163:80 DNFS_PORT_80_TCP_ADDR=10.1.62.34 NGINX_PORT_80_TCP_PORT=80 JENKINS_PORT=tcp://10.1.210.74:80 KUBERNETES_PORT_443_TCP=tcp://10.1.0.1:443 DNFS_PORT=tcp://10.1.62.34:80 DEVPORTAL_WEB_SERVICE_HOST=10.1.172.163 DNFS_PORT_80_TCP_PROTO=tcp DEVPORTAL_WEB_SERVICE_PORT_WEB=80 DEVPORTAL_WEB_PORT_80_TCP_PROTO=tcp TERM=xterm JENKINS_PORT_80_TCP=tcp://10.1.210.74:80 NGINX_PORT_80_TCP_PROTO=tcp JENKINS_PORT_50000_TCP_PORT=50000 SHLVL=1 KUBERNETES_SERVICE_PORT=443 DNFS_PORT_80_TCP=tcp://10.1.62.34:80 PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin DEVPORTAL_WEB_SERVICE_PORT=80 DNFS_SERVICE_PORT=80 DNFS_PORT_22_TCP_PORT=22 KUBERNETES_SERVICE_HOST=10.1.0.1 NGINX_SERVICE_HOST=10.1.180.137 JENKINS_PORT_50000_TCP_ADDR=10.1.210.74 JENKINS_SERVICE_PORT=80 JENKINS_PORT_80_TCP_PROTO=tcp _=/usr/bin/env 其中，和k8s集群有关的环境变量为： root@nginxtest-7b46d4c47f-dhjxp:/# env | grep KUBE KUBERNETES_PORT_443_TCP_PROTO=tcp KUBERNETES_PORT_443_TCP_ADDR=10.1.0.1 KUBERNETES_PORT=tcp://10.1.0.1:443 KUBERNETES_SERVICE_PORT_HTTPS=443 KUBERNETES_PORT_443_TCP_PORT=443 KUBERNETES_PORT_443_TCP=tcp://10.1.0.1:443 KUBERNETES_SERVICE_PORT=443 KUBERNETES_SERVICE_HOST=10.1.0.1 使用init container 的方式给pod赋予变量 正因为上面的原理,所以在以前,我们如果需要提前给容器富裕变量的时候,可以使用init container来预制pod的环境变量. 但是这个有一个弊端,就是依赖的服务必须在 Pod 启动之前就存在，不然是不会被注入到环境变量中的。 使用 coredns/kubedns 服务发现 我们真正的使用其实是利用 coredns/kubedns 来发现服务。这个和我们日常用到的DNS域名解析非常相似。 DNS是根据域名来访问相应的ip，而k8s中的DNS也是根据相应的域名去获取相应的pod的ip，从而最终的目的端还是我们需要访问的pod。 在以前我都以为在k8s集群中垮namespace是不能互相访问的，但在使用了coredns之后，才知道是可以跨namespace访问的。 下面使用瑞士军刀busybox做这个测试： 在default namespace下面创建这个busybox的deployment [root@master ~]# kubectl run --rm -i --tty test-dns --image=busybox /bin/sh If you don't see a command prompt, try pressing enter. / # 查看它的dns解析 / # cat /etc/resolv.conf nameserver 10.1.0.2 search default.svc.cluster.local. svc.cluster.local. cluster.local. openstacklocal options ndots:5 / # 使用k8s提供的规则来进行svc的访问,域名格式为： 普通的 Service：会生成 servicename.namespace.svc.cluster.local 的域名，会解析到 Service 对应的 ClusterIP 上，在 Pod 之间的调用可以简写成 servicename.namespace，如果处于同一个命名空间下面，甚至可以只写成 servicename 即可访问 Headless Service：无头服务，就是把 clusterIP 设置为 None 的，会被解析为指定 Pod 的 IP 列表，同样还可以通过 podname.servicename.namespace.svc.cluster.local 访问到具体的某一个 Pod。 # kubectl get svc --all-namespaces |grep nginx 3d default ingress-nginx NodePort 10.1.119.103 80:21616/TCP,443:28213/TCP 38d default nginxtest NodePort 10.1.169.229 80:24495/TCP 60d orz mynginx ClusterIP 10.1.27.58 80/TCP 4h 上面显示我有两个nginx服务,分别在default和orz namespace,所以在busybox这个pod中,我的访问方式为: 访问同一个namespace下的nginxtest: / # wget -q -O- nginxtest Welcome to nginx! body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } Welcome to nginx! If you see this page, the nginx web server is successfully installed and working. Further configuration is required. For online documentation and support please refer to nginx.org. Commercial support is available at nginx.com. Thank you for using nginx. / # / # 访问非同一个namespace是mynginx / # wget -q -O- mynginx.orz Welcome to nginx! body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } Welcome to nginx! If you see this page, the nginx web server is successfully installed and working. Further configuration is required. For online documentation and support please refer to nginx.org. Commercial support is available at nginx.com. Thank you for using nginx. / # 其实,真正的访问的地址应该解析为了: default下的nginx nginx.default.svc.cluster.local orz下的mynginx mynginx.orz.svc.cluster.local CoreDNS/KubeDNS的原理 在k8s集群中提供的这个DNS 服务不是一个独立的系统服务，而是作为一种 addon 插件而存在，也就是说不是 Kubernetes 集群必须安装的，当然我们强烈推荐安装，可以将这个插件看成是一种运行在 Kubernetes 集群上的一直比较特殊的应用，现在比较推荐的两个插件：kube-dns 和 CoreDNS。 下面以kubedns说明下工作原理 kubedns里面有3个container,分别是: kubedns: kubedns 基于 SkyDNS 库，通过 apiserver 监听 Service 和 Endpoints 的变更事件同时也同步到本地 Cache，实现了一个实时的 Kubernetes 集群内 Service 和 Pod 的 DNS服务发现 dnsmasq: dsnmasq 容器则实现了 DNS 的缓存功能(在内存中预留一块默认大小为 1G 的地方，保存当前最常用的 DNS 查询记录，如果缓存中没有要查找的记录，它会到 kubedns 中查询，并把结果缓存起来)，通过监听 ConfigMap 来动态生成配置 sider: sidecar 容器实现了可配置的 DNS 探测，并采集对应的监控指标暴露出来供 prometheus 使用 CoreDNS 实现的功能和 KubeDNS 是一致的，不过 CoreDNS 的所有功能都集成在了同一个容器中，在k8s 1.11.0版本中官方已经推荐使用 CoreDNS了，生产环境中可以安装 CoreDNS 来代替 KubeDNS，其他使用方法都是一致的：https://coredns.io/ 自定义域和上下游域名服务器 DNS Pod 具有静态 IP 并作为 Kubernetes 服务暴露出来。该静态 IP 被分配后，kubelet 会将使用 --cluster-dns = 参数配置的 DNS 传递给每个容器。DNS 名称也需要域名，本地域可以使用参数--cluster-domain = 在 kubelet 中配置。 dnsmasq 容器通过监听 ConfigMap 来动态生成配置，可以自定义存根域和上下游域名服务器。 例如，下面的 ConfigMap 建立了一个 DNS 配置，它具有一个单独的存根域和两个上游域名服务器： apiVersion: v1 kind: ConfigMap metadata: name: kube-dns namespace: kube-system data: stubDomains: | {\"qmxccs.local\": [\"1.2.3.4\"]} upstreamNameservers: | [\"8.8.8.8\", \"8.8.4.4\"] 按如上说明，具有.qmxccs.local后缀的 DNS 请求被转发到 DNS 1.2.3.4。Google 公共 DNS 服务器 为上游查询提供服务。下表描述了具有特定域名的查询如何映射到它们的目标 DNS 服务器： 域名 响应查询的服务器 kubernetes.default.svc.cluster.local kube-dns mircosvc.yfzx.qmxccs.local 自定义 DNS (1.2.3.4) www.google.com 上游 DNS (8.8.8.8, 8.8.4.4，其中之一) 为每个 Pod 设置 DNS 策略 我们还可以为每个 Pod 设置 DNS 策略。 当前 Kubernetes 支持两种 Pod 特定的 DNS 策略：“Default” 和 “ClusterFirst”。 可以通过 dnsPolicy 标志来指定这些策略。 注意：Default 不是默认的 DNS 策略。如果没有显式地指定dnsPolicy，将会使用 ClusterFirst 说说无头服务 上面说了下Headless Service：无头服务，就是把 clusterIP 设置为 None 的，会被解析为指定 Pod 的 IP 列表，同样还可以通过 podname.servicename.namespace.svc.cluster.local 访问到具体的某一个 Pod。 这个主要的应用场景两个: statefulset ：比如在k8s集群上的yfzx这个namespace上部署了一套redis，那么访问的时候，可以是 redis-0.yfzx.svc.cluster.local redis-1.yfzx.svc.cluster.local k8s集群外部署的服务，接入到k8s集群中 service中设置clusterip=none Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"service/service的使用.html":{"url":"service/service的使用.html","title":"service的使用","keywords":"","body":" service service概述 k8s集群中的三种IP k8s中找到svc的方法 容器的Service环境变量 DNS(CoreDNS/kubedns) service的类型 几个port易混淆的概念：nodePort、port、targetPort nodePort port targetPort service service概述 在k8s集群中，每个Pod都会被分配一个单独的IP地址，但这个IP地址会随着Pod的销毁而消失。这就引出的一个问题是：如果有一组Pod组成一个应用集群来提供服务，那么该如何访问它们呢？ Service就是用来解决这个问题的。它定义了一组Pod的逻辑集合和一个用于访问它们的策略，其实这个概念和微服务非常类似。，一个Service可以看作一组提供相同服务的Pod的对外接口，一个Serivce下面包含的Pod集合一般是由Label Selector来决定的。 举个例子：redis运行了2个副本，这两个Pod对于前端程序来说没有区别，所以前端程序并不关心是哪个后端副本在提供服务。并且后端Pod在发生变化时，前端也无须跟踪这些变化。Service就是用来实现这种解耦的抽象概念。 如何通过Service Cluster IP访问到后端的Pod呢？Kubernetes群集中的每个节点运行kube-proxy。该程序负责对Service实现虚拟IP的实现。在 Kubernetes v1.0，代理即是纯粹在用户空间。在Kubernetes v1.1添加了iptables代理，但是并不是默认的操作模式。在Kubernetes v1.2默认用iptables代理模式。在iptables代理模式下kube-proxy会观察Kubernetes Master节点添加和删除Service对象和Endpoint对象的行为，对于每个服务，kube-proxy会在本机的iptables中安装相应的规则，iptables通过这些规则将会捕获到该Service的流量并将它们重定向到一个后端的Pod。默认情况下后, 后端的选择是随机的。 但是也可以选择基于客户端IP的sessionaffinity，可以通过设置service.spec.sessionAffinity=ClientIP(默认值为“None”)来选择该方式。与用户空间的代理一样，客户端不知道Kubernetes或Service或Pod，任何对于Service的IP:Port的访问都会被代理到后端。但是iptables的代理比用户空间代理是更快、 更可靠。 k8s集群中的三种IP k8s集群中有3种IP： Node IP：Node节点的IP地址 Pod IP: Pod的IP地址 Pod的IP地址是由Docker Daemon根据docker0网桥的IP地址段进行分配的 Cluster IP: Service的IP地址 IP地址是Kubernetes系统中的虚拟IP地址，由系统动态分配。 Service的IP地址相对于Pod的IP地址来说是稳定的，Service被创建时即被分配一个IP地址，在销毁该Service之前，这个IP地址都不会再变化了。而Pod在Kubernetes集群中生命周期较短，可能被Replication Controller销毁、再次创建，新创建的Pod就会被分配一个新的IP地址。 Pods是短暂的，那么重启时IP地址可能会改变，怎么才能从前端容器正确可靠地指向后台容器呢？ Service是定义一系列Pod以及访问这些Pod的策略的一层抽象。Service通过Label找到Pod组。因为Service是抽象的，所以在图表里通常看不到它们的存在，这也就让这一概念更难以理解。 现在，假定有2个后台Pod，并且定义后台Service的名称为‘backend-service’，lable选择器为（tier=backend, app=myapp）。backend-service 的Service会完成如下两件重要的事情： 会为Service创建一个本地集群的DNS入口，因此前端Pod只需要DNS查找主机名为 ‘backend-service’，就能够解析出前端应用程序可用的IP地址。 现在前端已经得到了后台服务的IP地址，但是它应该访问2个后台Pod的哪一个呢？Service在这2个后台Pod之间提供透明的负载均衡，会将请求分发给其中的任意一个（如下面的动画所示）。通过每个Node上运行的代理（kube-proxy）完成。这里有更多技术细节。 下述动画展示了Service的功能。注意该图作了很多简化。如果不进入网络配置，那么达到透明的负载均衡目标所涉及的底层网络和路由相对先进。 k8s中找到svc的方法 Kubernetes支持两种主要的模式来找到Service： 一个是容器的Service环境变量 另一个是DNS。 容器的Service环境变量 在创建一个Pod时，kubelet在该Pod的所有容器中为当前所有Service添加一系列环境变量。 Kubernetes支持形如“{SVCNAME}SERVICE_HOST”和“{SVCNAME}_SERVICE_PORT”的变量。其中“{SVCNAME}”是大写的ServiceName，同时Service Name包含的“-”符号会转化为“”符号。 例如，已存在名称为“redis-master”的Service，它对外暴露6379的TCP端口，且集群IP为10.0.0.11。kubelet会为新建的容器添加以下环境变量： REDIS_MASTER_SERVICE_HOST=10.0.0.11 REDIS_MASTER_SERVICE_PORT=6379 REDIS_MASTER_PORT=tcp://10.0.0.11:6379 REDIS_MASTER_PORT_6379_TCP=tcp://10.0.0.11:6379 REDIS_MASTER_PORT_6379_TCP_PROTO=tcp REDIS_MASTER_PORT_6379_TCP_PORT=6379 REDIS_MASTER_PORT_6379_TCP_ADDR=10.0.0.11 通过环境变量来创建Service会带来一个不好的结果，即任何被某个Pod所访问的Service，必须先于该Pod创建，否则和这个后创建的Service相关的环境变量，将不会被加入该Pod的容器中。 DNS(CoreDNS/kubedns) DNS服务器通过Kubernetes API Server监控与Service相关的活动。当监控到添加Service的时，DNS服务器为每个Service创建一系列DNS记录。 例如：有个叫做”my-service“的service，它对应的kubernetes namespace为“my-ns”，那么会有它对应的dns记录，叫做“my-service.my-ns”。那么在my-ns的namespace中的pod都可以对my-service做name解析来轻松找到这个service。在其它namespace中的pod解析“my-service.my-ns”来找到它。解析出来的结果是这个service对应的cluster ip。 Service的ClusterIP地址只能在集群内部访问，如果集群外部的用户希望Service能够提供一个供集群外用户访问的IP地址。Kubernetes通过两种方式来实现上述需求，一个是“NodePort”，另一个是“LoadBalancer”。 service的类型 ClusterIP：使用集群内的私有ip —— 这是默认值。 NodePort：除了使用cluster ip外，也将service的port映射到每个node的一个指定内部port上，映射的每个node的内部port都一样。 LoadBalancer：使用一个ClusterIP & NodePort，但是会向cloud provider申请映射到service本身的负载均衡。 如果将type字段设置为NodePort，kubernetesmaster将会为service的每个对外映射的port分配一个”本地port“，这个本地port作用在每个node上，且必须符合定义在配置文件中的port范围（为–service-node-port-range）。这个被分配的”本地port“定义在service配置中的spec.ports[*].nodePort字段，如果为这个字段设定了一个值，系统将会使用这个值作为分配的本地port 或者 提示你port不符合规范。 几个port易混淆的概念：nodePort、port、targetPort nodePort 外部流量访问k8s集群中service入口的一种方式（另一种方式是LoadBalancer），即nodeIP:nodePort是提供给外部流量访问k8s集群中service的入口。 比如外部用户要访问k8s集群中的一个Web应用，那么我们可以配置对应service的type=NodePort，nodePort=30001。其它用户就可以通过浏览器http://node:30001访问到该web服务。 而数据库等服务可能不需要被外界访问，只需被内部服务访问即可，那么我们就不必设置service的NodePort。 port k8s集群内部服务之间访问service的入口。即clusterIP:port是service暴露在clusterIP上的端口。 mysql容器暴露了3306端口（参考DockerFile），集群内其它容器通过33306端口访问mysql服务，但是外部流量不能访问mysql服务，因为mysql服务没有配置NodePort。对应的service.yaml如下： apiVersion: v1 kind: Service metadata: name: mysql-service spec: ports: - port: 33306 targetPort: 3306 selector: name: mysql-pod targetPort 容器的端口（最终的流量端口）。targetPort是pod上的端口，从port和nodePort上来的流量，经过kube-proxy流入到后端pod的targetPort上，最后进入容器。 与制作容器时暴露的端口一致（使用DockerFile中的EXPOSE），例如官方的nginx（参考DockerFile）暴露80端口。 对应的service.yaml如下： apiVersion: v1 kind: Service metadata: name: nginx-service spec: type: NodePort // 有配置NodePort，外部流量可访问k8s中的服务 ports: - port: 30080 // 服务访问端口 targetPort: 80 // 容器端口 nodePort: 30001 // NodePort selector: name: nginx-pod 另外,targetport其实也可以是一个字符串 如:在deployment中定义container的port的时候，加上了一个name叫做ports apiVersion: apps/v1beta1 kind: Deployment metadata: name: nginx-deploy namespace: test labels: app: nginx-demo spec: replicas: 3 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 name: nginxweb 所以就可以在service的targetPort中直接用上这个名字的字符串，而非端口号了。 apiVersion: v1 kind: Service metadata: name: nginx-deploy-service spec: type: NodePort ports: - name: mynginx-http protocol: TCP port: 80 targetPort: nginxweb 另外Service 支持 TCP 和 UDP 协议，默认是 TCP 协议。 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"service/集群外部服务发现.html":{"url":"service/集群外部服务发现.html","title":"集群外部服务发现","keywords":"","body":" 集群外部服务访问--ingress 什么是ingress ingress的使用流程 Ingress 与 LoadBalancer 类型的 Service 的区别 使用ingress 反向代理负载均衡器 Ingress Controller Ingress ingress工作流程 部署 Traefik rbac.yaml daemonset.yaml treafikui.yaml 访问ui 集群外部服务访问--ingress service通常只是在集群内部有效，从集群外是无法访问到的。 在kubernets的世界里，服务要对集群外暴漏，有这么几种方式： NodePort LoadBalancer External IPs Ingress 什么是ingress # kubectl explain ingress KIND: Ingress VERSION: extensions/v1beta1 DESCRIPTION: Ingress is a collection of rules that allow inbound connections to reach the endpoints defined by a backend. An Ingress can be configured to give services externally-reachable urls, load balance traffic, terminate SSL, offer name based virtual hosting etc. Ingress是一种资源，和Pod、Configmap等等类似，也需要一个controller来管理。 之前比较常用的是NodePort，搭配keepalived可以获得一定程度的HA。 NodePort、LoadBalance、External IPs可以认为是L4的，而Ingress则是L7的。 Ingress 是一组允许外部请求进入集群的路由规则的集合。它可以给 Service 提供集群外部访问的 URL，负载均衡，SSL 终止等。 Ingress 起到了智能路由的角色，外部流量到达 Ingress ，再由它按已经制定好的规则分发到不同的后端服务中去。 看起来它很像我们使用的负载均衡器之类的,和Nginx很类似。 ingress的使用流程 ingress的使用流程是这样的： 集群部署时，创建一个或多个ingress controller，他们会去监听api server 当用户创建Ingress时，controller会通过api server获取新创建的Ingress的信息，主要是vhost、路径、service+port 注意ingress-nginx会直接将service翻译为endpoint，减少了service这一层转换，根据nginx模板生成新的nginx配置文件（主要是proxy_pass到RS） 最后reload nginx重新加载配置。 Ingress 与 LoadBalancer 类型的 Service 的区别 Ingress 不是一种 Service 类型 Ingress 是 K8S 中的一种资源类型，我们可以直接通过 kubectl get ingress 的方式获取我们已有的 Ingress 资源。 Ingress 可以有多种控制器（实现） 社区维护的有两个： 适用于 Google Cloud 的 GLBC NGINX Ingress Controller 它是使用 ConfigMap 存储 NGINX 配置实现的。 第三方的实现： 基于 Envoy 的 Contour; F5 的 F5 BIG-IP Controller; 基于 HAProxy 的 haproxy-ingress; 基于 Istio 的 Control Ingress Traffic; 现代化的反向代理服务器 Traefik; 以及 Kong 支持的 Kong Ingress Controller for Kubernetes NGINX 官方支持的 NGINX Ingress Controller。 使用ingress Ingress 这个东西是 1.2 后才出现的，通过 Ingress 用户可以实现使用 nginx 等开源的反向代理负载均衡器实现对外暴露服务 使用 Ingress 时一般会有三个组件: 反向代理负载均衡器 Ingress Controller Ingress 反向代理负载均衡器 反向代理负载均衡器很简单，说白了就是 nginx、apache 什么的；在集群中反向代理负载均衡器可以自由部署，可以使用 Replication Controller、Deployment、DaemonSet 等等，不过个人喜欢以 DaemonSet 的方式部署，感觉比较方便 Ingress Controller Ingress Controller 实质上可以理解为是个监视器，Ingress Controller 通过不断地跟 kubernetes API 打交道，实时的感知后端 service、pod 等变化，比如新增和减少 pod，service 增加与减少等；当得到这些变化信息后，Ingress Controller 再结合下文的 Ingress 生成配置，然后更新反向代理负载均衡器，并刷新其配置，达到服务发现的作用 Ingress Ingress 简单理解就是个规则定义；比如说某个域名对应某个 service，即当某个域名的请求进来时转发给某个 service;这个规则将与 Ingress Controller 结合，然后 Ingress Controller 将其动态写入到负载均衡器配置中，从而实现整体的服务发现和负载均衡 ingress工作流程 从上图中可以很清晰的看到，实际上请求进来还是被负载均衡器拦截，比如 nginx，然后 Ingress Controller 通过跟 Ingress 交互得知某个域名对应哪个 service，再通过跟 kubernetes API 交互得知 service 地址等信息；综合以后生成配置文件实时写入负载均衡器，然后负载均衡器 reload 该规则便可实现服务发现，即动态映射。 了解了以上内容以后，这也就很好的说明了我为什么喜欢把负载均衡器部署为 Daemon Set；因为无论如何请求首先是被负载均衡器拦截的，所以在每个 node 上都部署一下，同时 hostport 方式监听 80 端口；那么就解决了其他方式部署不确定 负载均衡器在哪的问题，同时访问每个 node 的 80 都能正确解析请求；如果前端再 放个 nginx 就又实现了一层负载均衡。 部署 Traefik 官网: https://traefik.io/ 由于微服务架构以及 Docker 技术和 kubernetes 编排工具最近几年才开始逐渐流行，所以一开始的反向代理服务器比如 nginx、apache 并未提供其支持，毕竟他们也不是先知；所以才会出现 Ingress Controller 这种东西来做 kubernetes 和前端负载均衡器如 nginx 之间做衔接；即 Ingress Controller 的存在就是为了能跟 kubernetes 交互，又能写 nginx 配置，还能 reload 它，这是一种折中方案；而最近开始出现的 traefik 天生就是提供了对 kubernetes 的支持，也就是说 traefik 本身就能跟 kubernetes API 交互，感知后端变化 在k8s 1.6版本上部署,需要3个yaml文件 rbac.yaml ： 用于创建相应的权限 daemonset.yaml ： 真正的controller treafikui.yaml ： 监控treafik的ui rbac.yaml apiVersion: v1 kind: ServiceAccount metadata: name: ingress namespace: kube-system --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: ingress subjects: - kind: ServiceAccount name: ingress namespace: kube-system roleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.io 可以部署为deployment,也可以部署为daemonset,在这儿我选择的是daemonset daemonset.yaml apiVersion: extensions/v1beta1 kind: DaemonSet metadata: name: traefik-ingress-lb namespace: kube-system labels: k8s-app: traefik-ingress-lb spec: template: metadata: labels: k8s-app: traefik-ingress-lb name: traefik-ingress-lb spec: terminationGracePeriodSeconds: 60 hostNetwork: true restartPolicy: Always serviceAccountName: ingress containers: - image: traefik:v1.6 name: traefik-ingress-lb resources: limits: cpu: 200m memory: 80Mi requests: cpu: 100m memory: 50Mi ports: - name: http containerPort: 80 hostPort: 80 - name: admin containerPort: 8580 hostPort: 8580 args: - --web - --web.address=:8580 - --kubernetes nodeSelector: edgenode: \"true\" traefik 监听 node 的 80 和 8580 端口，80 提供正常服务，8580 是其自带的 UI 界面，原本默认是 8080，因为环境里端口冲突了，所以这里临时改一下 注意有个label nodeSelector: edgenode: \"true\" 所以需要在部署的节点上打上这个label treafikui.yaml traefik 本身还提供了一套 UI 供我们使用，其同样以 Ingress 方式暴露，只需要创建一下即可 apiVersion: v1 kind: Service metadata: name: traefik-web-ui namespace: kube-system spec: selector: k8s-app: traefik-ingress-lb ports: - name: web port: 80 targetPort: 8580 --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: traefik-web-ui namespace: kube-system spec: rules: - host: k8sig.com http: paths: - path: / backend: serviceName: traefik-web-ui servicePort: web 访问ui 都创建无误以后，可以在官方的dashbord上看到部署的daemonset 给访问的机器的hosts配上node对应的域名 访问traefikui的地址: http://k8sig.com/dashboard/ 注意: traefik是有namespace隔离的！所以，如果是在不同的namespace下，需要部署多套ingress 接下来应该是加上tls,参考: https://mritd.me/2017/03/04/how-to-use-nginx-ingress/ Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"storage/存储介绍.html":{"url":"storage/存储介绍.html","title":"存储介绍","keywords":"","body":" 1.基本概念 2. Kubernetes中存储中有四个重要的概念： 3. POD使用存储的方法： 4. 用通俗易懂的话讲volume： 1.基本概念 容器中的存储都是临时的，因此Pod重启的时候，内部的数据会发生丢失。实际应用中，我们有些应用是无状态，有些应用则需要保持状态数据，确保Pod重启之后能够读取到之前的状态数据，有些应用则作为集群提供服务。这三种服务归纳为无状态服务、有状态服务以及有状态的集群服务，其中后面两个存在数据保存与共享的需求，因此就要采用容器外的存储方案。下图为存储的概念图： 2. Kubernetes中存储中有四个重要的概念： Volume Volumes是最基础的存储抽象，其支持多种类型，包括本地存储、NFS、FC以及众多的云存储，也可以编写自己的存储插件来支持特定的存储系统。Volume可以被Pod直接使用，也可以被PV使用。普通的Volume和Pod之间是一种静态的绑定关系，在定义Pod的同时，通过volume属性来定义存储的类型，通过volumeMount来定义容器内的挂载点。 PV (PersistentVolume ) 与普通的Volume不同，PV是Kubernetes中的一个资源对象，创建一个PV相当于创建了一个存储资源对象，这个资源的使用要通过PVC来请求。 PVC (PersistentVolumeClaim ) PVC是用户对存储资源PV的请求，根据PVC中指定的条件Kubernetes动态的寻找系统中的PV资源并进行绑定。目前PVC与PV匹配可以通过StorageClassName、matchLabels或者matchExpressions三种方式。 StorageClass PV与PVC的绑定的前提是需要先将PV创建出来，否则pvc的状态一直会是pending。如果需要动态的进行PV的创建的时候，我们就可以使用storageclass了。 3. POD使用存储的方法： 可以直接使用存储 可以使用PVC+PV+存储 可以使用PVC+SC+PV+存储 4. 用通俗易懂的话讲volume： Volume就是存储，和pod同生共死的。Volume有很多种类，包括：emptydir、hostpath、local。对于需要持久化的业务需求，基本不能满足要求。 PV就是包装了一层的存储，独立于pod的生命周期，专注于分布式存储，满足需要持久化的需求。 PVC就是用户的请求，和PV进行绑定，在没有使用SC的情况下需要PV创建了才会绑定，否则状态一直是pending，(绑定的时候判断依据是访问方式和容量大小)。 SC就是用来支持动态的创建PV，当有用户创建的PVC后，动态创建PV自动与PVC进行绑定(绑定的时候判断依据是访问方式和容量大小)。 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"storage/volumn.html":{"url":"storage/volumn.html","title":"volumn","keywords":"","body":" volume的种类： emptyDir Hostpath Docker提供了Volumes，Volume 是磁盘上的文件夹并且没有生命周期的管理。 Kubernetes 中的 Volume 是存储的抽象，并且能够为Pod提供多种存储解决方案。 Volume 最终会映射为Pod中容器可访问的一个文件夹或裸设备，但是背后的实现方式可以有很多种。 volume的种类： cephfs configMap emptyDir hostPath local nfs persistentVolumeClaim emptyDir emptyDir在Pod被分配到Node上之后创建，它的生命周期和所属的 Pod 完全一致，emptyDir主要作用可以在同一 Pod 内的不同容器之间共享工作过程中产生的文件。初始的时候为一个空文件夹，当Pod从Node中移除时，emptyDir将被永久删除。容器挂掉并不会导致emptyDir被删除，但是如果Pod从Node上被删除（Pod被删除，或者Pod发生迁移），emptyDir也会被删除，并且永久丢失。。emptyDir适用于一些临时存放数据的场景。 示例： # cat hostpath.yaml apiVersion: v1 kind: Pod metadata: name: busybox spec: containers: - name : busybox image: busybox imagePullPolicy: IfNotPresent command: - sleep - \"3600\" volumeMounts: - mountPath: /busybox-data name: data volumes: - name: data emptyDir: {} Hostpath hostPath就是将Node节点的文件系统挂载到Pod中。但是如果 Pod 发生跨主机的重建，其内容就很难保证了。这种卷一般和DaemonSet搭配使用。hostPath允许挂载Node上的文件系统到Pod里面去。如果Pod有需要使用Node上的东西，可以使用hostPath，不过不过建议使用，因为理论上Pod不应该感知Node的。 示例： # cat hostpath.yaml apiVersion: v1 kind: Pod metadata: name: busybox spec: containers: - name : busybox image: busybox imagePullPolicy: IfNotPresent command: - sleep - \"3600\" volumeMounts: - mountPath: /busybox-data name: data volumes: - hostPath: path: /tmp name: data emptyDir和hostPat很多场景是无法满足持久化需求，因为在Pod发生迁移的时候，数据都无法进行转移的，这就需要分布式文件系统的支持。 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"storage/pv、pvc、sc使用.html":{"url":"storage/pv、pvc、sc使用.html","title":"pv、pvc、sc","keywords":"","body":" PV PV的提供方式 PV的生命周期： Static Dynamic Binding Using Releasing Reclaiming pv 支持的类型 PV示例： PV的存储容量--Capacity PV的访问模式 Class PV的回收策略 PV的状态 Mount Options PVC Access Modes Resources Selector matchLabels - 卷必须具有带此值的标签 Class 声明PVC作为Volumes StorageClass Provisioner Parameters 使用建议 PV PV的提供方式 静态 动态 PV的生命周期： PV是集群中的资源。 PVC是对这些资源的请求，也是对资源的索赔检查。 PV和PVC之间的相互作用遵循这个生命周期: Provisioning ——-> Binding ——–> Using ——> Releasing ——> Recycling Static 集群管理员创建多个PV。 它们携带可供集群用户使用的真实存储的详细信息。 它们存在于Kubernetes API中，可用于消费。 Dynamic 当管理员创建的静态PV都不匹配用户的PersistentVolumeClaim时，集群可能会尝试为PVC动态配置卷。 此配置基于StorageClasses：PVC必须请求一个类，并且管理员必须已创建并配置该类才能进行动态配置。 要求该类的声明有效地为自己禁用动态配置 Binding 在动态配置的情况下，用户创建或已经创建了具有特定数量的存储请求和特定访问模式的PersistentVolumeClaim。 主机中的控制回路监视新的PVC，找到匹配的PV（如果可能），并将它们绑定在一起。 如果为新的PVC动态配置PV，则循环将始终将该PV绑定到PVC。 否则，用户总是至少得到他们要求的内容，但是卷可能超出了要求。 一旦绑定，PersistentVolumeClaim绑定是排他的，不管用于绑定它们的模式。 如果匹配的卷不存在，PVC将保持无限期。 随着匹配卷变得可用，PVC将被绑定。 例如，提供许多50Gi PV的集群将不匹配要求100Gi的PVC。 当集群中添加100Gi PV时，可以绑定PVC。 Using Pod使用PVC作为卷。 集群检查声明以找到绑定的卷并挂载该卷的卷。 对于支持多种访问模式的卷，用户在将其声明用作pod中的卷时指定所需的模式。 一旦用户有声明并且该声明被绑定，绑定的PV属于用户，只要他们需要它。 用户通过在其Pod的卷块中包含persistentVolumeClaim来安排Pods并访问其声明的PV。 Releasing 当用户完成卷时，他们可以从允许资源回收的API中删除PVC对象。 当声明被删除时，卷被认为是“释放的”，但是它还不能用于另一个声明。 以前的索赔人的数据仍然保留在必须根据政策处理的卷上. Reclaiming PersistentVolume的回收策略告诉集群在释放其声明后，该卷应该如何处理。 目前，卷可以是保留，回收或删除。 保留可以手动回收资源。 对于那些支持它的卷插件，删除将从Kubernetes中删除PersistentVolume对象，以及删除外部基础架构（如AWS EBS，GCE PD，Azure Disk或Cinder卷）中关联的存储资产。 动态配置的卷始终被删除 pv 支持的类型 GCEPersistentDisk AWSElasticBlockStore AzureFile AzureDisk FC (Fibre Channel) FlexVolume Flocker NFS iSCSI RBD (Ceph Block Device) CephFS Cinder (OpenStack block storage) Glusterfs VsphereVolume Quobyte Volumes HostPath (single node testing only – local storage is not supported in any way and WILL NOT WORK in a multi-node cluster) VMware Photon Portworx Volumes ScaleIO Volumes PV示例： apiVersion: v1 kind: PersistentVolume metadata: name: pv0003 spec: capacity: storage: 5Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: slow nfs: path: /tmp server: 172.17.0.2 PV的存储容量--Capacity 通常，PV将具有特定的存储容量。 这是使用PV的容量属性设置的。 看到库伯纳斯资源模型，以了解容量预期的单位。 目前，存储大小是唯一可以设置或请求的资源。 未来的属性可能包括IOPS，吞吐量等 PV的访问模式 PersistentVolume可以以资源提供者支持的任何方式安装在主机上。 提供商将具有不同的功能，每个PV的访问模式都被设置为该特定卷支持的特定模式。 例如，NFS可以支持多个读/写客户端，但是特定的NFS PV可能会以只读方式在服务器上导出。 每个PV都有自己的一组访问模式来描述具体的PV功能。 访问模式: ReadWriteOnce – the volume can be mounted as read-write by a single node (单node的读写) ReadOnlyMany – the volume can be mounted read-only by many nodes (多node的只读) ReadWriteMany – the volume can be mounted as read-write by many nodes (多node的读写) Notice:单个PV挂载的时候只支持一种访问模式 PV提供插件支持的access mode参考kubernetes官方文档 Class PV可以有一个类，通过将storageClassName属性设置为StorageClass的名称来指定。 特定类的PV只能绑定到请求该类的PVC。 没有storageClassName的PV没有类，只能绑定到不需要特定类的PVC。 在过去，使用了注释volume.beta.kubernetes.io/storage-class而不是storageClassName属性。 该注释仍然可以工作，但将来Kubernetes版本将不再适用。 PV的回收策略 目前的回收政策是： Retain – manual reclamation Recycle – basic scrub (“rm -rf /thevolume/*”) Delete – associated storage asset such as AWS EBS, GCE PD, Azure Disk, or OpenStack Cinder volume is deleted 目前，只有NFS和HostPath支持回收。 AWS EBS，GCE PD，Azure Disk和Cinder卷支持删除 PV的状态 Available – a free resource that is not yet bound to a claim Bound – the volume is bound to a claim Released – the claim has been deleted, but the resource is not yet reclaimed by the cluster Failed – the volume has failed its automatic reclamation Mount Options Kubernetes管理员可以指定在一个节点上挂载一个持久卷时的其他安装选项。 可以通过使用持久卷上的注释volume.beta.kubernetes.io/mount-options来指定安装选项。 示例: apiVersion: \"v1\" kind: \"PersistentVolume\" metadata: name: gce-disk-1 annotations: volume.beta.kubernetes.io/mount-options: \"discard\" spec: capacity: storage: \"10Gi\" accessModes: - \"ReadWriteOnce\" gcePersistentDisk: fsType: \"ext4\" pdName: \"gce-disk-1 PVC 每个PVC包含spec和status ! yaml示例 kind: PersistentVolumeClaim apiVersion: v1 metadata: name: myclaim spec: accessModes: - ReadWriteOnce resources: requests: storage: 8Gi storageClassName: slow selector: matchLabels: release: \"stable\" matchExpressions: - {key: environment, operator: In, values: [dev]} Access Modes 当请求具有特定访问模式的存储时，声明使用与卷相同的约定 Resources 声明（如pod）可以请求特定数量的资源。 在这种情况下，请求用于存储。 相同的资源模型适用于卷和声明 Selector 声明可以指定标签选择器以进一步过滤该卷集。 只有标签与选择器匹配的卷才能绑定到声明。 选择器可以由两个字段组成： matchLabels - 卷必须具有带此值的标签 matchExpressions - 通过指定关键字和值的关键字，值列表和运算符所做的要求列表。 有效运算符包括In，NotIn，Exists和DoesNotExist。 所有来自matchLabels和matchExpressions的要求都与AND一起使用，所有这些要求都必须满足才能匹配。 Class 声明可以通过使用属性storageClassName指定StorageClass的名称来请求特定的类。只有所请求的类的PV，与PVC相同的storageClassName的PV可以绑定到PVC。 PVC不一定要求一个className。它的storageClassName设置为等于“”的PVC总是被解释为请求没有类的PV，因此它只能绑定到没有类的PV（没有注释或一个等于“”）。没有storageClassName的PVC不完全相同，并且根据是否启用了DefaultStorageClass入门插件，集群的处理方式不同。 如果接纳插件已打开，则管理员可以指定默认的StorageClass。没有storageClassName的所有PVC只能绑定到该默认的PV。通过将StorageClass对象中的annotation storageclass.kubernetes.io/is-default-class设置为“true”来指定默认的StorageClass。如果管理员没有指定默认值，则集群会对PVC创建做出响应，就像入门插件被关闭一样。如果指定了多个默认值，则验收插件禁止创建所有PVC。 如果入门插件已关闭，则不存在默认StorageClass的概念。没有storageClassName的所有PVC只能绑定到没有类的PV。在这种情况下，没有storageClassName的PVC的处理方式与将其storageClassName设置为“”的PVC相同。 根据安装方法，安装过程中可以通过addon manager在Kubernetes群集中部署默认的StorageClass。 当PVC指定一个选择器，除了请求一个StorageClass之外，这些要求被AND组合在一起：只有所请求的类和所请求的标签的PV可能被绑定到PVC。请注意，当前，具有非空选择器的PVC不能为其动态配置PV。 在过去，使用了注释volume.beta.kubernetes.io/storage-class，而不是storageClassName属性。该注释仍然可以工作，但在未来的Kubernetes版本中它将不被支持。 声明PVC作为Volumes Pods通过将声明用作卷来访问存储。 声明必须存在于与使用声明的pod相同的命名空间中。 群集在pod的命名空间中查找声明，并使用它来获取支持声明的PersistentVolume。 然后将体积安装到主机并进入Pod。 kind: Pod apiVersion: v1 metadata: name: mypod spec: containers: - name: myfrontend image: dockerfile/nginx volumeMounts: - mountPath: \"/var/www/html\" name: mypd volumes: - name: mypd persistentVolumeClaim: claimName: myclaim PersistentVolumes绑定是独占的，并且由于PersistentVolumeClaims是命名空间对象，所以只能在一个命名空间内安装“许多”模式（ROX，RWX）—PVC支持被多个pod挂载 StorageClass 每个 StorageClass 包含字段provisioninger和参数，当属于类的PersistentVolume需要动态配置时使用。 StorageClass对象的名称很重要，用户可以如何请求特定的类。 管理员在首次创建StorageClass对象时设置类的名称和其他参数，并且在创建对象后无法更新对象。 管理员可以仅为不要求任何特定类绑定的PVC指定默认的StorageClass：有关详细信息，请参阅PersistentVolumeClaim部分。 示例: kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: standard provisioner: kubernetes.io/aws-ebs parameters: type: gp2 Provisioner 存储类有一个供应商，它确定用于配置PV的卷插件。 必须指定此字段。 您不限于指定此处列出的“内部”供应商（其名称前缀为“kubernetes.io”并与Kubernetes一起运送）。 您还可以运行和指定外部提供程序，它们是遵循Kubernetes定义的规范的独立程序。 外部提供者的作者对代码的生命周期，供应商的运输状况，运行状况以及使用的卷插件（包括Flex）等都有充分的自主权。存储库kubernetes-incubator /外部存储库包含一个库 用于编写实施大部分规范的外部提供者以及各种社区维护的外部提供者。 Parameters 存储类具有描述属于存储类的卷的参数。 取决于供应商，可以接受不同的参数。 例如，参数类型的值io1和参数iopsPerGB特定于EBS。 当省略参数时，使用一些默认值。 示例: AWS/GCE/Glusterfs/OpenStack Cinder kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: gold provisioner: kubernetes.io/cinder parameters: type: fast availability: nova 其他配置参考： https://kubernetes.io/docs/concepts/storage/persistent-volumes/#types-of-persistent-volumes 使用建议 如果正在编写在各种群集上运行并需要持久存储的配置模板或示例，建议使用以下模式： 在配置文件夹（包括部署，ConfigMaps等）中包含PersistentVolumeClaim对象。 在配置中不要包含PersistentVolume对象，因为实例化配置的用户可能没有创建PersistentVolumes的权限。 给用户提供实例化模板时提供存储类名称的选项。 如果用户提供存储类名称，并且集群是1.4或更高版本，请将该值放入PVC的volume.beta.kubernetes.io/storage-class注释中。如果集群的管理员启用了StorageClasses，这将导致PVC与正确的存储类匹配。 如果用户不提供存储类名称或者集群是版本1.3，那么在PVC上放置一个volume.alpha.kubernetes.io/storage-class：default注释。这将导致在某些群集上为用户自动配置PV，并具有合理的默认特性。 尽管在名称中使用了alpha，但这个注释背后的代码具有beta级别的支持。 不要使用volume.beta.kubernetes.io/storage-class：包含空字符串的任何值，因为它将阻止DefaultStorageClass接纳控制器运行（如果启用）。 在我们的工具中，请注意在一段时间后未绑定的PVC，并将其表示给用户，因为这可能表明集群没有动态存储支持（在这种情况下，用户应创建匹配的PV）或集群没有存储系统（在这种情况下，用户无法部署需要PVC的配置）。 在将来，预计大多数集群都将启用DefaultStorageClass，并提供某种形式的存储。但是，可能没有任何存储类名可用于所有集群，因此默认情况下继续不设置。在某种程度上，alpha注释将不再有意义，但PVC上的未设置的storageClass字段将具有所需的效果。 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"storage/pvpvcsc进一步说明.html":{"url":"storage/pvpvcsc进一步说明.html","title":"再说说 pv、pvc和storageclass","keywords":"","body":" 再说说 pv、pvc和storageclass的相关概念 三个概念 六个生命周期 三种PV的访问模式 九个PV Plugins 三个重声明策略(reclaim policy) 四个阶段(volumn phase) 四个PV选择器 五个可移植性建议 如何创建和使用PV， PVC pv示例 pvc示例 在pod中使用pvc 参考文献 再说说 pv、pvc和storageclass的相关概念 三个概念 pv ：持久化卷， 支持本地存储和网络存储， 例如hostpath，ceph rbd， nfs等，只支持两个属性， capacity和accessModes。其中capacity只支持size的定义，不支持iops等参数的设定，accessModes有三种，ReadWriteOnce（被单个node读写）， ReadOnlyMany（被多个nodes读）， ReadWriteMany（被多个nodes读写） storageclass：另外一种提供存储资源的方式， 提供更多的层级选型， 如iops等参数。 但是具体的参数与提供方是绑定的。 如aws和gce它们提供的storageclass的参数可选项是有不同的。 pvc ： 对pv或者storageclass资源的请求， pvc 对 pv 类比于pod 对不同的cpu， mem的请求。 六个生命周期 Provisioning Binding Using Releasing Reclaiming Recycling k8s对pv和pvc之间的交互的生命周期进行管理： provisioning－ 配置阶段， 分为static， dynamic两种方式。静态的方式是创建一系列的pv，然后pvc从pv中请求。 动态的方式是基于storageclass的。 Binding － 绑定阶段， pvc根据请求的条件筛选并绑定对应的pv。 一定pvc绑定pv后， 就会排斥其它绑定，即其它pvc无法再绑定同一个pv，即使这个pv设定的access mode允许多个node读写。 此外 ，pvc 如何匹配不到相应条件的pv， 那么就会显示unbound状态， 直到匹配为止。 需要注意的是，pvc请求100Gi大小的存储，即使用户创建了很多50Gi大小的存储， 也是无法被匹配的。 Using－ 使用阶段， pods 挂载存储， 即在pod的template文件中定义volumn使用某个pvc。 Releasing － 释放阶段， 当pvc对象被删除后， 就处于释放阶段。 在这个阶段， 使用的pv还不能被其它的pvc请求。 之前数据可能还会留存下来， 取决于用户在pv中设定的policy， 见persistentVolumeReclaimPolicy。 Reclaiming － 重声明阶段。 到这个阶段， 会告诉cluster如何处理释放的pv。 数据可能被保留（需要手工清除）， 回收和删除。动态分配的存储总是会被删除掉的。 Recycling － 回收阶段。回收阶段会执行基本的递归删除（取决于volumn plugins的支持），把pv上的数据删除掉， 以使pv可以被新的pvc请求。 用户也可以自定义一个 recycler pod ， 对数据进行删除。 三种PV的访问模式 ReadWriteOnce：是最基本的方式，可读可写，但只支持被单个Pod挂载。 ReadOnlyMany：可以以只读的方式被多个Pod挂载。 ReadWriteMany：这种存储可以以读写的方式被多个Pod共享。不是每一种存储都支持这三种方式，像共享方式，目前支持的还比较少，比较常用的是NFS。在PVC绑定PV时通常根据两个条件来绑定，一个是存储的大小，另一个就是访问模式。 九个PV Plugins pv是以plugin的形式来提供支持的， 考虑到私有云的使用场景， 排除掉azure， aws，gce等公有云厂商绑定的plugin， 有9个插件值得关注。这些plugin所支持的accessmode是不同的。 分别是： 存储Plugin | ReadWriteOnce | ReadOnlyMany | ReadWriteMany | 备注 FC (Fibre Channel) | 支持 | 支持 | 不支持 | NFS | 支持 | 支持 | 支持 | iSCSI | 支持 | 支持 | 不支持 | RBD (Ceph Block Device) | 支持 | 支持 | 不支持 | CephFS | 支持 | 支持 | 支持 | Cinder (OpenStack block storage) | 支持 | 不支持 | 不支持 | Glusterfs | 支持 | 支持 | 支持 | VsphereVolume | 支持 | 不支持 | 不支持 | HostPath | 支持 | 不支持 | 不支持 | 只支持单节点， 不支持跨节点 三个重声明策略(reclaim policy) Retain – 手动重新使用 Recycle – 基本的数据擦除 (“rm -rf /thevolume/*”) Delete – 相关联的后端存储卷删除， 后端存储比如AWS EBS, GCE PD, Azure Disk, or OpenStack Cinder 需要特别注意的是只有本地盘和nfs支持数据盘Recycle 擦除回收， AWS EBS, GCE PD, Azure Disk, and Cinder 存储卷支持Delete策略 四个阶段(volumn phase) 一个存储卷会处于下面几个阶段中的一个阶段： Available –资源可用， 还没有被声明绑定 Bound – 被声明绑定 Released – 绑定的声明被删除了，但是还没有被集群重声明 Failed – 自动回收失败 四个PV选择器 在PVC中绑定一个PV，可以根据下面几种条件组合选择 Access Modes， 按照访问模式选择pv Resources， 按照资源属性选择， 比如说请求存储大小为8个G的pv Selector， 按照pv的label选择 Class， 根据StorageClass的class名称选择, 通过annotation指定了Storage Class的名字, 来绑定特定类型的后端存储 关于根据class过滤出pv的说明： 所有的 PVC 都可以在不使用 StorageClass 注解的情况下，直接使用某个动态存储。把一个StorageClass 对象标记为 “default” 就可以了。StorageClass 用注解storageclass.beta.kubernetes.io/is-default-class 就可以成为缺省存储。 有了缺省的 StorageClass，用户创建 PVC 就不用 storage-class 的注解了，1.4 中新加入的DefaultStorageClass 准入控制器会自动把这个标注指向缺省存储类。 PVC 指定特定storageClassName，如fast时， 绑定名称为fast的storageClass PVC中指定storageClassName为“”时， 绑定no class的pv（pv中无class annotation， 或者其值为“”） PVC不指定storageClassName时， DefaultStorageClass admission plugin 开启与否（在apiserver启动时可以指定）， 对default class的解析行为是不同的。 当DefaultStorageClass admission plugin启用时， 针对没有storageClass annotation的pvc，DefaultStorageClass会分配一个默认的class， 这个默认的class需要用户指定，比如在创建storageclass对象时加入annotation,如 storageclass.beta.kubernetes.io/is-default-class: “true” 。如果有多个默认的class， 则pvc会被拒绝创建， 如果用户没有指定默认的class， 则这个DefaultStorageClass admission plugin不会起任何作用。 pvc会找那些no class的pv做绑定。 当DefaultStorageClass admission plugin没有启用时， 针对没有storageClass annotation的pvc， 会绑定no class的pv（pv中无class annotation， 或者其值为“”） 五个可移植性建议 把 pvc 和 其它一系列配置放一起， 比如说deployment，configmap 不要把pv放在其它配置里， 因为用户可能没有权限创建pv 初始化 pvc 模版的时候， 提供一个storageclass 在工具软件中，watch那些没有bound的pvc，并呈现给用户 集群启动的时候启用DefaultStorageClass， 但是不要指定某一类特定的class， 因为不同provisioner的class，参数很难一致 如何创建和使用PV， PVC pv示例 apiVersion: v1 kind: PersistentVolume metadata: name: pv0003 spec: capacity: storage: 5Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: slow nfs: path: /tmp server: 172.17.0.2 pvc示例 kind: PersistentVolumeClaim apiVersion: v1 metadata: name: myclaim spec: accessModes: - ReadWriteOnce resources: requests: storage: 8Gi storageClassName: slow selector: matchLabels: release: \"stable\" matchExpressions: - {key: environment, operator: In, values: [dev]} 在pod中使用pvc kind: Pod apiVersion: v1 metadata: name: mypod spec: containers: - name: myfrontend image: dockerfile/nginx volumeMounts: - mountPath: \"/var/www/html\" name: mypd volumes: - name: mypd persistentVolumeClaim: claimName: myclaim 参考文献 官方文档之持久化存储 官方文档之存储方案 flocker get started guide k8s-flocker-example Docker容器的持久存储模式介绍 开源数据卷管理工具flocker flcoker 支持的后端列表 Docker容器对存储的定义（Volume 与 Volume Plugin） 典型容器存储项目揭密：Flocker，Portworx和VSAN 官方文档之持久化存储 探讨容器中使用块存储 Configuring a Pod to Use a PersistentVolume for Storage Rook存储：Kubernetes中最优秀的存储 基于docker部署ceph以及修改docker image Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"storage/nfs-storageclass使用.html":{"url":"storage/nfs-storageclass使用.html","title":"基于nfs的storageclass的配置","keywords":"","body":" nfs-storageclass的配置过程 一、配置nfs 1.查看hosts配置 2.配置一块盘用作nfs 3.服务端配置 4. 客户端配置(每个节点都要配置) 二、配置StorageClass 1.rbac的配置 2.storageclass的配置 3.deployment的配置 4.创建及测试 nfs-storageclass的配置过程 配置过程分为两步： 配置nfs 配置storageclass 一、配置nfs 1.查看hosts配置 [root@k8s-master ~]# cat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 192.168.3.6 k8s-master 192.168.3.25 k8s-node1 192.168.3.26 k8s-node2 192.168.3.22 k8s-node3 从上面可以看到我的master节点的名字叫做 k8s-master ，在这儿我也准备将nfs的server部署到这个节点上。 2.配置一块盘用作nfs 我用的是lvm进行管理的，在这个地方挂载了一个目录为/nfsdisk,大小为50G的目录作为nfs磁盘 [root@k8s-master ~]# df -h Filesystem Size Used Avail Use% Mounted on /dev/vda1 34G 15G 20G 43% / devtmpfs 16G 0 16G 0% /dev tmpfs 16G 84K 16G 1% /dev/shm tmpfs 16G 137M 16G 1% /run tmpfs 16G 0 16G 0% /sys/fs/cgroup /dev/mapper/dockerlocal_vg-dockerlocal_lv_01 99G 3.2G 91G 4% /dockerlocal tmpfs 1.6G 16K 1.6G 1% /run/user/42 /dev/mapper/dockerlocal_vg-nfsdisk 50G 53M 47G 1% /nfsdisk 3.服务端配置 3.1 进行nfs的相关配置 # vim /etc/exports # cat /etc/exports /nfsdisk 192.168.3.0/24(rw,no_root_squash,no_all_squash,sync) 3.2 安装相关包 yum install -y nfs-utils 3.3 开始相关服务自启动 systemctl enable rpcbind.service systemctl enable nfs-server.service systemctl start rpcbind.service systemctl start nfs-server.service 3.4 检查 [root@k8s-master ~]# rpcinfo -p program vers proto port service 100000 4 tcp 111 portmapper 100000 3 tcp 111 portmapper 100000 2 tcp 111 portmapper 100000 4 udp 111 portmapper 100000 3 udp 111 portmapper 100000 2 udp 111 portmapper 100005 1 udp 20048 mountd 100005 1 tcp 20048 mountd 100005 2 udp 20048 mountd 100005 2 tcp 20048 mountd 100005 3 udp 20048 mountd 100005 3 tcp 20048 mountd 100024 1 udp 50252 status 100024 1 tcp 54712 status 100003 3 tcp 2049 nfs 100003 4 tcp 2049 nfs 100227 3 tcp 2049 nfs_acl 100003 3 udp 2049 nfs 100003 4 udp 2049 nfs 100227 3 udp 2049 nfs_acl 100021 1 udp 51608 nlockmgr 100021 3 udp 51608 nlockmgr 100021 4 udp 51608 nlockmgr 100021 1 tcp 42493 nlockmgr 100021 3 tcp 42493 nlockmgr 100021 4 tcp 42493 nlockmgr [root@k8s-master ~]# exportfs /nfsdisk 192.168.3.0/24 4. 客户端配置(每个节点都要配置) 在相应的3个节点进行配置 4.1 安装相关包并开启服务 yum install -y nfs-utils systemctl enable rpcbind.service systemctl start rpcbind.service 4.2 查看挂载的情况 [root@k8s-node2 ]# showmount -e k8s-master Export list for k8s-master: /nfsdisk 192.168.3.0/24 二、配置StorageClass 参考:https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client 配置StorageClass需要配置三个地方: rbac：创建一个sa,专门用于nfs的,所以权限那一块要给够 deployment：部署有关nfs的storageclass的一个pod，要注意上一步的ServiceAccount和PVC的写法 storageclass： 声明sc的相关信息 1.rbac的配置 rbac.yaml kind: ServiceAccount apiVersion: v1 metadata: name: nfs-client-provisioner --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: nfs-client-provisioner-runner rules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"create\", \"update\", \"patch\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-nfs-client-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner namespace: default roleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner rules: - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner namespace: default roleRef: kind: Role name: leader-locking-nfs-client-provisioner apiGroup: rbac.authorization.k8s.io 当然也可以偷懒给一个最大的cluster-admin的权限，如： apiVersion: v1 kind: ServiceAccount metadata: name: scpvcsa namespace: default --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-nfs roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: scpvcsa namespace: default 2.storageclass的配置 sc.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: managed-nfs-storage provisioner: fuseim.pri/ifs # or choose another name, must match deployment's env PROVISIONER_NAME' parameters: archiveOnDelete: \"false\" 3.deployment的配置 kind: Deployment apiVersion: extensions/v1beta1 metadata: name: nfs-client-provisioner spec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: 192.168.3.6:8888/private/nfs-client-provisioner:latest imagePullPolicy: IfNotPresent volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: fuseim.pri/ifs - name: NFS_SERVER value: 192.168.3.6 - name: NFS_PATH value: /nfsdisk imagePullSecrets: - name: harborsecret volumes: - name: nfs-client-root nfs: server: 192.168.3.6 path: /nfsdisk 4.创建及测试 4.1创建 [root@k8s-master softdb]# ls sc_config/ deployment.yaml rbac.yaml sc.yaml [root@k8s-master softdb]# kubectl create -f sc_config/ deployment.extensions/nfs-client-provisioner created serviceaccount/nfs-client-provisioner created clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created role.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created rolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created storageclass.storage.k8s.io/managed-nfs-storage created [root@k8s-master softdb]# kubectl get pod |grep nfs nfs-client-provisioner-6594988b5b-297b8 1/1 Running 0 44m [root@k8s-master softdb]# kubectl describe pod nfs-client-provisioner-6594988b5b-297b8 Name: nfs-client-provisioner-6594988b5b-297b8 Namespace: default Node: k8s-node1/192.168.3.25 Start Time: Thu, 27 Dec 2018 16:13:02 +0800 Labels: app=nfs-client-provisioner pod-template-hash=2150544616 Annotations: Status: Running IP: 10.2.19.6 Controlled By: ReplicaSet/nfs-client-provisioner-6594988b5b Containers: nfs-client-provisioner: Container ID: docker://994664692a1cc69519c988bbcbda658dfe9bf3c3325d3fce446902f34d5f7528 Image: 192.168.3.6:8888/private/nfs-client-provisioner:latest Image ID: docker-pullable://192.168.3.6:8888/private/nfs-client-provisioner@sha256:4c16495be5b893efea1c810e8451c71e1c58f076494676cae2ecab3a382b6ed0 Port: Host Port: State: Running Started: Thu, 27 Dec 2018 16:19:27 +0800 Ready: True Restart Count: 0 Environment: PROVISIONER_NAME: fuseim.pri/ifs NFS_SERVER: 192.168.3.6 NFS_PATH: /nfsdisk Mounts: /persistentvolumes from nfs-client-root (rw) /var/run/secrets/kubernetes.io/serviceaccount from nfs-client-provisioner-token-cn4kl (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: nfs-client-root: Type: NFS (an NFS mount that lasts the lifetime of a pod) Server: 192.168.3.6 Path: /nfsdisk ReadOnly: false nfs-client-provisioner-token-cn4kl: Type: Secret (a volume populated by a Secret) SecretName: nfs-client-provisioner-token-cn4kl Optional: false QoS Class: BestEffort Node-Selectors: Tolerations: Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 44m default-scheduler Successfully assigned default/nfs-client-provisioner-6594988b5b-297b8 to k8s-node1 Normal Pulling 43m (x4 over 44m) kubelet, k8s-node1 pulling image \"192.168.3.6:8888/private/nfs-client-provisioner:latest\" Warning Failed 43m (x4 over 44m) kubelet, k8s-node1 Failed to pull image \"192.168.3.6:8888/private/nfs-client-provisioner:latest\": rpc error: code = Unknown desc = Error resp onse from daemon: Get https://192.168.3.6:8888/v2/: http: server gave HTTP response to HTTPS client Warning Failed 43m (x4 over 44m) kubelet, k8s-node1 Error: ErrImagePull Warning Failed 42m (x7 over 44m) kubelet, k8s-node1 Error: ImagePullBackOff Normal BackOff 39m (x20 over 44m) kubelet, k8s-node1 Back-off pulling image \"192.168.3.6:8888/private/nfs-client-provisioner:latest\" Normal SandboxChanged 39m kubelet, k8s-node1 Pod sandbox changed, it will be killed and re-created. Normal Pulling 38m kubelet, k8s-node1 pulling image \"192.168.3.6:8888/private/nfs-client-provisioner:latest\" Normal Pulled 38m kubelet, k8s-node1 Successfully pulled image \"192.168.3.6:8888/private/nfs-client-provisioner:latest\" Normal Created 38m kubelet, k8s-node1 Created container Normal Started 38m kubelet, k8s-node1 Started container 4.2测试 创建一个PVC，查看是否能动态的创建PV test-pvc.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: test-claim annotations: volume.beta.kubernetes.io/storage-class: \"managed-nfs-storage\" spec: accessModes: - ReadWriteMany resources: requests: storage: 1Mi [root@k8s-master softdb]# kubectl create -f test-pvc.yaml persistentvolumeclaim/test-claim created [root@k8s-master softdb]# kubectl get sc,pvc,pv NAME PROVISIONER AGE storageclass.storage.k8s.io/managed-nfs-storage fuseim.pri/ifs 14m NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/test-claim Bound pvc-b51ca970-09b0-11e9-ada0-525400f9dfac 1Mi RWX managed-nfs-storage 32s NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/pvc-b51ca970-09b0-11e9-ada0-525400f9dfac 1Mi RWX Delete Bound default/test-claim managed-nfs-storage 31s 可以看到在创建了名为test-claim的PVC后，动态的创建出了名为pvc-b51ca970-09b0-11e9-ada0-525400f9dfac的PV,它使用的storageclass正是之前创建的storageclassmanaged-nfs-storage Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 18:02:26 "},"storage/在k8s中使用rbd.html":{"url":"storage/在k8s中使用rbd.html","title":"在k8s中使用ceph rbd","keywords":"","body":" 在k8s中使用rbd 存储可以分为三类 ceph集群使用ceph rbd rbd使用过程 直接使用RBD作为volume RBD用作PV/PVC 利用storage class使用RBD 参考: 在k8s中使用rbd 存储可以分为三类 对象存储（例如aws s3、aliyun oss） 文件存储（例如nfs、nas） 块存储。（例如Ceph RBD ceph集群使用ceph rbd 对于k8s来说，需要的Ceph信息有这些： IP地址 端口号 管理员用户名 管理员keyring node上准备ceph yum install ceph-common ceph-fs-common -y ceph命令行需要2个配置文件，具体配置问ceph的管理团队 /etc/ceph/ceph.conf /etc/ceph/ceph.client.admin.keyring rbd使用过程 RBD是这样使用的：用户在Ceph上创建Pool（逻辑隔离），然后在Pool中创建image（实际存储介质），之后再将image挂载到本地服务器的某个目录上。 # rbd list #列出默认pool下的image # rbd list -p k8s #列出pool k8s下的image # rbd create foo -s 1024 #在默认pool中创建名为foo的image，大小为1024MB # rbd map foo #将ceph集群的image映射到本地的块设备 /dev/rbd0 # ls -l /dev/rbd0 #是b类型 brw-rw---- 1 root disk 252, 0 May 22 20:57 /dev/rbd0 $ rbd showmapped #查看已经map的rbd image id pool image snap device 0 rbd foo - /dev/rbd0 # mount /dev/rbd1 /mnt/bar/ #此时去mount会失败，因为image还没有格式化文件系统 mount: /dev/rbd1 is write-protected, mounting read-only mount: wrong fs type, bad option, bad superblock on /dev/rbd1, missing codepage or helper program, or other error In some cases useful info is found in syslog - try dmesg | tail or so. # mkfs.ext4 /dev/rbd0 #格式化为ext4 ... Writing superblocks and filesystem accounting information: done # mount /dev/rbd0 /mnt/foo/ #重新挂载 # df -h |grep foo #ok /dev/rbd0 976M 2.6M 907M 1% /mnt/foo 上面都是在默认pool中做的，在k8s里，建议使用自己的pool了。 # ceph osd lspools # 看看已经有那些pool # ceph osd pool create k8s 128 #创建pg_num 为128的名为k8s的pool # rados df # rbd create foobar -s 1024 -p k8s #在k8s pool中创建名为foobar的image，大小为1024MB 直接使用RBD作为volume 前面已经创建好了image foobar，给volume用简单直接粗暴，挂就行了。 apiVersion: v1 kind: Pod metadata: name: rbd spec: containers: - image: gcr.io/nginx name: rbd-rw volumeMounts: - name: rbdpd mountPath: /mnt/rbd volumes: - name: rbdpd rbd: monitors: - '1.2.3.4:6789' pool: k8s image: foobar fsType: ext4 readOnly: false user: admin keyring: /etc/ceph/ceph.client.admin.keyring Pod启动后，可以看到文件系统由k8s做好并挂载到了容器里。我们将/etc/hosts文件拷贝到/mnt/rbd/目录去。 # kubectl exec rbd -- df -h|grep rbd /dev/rbd6 976M 2.6M 907M 1% /mnt/rbd # kubectl exec rbd -- cp /etc/hosts /mnt/rbd/ # kubectl exec rbd -- cat /mnt/rbd/hosts # Kubernetes-managed hosts file. 127.0.0.1 localhost ::1 localhost ip6-localhost ip6-loopback fe00::0 ip6-localnet fe00::0 ip6-mcastprefix fe00::1 ip6-allnodes fe00::2 ip6-allrouters 10.244.3.249 rbd 然后将Pod删除、重新挂载foobar image。 前面Pod要求各node上都要有keyring文件，很不方便也不安全。新的Pod我使用推荐的做法：secret（虽然也安全不到哪里） 先创建一个secret apiVersion: v1 kind: Secret metadata: name: ceph-secret type: \"kubernetes.io/rbd\" data: key: QVFCXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX9PQ== 在新的Pod里Ref这个secret apiVersion: v1 kind: Pod metadata: name: rbd3 spec: containers: - image: gcr.io/nginx name: rbd-rw volumeMounts: - name: rbdpd mountPath: /mnt/rbd volumes: - name: rbdpd rbd: monitors: - '1.2.3.4:6789' pool: k8s image: foobar fsType: ext4 readOnly: false user: admin secretRef: name: ceph-secret RBD用作PV/PVC 先在k8s pool里创建一个名为pv的 image。 rbd image create pv -s 1024 -p k8s 再创建一个PV，使用上面创建的image pv。 apiVersion: v1 kind: PersistentVolume metadata: name: ceph-rbd-pv spec: capacity: storage: 1Gi accessModes: - ReadWriteOnce rbd: monitors: - '1.2.3.4:6789' pool: k8s image: pv user: admin secretRef: name: ceph-secret fsType: ext4 readOnly: false persistentVolumeReclaimPolicy: Recycle 创建一个PVC，要求一块1G的存储。 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: ceph-rbd-pv-claim spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi 利用storage class使用RBD 先创建一个SC。根PV一样，SC也是集群范围的（RBD认为是fast的）。 kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: fast provisioner: kubernetes.io/rbd parameters: monitors: 172.25.60.3:6789 adminId: admin adminSecretName: ceph-secret adminSecretNamespace: resource-quota pool: k8s userId: admin userSecretName: ceph-secret fsType: ext4 imageFormat: \"2\" imageFeatures: \"layering\" 之后创建应用的时候，需要同时创建 pvc+pod，二者通过claimName关联。pvc中需要指定其storageClassName为上面创建的sc的name（即fast）。 kind: PersistentVolumeClaim apiVersion: v1 metadata: name: rbd-pvc-pod-pvc spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi storageClassName: fast RBD只支持 ReadWriteOnce 和 ReadOnlyAll，不支持ReadWriteAll。注意这两者的区别点是，不同nodes之间是否可以同时挂载。同一个node上，即使是ReadWriteOnce，也可以同时挂载到2个容器上的。 apiVersion: v1 kind: Pod metadata: labels: test: rbd-pvc-pod name: ceph-rbd-sc-pod2 spec: containers: - name: ceph-rbd-sc-nginx image: gcr.io/nginx volumeMounts: - name: ceph-rbd-vol1 mountPath: /mnt/ceph-rbd-pvc/nginx readOnly: false volumes: - name: ceph-rbd-vol1 persistentVolumeClaim: claimName: rbd-pvc-pod-pvc 再来一个statefulset的例子: apiVersion: apps/v1 kind: StatefulSet metadata: name: nginx spec: selector: matchLabels: app: nginx serviceName: \"nginx\" replicas: 3 template: metadata: labels: app: nginx spec: terminationGracePeriodSeconds: 10 containers: - name: nginx image: gcr.io/nginx volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: [ \"ReadWriteOnce\" ] storageClassName: \"fast\" resources: requests: storage: 8Gi 参考: 使用Ceph RBD为Kubernetes集群提供存储卷 初试 Kubernetes 集群使用 Ceph RBD 块存储 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"storage/configmap使用.html":{"url":"storage/configmap使用.html","title":"configmap使用","keywords":"","body":" Configmap 介绍 创建ConfigMap的方式有4种： 通过命令行参数--from-literal创建 指定文件创建 指定目录创建 通过事先写好configmap的标准yaml文件创建 使用ConfigMap 通过环境变量使用 在启动命令中引用 Configmap 介绍 镜像使用的过程中，经常需要利用配置文件、启动脚本等方式来影响容器的运行方式，如果仅有少量配置，我们可以使用环境变量的方式来进行配置。然而对于一些较为复杂的配置，k8s提供了configmap解决方案。　 ConfigMap API资源存储键/值对配置数据，这些数据可以在pods里使用。 ConfigMap跟Secrets类似，但是ConfigMap可以更方便的处理不包含敏感信息的字符串。 当ConfigMap以数据卷的形式挂载进Pod的时，这时更新ConfigMap（或删掉重建ConfigMap），Pod内挂载的配置信息会热更新。这时可以增加一些监测配置文件变更的脚本，然后reload对应服务 ConfigMap的API概念上来说是很简单的。从数据角度来看，ConfigMap的类型只是键值组。应用可以从不同角度来配置。在一个pod里面使用ConfigMap大致有三种方式： 1、命令行参数 2、环境变量 3、数据卷文件 创建ConfigMap的方式有4种： 通过直接在命令行中指定configmap参数创建，即--from-literal 通过指定文件创建，即将一个配置文件创建为一个ConfigMap--from-file= 通过指定目录创建，即将一个目录下的所有配置文件创建为一个ConfigMap，--from-file= 事先写好标准的configmap的yaml文件，然后kubectl create -f 创建 通过命令行参数--from-literal创建 kubectl create configmap test-config1 --from-literal=db.host=10.5.10.116 --from-listeral=db.port='3306' 指定文件创建 配置文件app.properties的内容： 创建命令（可以有多个--from-file）： kubectl create configmap test-config2 --from-file=./app.properties 可以看到指定文件创建时configmap会创建一个key/value对，key是文件名，value是文件内容。 假如不想configmap中的key为默认的文件名，还可以在创建时指定key名字： kubectl create configmap game-config-3 --from-file== 指定目录创建 configs 目录下的config-1和config-2内容如下所示： 创建命令： kubectl create configmap test-config3 --from-file=./configs 可以看到指定目录创建时configmap内容中的各个文件会创建一个key/value对，key是文件名，value是文件内容。 那假如目录中还包含子目录呢？继续做实验： 在上一步的configs目录下创建子目录subconfigs，并在subconfigs下面创建两个配置文件，指定目录configs创建名为test-config4的configmap: kubectl create configmap test-config4 --from-file=./configs 结果说明指定目录时只会识别其中的文件，忽略子目录 通过事先写好configmap的标准yaml文件创建 注意其中一个key的value有多行内容时的写法 使用ConfigMap 使用ConfigMap有三种方式: 第一种是通过环境变量的方式，直接传递给pod 使用configmap中指定的key 使用configmap中所有的key 第二种是通过在pod的命令行下运行的方式(启动命令中) 第三种是作为volume的方式挂载到pod内 通过环境变量使用 (1)使用valueFrom、configMapKeyRef、name、key指定要用的key: apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: test-container image: k8s.gcr.io/busybox command: [ \"/bin/sh\", \"-c\", \"env\" ] env: - name: SPECIAL_LEVEL_KEY valueFrom: configMapKeyRef: name: special-config key: special.how - name: LOG_LEVEL valueFrom: configMapKeyRef: name: env-config key: log_level restartPolicy: Never (2)还可以通过envFrom、configMapRef、name使得configmap中的所有key/value对都自动变成环境变量： apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: test-container image: k8s.gcr.io/busybox command: [ \"/bin/sh\", \"-c\", \"env\" ] envFrom: - configMapRef: name: special-config restartPolicy: Never 在启动命令中引用 在命令行下引用时，需要先设置为环境变量，之后可以通过$(VAR_NAME)设置容器启动命令的启动参数： apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: test-container image: k8s.gcr.io/busybox command: [ \"/bin/sh\", \"-c\", \"echo $(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)\" ] env: - name: SPECIAL_LEVEL_KEY valueFrom: configMapKeyRef: name: special-config key: SPECIAL_LEVEL - name: SPECIAL_TYPE_KEY valueFrom: configMapKeyRef: name: special-config key: SPECIAL_TYPE restartPolicy: Never 作为volume挂载使用 把1.4中test-config4所有key/value挂载进来： apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-configmap spec: replicas: 1 template: metadata: labels: app: nginx-configmap spec: containers: - name: nginx-configmap image: nginx ports: - containerPort: 80 volumeMounts: - name: config-volume4 mountPath: /tmp/config4 volumes: - name: config-volume4 configMap: name: test-config4 (2)假如不想以key名作为配置文件名可以引入items 字段，在其中逐个指定要用相对路径path替换的key： volumes: - name: config-volume4 configMap: name: test-config4 items: - key: my.cnf path: mysql-key - key: cache_host path: cache-host 备注： 删除configmap后原pod不受影响；然后再删除pod后，重启的pod的events会报找不到cofigmap的volume； pod起来后再通过kubectl edit configmap …修改configmap，过一会pod内部的配置也会刷新。 在容器内部修改挂进去的配置文件后，过一会内容会再次被刷新为原始configmap内容 (3)还可以为以configmap挂载进的volume添加subPath字段: volumeMounts: - name: config-volume5 mountPath: /tmp/my subPath: my.cnf - name: config-volume5 mountPath: /tmp/host subPath: cache_host - name: config-volume5 mountPath: /tmp/port subPath: cache_port - name: config-volume5 mountPath: /tmp/prefix subPath: cache_prefix volumes: - name: config-volume5 configMap: name: test-config4 注意在容器中的形式与（2）中的不同，（2）中是个链接，链到..data/。 备注： 删除configmap后原pod不受影响；然后再删除pod后，重启的pod的events会报找不到cofigmap的volume。 pod起来后再通过kubectl edit configmap …修改configmap，pod内部的配置也会自动刷新。 在容器内部修改挂进去的配置文件后，内容可以持久保存，除非杀掉再重启pod才会刷回原始configmap的内容。 subPath必须要与configmap中的key同名。 mountPath如/tmp/prefix： 当/tmp/prefix不存在时(备注：此时/tmp/prefix和/tmp/prefix/无异)，会自动创建prefix文件并把value写进去； 当/tmp/prefix存在且是个文件时，里面内容会被configmap覆盖； 当/tmp/prefix存在且是文件夹时，无论写/tmp/prefix还是/tmp/prefix/都会报错。 Configmap的热更新研究 更新 ConfigMap 后： 使用该 ConfigMap 挂载的 Env 不会同步更新 使用该 ConfigMap 挂载的 Volume 中的数据需要一段时间（实测大概10秒）才能同步更新 ENV 是在容器启动的时候注入的，启动之后 kubernetes 就不会再改变环境变量的值，且同一个 namespace 中的 pod 的环境变量是不断累加的，参考 Kubernetes中的服务发现与docker容器间的环境变量传递源码探究。为了更新容器中使用 ConfigMap 挂载的配置，可以通过滚动更新 pod 的方式来强制重新挂载 ConfigMap，也可以在更新了 ConfigMap 后，先将副本数设置为 0，然后再扩容。 　　当ConfigMap以数据卷的形式挂载进Pod的时，这时更新ConfigMap（或删掉重建ConfigMap），Pod内挂载的配置信息会热更新。这时可以增加一些监测配置文件变更的脚本，然后reload对应服务。 参考： 1.https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/ 2.https://www.cnblogs.com/breezey/p/6582082.html 3.https://kubernetes.io/docs/concepts/storage/volumes/ 4.https://www.kubernetes.org.cn/3138.html Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"storage/secret的使用.html":{"url":"storage/secret的使用.html","title":"secret使用","keywords":"","body":" Secret 参考 官方文档之Secrets secret概览 secret是k8s里的一个对象， 它用于存储一些敏感数据，比如说密码， token， 密钥等等。这类信息如果直接明文放在镜像或者pod里， 比较不安全。 用secret来保存会更加安全，可以防止意外泄露。 secret如何被使用？ 有两种使用方式： 1. 作为一个卷被pod挂载 2. kubelet 为pod拉取镜像的时候使用 内置的secret Service Account创建时会自动secret，供集群访问API时使用 使用kubectl命令创建secret 先在本地创建两个文本用于存放username和password# Create files needed for rest of example. $ echo -n \"admin\" > ./username.txt $ echo -n \"1f2d1e2e67df\" > ./password.txt 用 kubectl命令创建Secret，把这两个文件的内容打包进Secret， 并在apiserver中创建API 对象$ kubectl create secret generic db-user-pass --from-file=./username.txt --from-file=./password.txt secret \"db-user-pass\" created 查看secret ```shell $ kubectl get secrets NAME TYPE DATA AGE db-user-pass Opaque 2 51s $ kubectl describe secrets/db-user-pass Name: db-user-pass Namespace: default Labels: Annotations: Type: Opaque Data password.txt: 12 bytes username.txt: 5 bytes 4. 解码secret的加密内容 ```shell $ kubectl get secret mysecret -o yaml apiVersion: v1 data: username: YWRtaW4= password: MWYyZDFlMmU2N2Rm kind: Secret metadata: creationTimestamp: 2016-01-22T18:41:56Z name: mysecret namespace: default resourceVersion: \"164619\" selfLink: /api/v1/namespaces/default/secrets/mysecret uid: cfee02d6-c137-11e5-8d73-42010af00002 type: Opaque $ echo \"MWYyZDFlMmU2N2Rm\" | base64 --decode 1f2d1e2e67df 通过yaml编排文件来创建secret # 先把内容用base64转换一下 $ echo -n \"admin\" | base64 YWRtaW4= $ echo -n \"1f2d1e2e67df\" | base64 MWYyZDFlMmU2N2Rm 然后创建如下yaml文件 apiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: username: YWRtaW4= password: MWYyZDFlMmU2N2Rm 用kubectl创建secret $ kubectl create -f ./secret.yaml secret \"mysecret\" created 在pod中使用secret apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mypod image: redis volumeMounts: - name: foo mountPath: \"/etc/foo\" readOnly: true volumes: - name: foo secret: secretName: mysecret username.txt和password.txt会出现在/etc/foo/username.txt, /etc/foo/password.txt路径下 你也可以为username指定不同的路径，用法如下(spec.volumes[].secret.items) apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mypod image: redis volumeMounts: - name: foo mountPath: \"/etc/foo\" readOnly: true volumes: - name: foo secret: secretName: mysecret items: - key: username path: my-group/my-username 此时， username的值会出现在/etc/foo/my-group/my-username下，而不是/etc/foo/username路径 为secret文件分配读写权限(spec.volumes[].secret.defaultMode) apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mypod image: redis volumeMounts: - name: foo mountPath: \"/etc/foo\" volumes: - name: foo secret: secretName: mysecret defaultMode: 256 注意这里的256，转成8进制就是0400， JSON不支持8进制，所以在指定值得时候要转换成10进制 如何在pod里读取挂载的secret的内容, secret的内容挂载到pod里之后变成了文件，直接在挂载的对应路径下读取即可 $ ls /etc/foo/ username password $ cat /etc/foo/username admin $ cat /etc/foo/password 1f2d1e2e67df 挂载到pod里的secret的内容， 会进行自动更新， 如果你更新了secret对象的内容， pod里对应的内容也会更新。 内容更新的时间为kubelet 的sync周期 + secret cache的ttl时间。 将Secret作为环境变量使用(env[x].valueFrom.secretKeyRef) apiVersion: v1 kind: Pod metadata: name: secret-env-pod spec: containers: - name: mycontainer image: redis env: - name: SECRET_USERNAME valueFrom: secretKeyRef: name: mysecret key: username - name: SECRET_PASSWORD valueFrom: secretKeyRef: name: mysecret key: password restartPolicy: Never 从环境变量中获取Secret的值 $ echo $SECRET_USERNAME admin $ echo $SECRET_PASSWORD 1f2d1e2e67df 使用 imagePullSecrets imagePullSecret是一个secret， 它会把docker registry的密码传递给kubelet，让kubelet代表pod下载镜像 你可以从创建一个imagePullSecrets， 并在serviceaccount中指向这个imagePullSecrets，所有使用这个serviceaccount的pod， 会自动设置它的imagePullSecret 字段 一些限制 secret需要先于使用它的pod创建， 否则pod的启动不会成功 secret只能被相同namespace的pod使用 单个secret的大小最大为1MB， 这个限制的出发点是为了避免过大的secret的会耗尽apiserver和kubelet的内存， 因此不建议创建非常大的secret。 然而创建大量小的secret对象仍然会耗尽内存。 这个目前无法对其限制。 对secret使用内存的复杂限制是规划中的一个特性。 kubelet目前只支持从apiserver创建的pod使用secret， 包括kubectl直接创建或者从复制控制器中的非直接创建方式。 其它方式，如使用 kubelet的 --manifest-url或 --config参数创建的pod，不能使用secret。 secretKeyRef指定的key如果不存在， 那么pod的启动不会成功 secret和pod的生命周期的交互 pod通过api创建时， 这时不会检查引用的secret是否存在 当pod被调度时，宿主机上的kubelet就会检查secret的值 当secret不能被获取时， kubelet会做周期性的尝试， 并会记录一个event说明为什么没有启动 当secret被获取时， kubelet就会创建和挂载一个包含这个secret的存储卷 在所有的卷被创建和挂载之前， pod不会启动 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"storage/subpath的使用.html":{"url":"storage/subpath的使用.html","title":"subpath的使用","keywords":"","body":"k8s允许我们将不同类型的volume挂载到容器的特定目录下。例如，我们可以将configmap的数据以volume的形式挂到容器下。 定义一个configmap，其中的数据以 key:value 的格式体现。 apiVersion: v1 kind: ConfigMap metadata: name: special-config namespace: default data: special.level: very special.type: |- property.1=value-1 property.2=value-2 property.3=value-3 创建一个Pod，其挂载上面定义的cm，并在启动时查看挂载目录 /etc/config/ 下有哪些文件。 apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: test-container image: k8s.gcr.io/busybox command: [ \"/bin/sh\", \"-c\", \"ls /etc/config/\" ] volumeMounts: - name: config-volume mountPath: /etc/config volumes: - name: config-volume configMap: # Provide the name of the ConfigMap containing the files you want # to add to the container name: special-config restartPolicy: Never 这里有几点需要注意: 挂载目录下的文件名称，即为cm定义里的key值。 挂载目录下的文件的内容，即为cm定义里的value值。value可以多行定义，这在一些稍微复杂的场景下特别有用，比如 my.cnf。 如果挂载目录下原来有文件，挂载后将不可见（AUFS）。 有的时候，我们希望将文件挂载到某个目录，但希望只是挂载该文件，不要影响挂载目录下的其他文件。 这个时候就可以用subPath: Path within the volume from which the container's volume should be mounted。 subPath 的目的是为了在单一Pod中多次使用同一个volume而设计的。 例如: 像下面的LAMP，可以将同一个volume下的 mysql 和 html目录，挂载到不同的挂载点上，这样就不需要为 mysql 和 html 单独创建volume了。 apiVersion: v1 kind: Pod metadata: name: my-lamp-site spec: containers: - name: mysql image: mysql env: - name: MYSQL_ROOT_PASSWORD value: \"rootpasswd\" volumeMounts: - mountPath: /var/lib/mysql name: site-data subPath: mysql - name: php image: php:7.0-apache volumeMounts: - mountPath: /var/www/html name: site-data subPath: html volumes: - name: site-data persistentVolumeClaim: claimName: my-lamp-site-data containers: - volumeMounts: - name: demo-config mountPath: /etc/special.type subPath: special.type volumes: - name: demo-config configMap: name: special-config 参考 https://kubernetes.io/docs/concepts/storage/volumes/#using-subpath Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"network/k8s-网络.html":{"url":"network/k8s-网络.html","title":"k8s-网络","keywords":"","body":" 网络要解决的问题 网络方案 CNI VS CNM CNI阵营 CNM阵营 k8s支持的网络解决方案 网络要解决的问题 Kubernetes中将容器的联网通过插件的方式来实现，那么该如何解决容器的联网问题呢？ 如果在本地单台机器上运行docker容器的话会注意到所有容器都会处在docker0网桥自动分配的一个网络IP段内（172.17.0.1/16）。该值可以通过docker启动参数--bip来设置。这样所有本地的所有的容器都拥有了一个IP地址，而且还是在一个网段内彼此就可以互相通信了。 但是Kubernetes管理的是集群，Kubernetes中的网络要解决的核心问题就是每台主机的IP地址网段划分，以及单个容器的IP地址分配。概括为： 保证每个Pod拥有一个集群内唯一的IP地址 保证不同节点的IP地址划分不会重复 保证跨节点的Pod可以互相通信 保证不同节点的Pod可以与跨节点的主机互相通信 网络方案 在网络概念上，Kubernetes中有两种核心IP： POD IP：有CNI实现提供，Kubernetes不管这个IP是否可达，只负责使用这个IP实现配置iptables、做健康检查等功能。默认情况下，这个IP在Kubernetes集群范围内都是可达的，并且可以进行ping等操作。 cluster IP：即服务IP，这个IP在Kubernetes中只是用于实现服务交互通信，本质上只是iptables上的几条DNAT规则。默认情况下，这个IP上只能提供服务端口的访问，且不可ping。 k8s本身不集成网络， 它需要外部的插件支持。 它支持cni网络模型，基于cni网络模型的常用的解决方案有 weave，flannel， calico, openvswitch等。 各种技术方案的部署难易程度不通， 性能上也有所差别。 k8s对集群的网络有如下要求： 所有的容器都可以在不用nat的情况下和别的容器通信 所有的节点都可以在不用nat的情况下同所有容器通信， 反之亦然 容器的地址和别人看到的地址是同一个地址 kubernetes的网络通信中和原生的docker有些不一样， 它有四个问题要解决： container-to-container， 容器到容器的访问（这个是由pod来解决， 同一个pod的容器共享同一网络空间） Pod-to-Pod ， pod到pod的通信（这个由各种不同的网络方案，如flannel等来处理） Pod-to-Service ， pod到service的通信（这个由service组件， 即kubeproxy处理） External-to-Service（这个由service组件， 即kubeproxy处理） CNI VS CNM 首先，需要先说说网络方案的两大阵营：CNN与CNM Kubernetes所选择的CNI网络接口，简单介绍下网络实现的背景。 CNI阵营 CNI即Container Network Interface，是一套容器网络的定义规范，包括方法规范、参数规范、响应规范等等。CNI只要求在容器创建时为容器分配网络资源、删除容器时释放网络资源。CNI与调用者之间的整个交互过程如下图所示： CNI实现与外界的交互都通过进程参数和环境变量传递，也只要求输出结果符合CNI规范即可，与实现语言也没什么特殊要求。比如Calico早期版本就使用Python实现了CNI规范，为Kubernetes提供了网络实现。常见的环境变量设置如下： CNI_COMMAND：调用指定CNI动作，ADD表示增加网卡，DEL表示释放网卡 CNI_CONTAINERID：容器ID CNI_NETNS：容器网络命名空间文件位置 CNI_ARGS：额外传递的参数 CNI_IFNAME：设置的容器网卡名称，如eth0 正因如此，CNI规范实现起来非常容易扩展，除了CNI自带的Bridge、Macvlan等基本实现以外，还有大量的第三方实现可供选择，包括Calico、Romana、Flannel等常用实现。同时CNI支持多种容器运行时，包括Docker、rkt、Mesos、Hyper等容器引擎都可以使用。这也是Kubernetes选择使用CNI的一大重要原因。 CNM阵营 相对的，Docker提出的CNM（Cotainer Network Model）模型实现就比较复杂，但更为完善，比较接近传统的网络概念。如下图所示： Sandbox就是容器的网络命名空间，Endpoint为容器连接到网络中的一张网卡，而网络则是一组相互通信的Endpoint的集合，比较接近Neutron中的网络定义。 在CNM中，docker engine通过HTTP REST API调用网络实现，为容器配置网络。这些API接口涵盖网络管理、容器管理、创建endpoint等十几个接口。同时CNM模型还隐含在docker自身附带的service机制、dns机制等附加约束，因此可以在一定程度上说，CNM模型只是专为docker容器实现的，对别的容器运行时并不友好。 由于上面这些技术上的原因以及一些商业上的原因，Kubernetes最终选择了CNI作为自己的网络接口。 当然，Kubernetes也提供一些取巧的方法，将CNI接口转化为对CNM模型的调用，从而实现两种模型的通用。例如to_docker，这个脚本就将Kubernetes对CNI的调用转换为Docker CNM网络的对应操作，从而实现CNI到CNM的转换。 参考 基于Neutron的Kubernetes SDN实践经验之谈 k8s支持的网络解决方案 Kubernetes社区中，比较常见的几种网络实现主要是以下两种： 基于Overlay网络：以Flannel、Weave为代表。Flannel是CoreOS为Kubernetes专门定制实现的Overlay网络方案，也是Kubernetes默认的网络实现。它基于VXLAN或者UDP整个集群的Overlay网络，从而实现容器在集群上的通信，满足Kubernetes网络模型的三大基本约束。由于在通信过程中存在数据包的封包解包等额外损耗，性能较差，但已经基本满足使用。 以L3路由为基础实现网络：以Calico、Romana为代表。其中，Calico是广泛流传的性能最好的Kubernetes网络实现，基于纯三层的路由实现网络通信，结合iptables实现的安全控制，可以满足大多数云的性能需求。但是由于它要求主机上必须打开BGP形成路由拓扑，在一些数据中心上可能不会被允许。同时，Calico还比较早地支持了Network Policy，并且可以将Calico自身的数据直接托管在Kubernetes中，从而实现与Kubernetes的深度集成。 kubernetes的网络通信中和原生的docker有些不一样， 它有四个问题要解决： container-to-container， 容器到容器的访问（这个是由pod来解决， 同一个pod的容器共享同一网络空间） Pod-to-Pod ， pod到pod的通信（这个由各种不同的网络方案，如flannel等来处理） Pod-to-Service ， pod到service的通信（这个由service组件， 即kubeproxy处理） External-to-Service（这个由service组件， 即kubeproxy处理） 为了解决该问题，出现了一系列开源的Kubernetes中的网络插件与方案，如： flannel calico contiv weave net kube-router cilium canal 参考资料： 官方文档之网络 Kubernetes网络插件CNI学习整理 Kubernetes-基于flannel的集群网络 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"network/开启ipvs.html":{"url":"network/开启ipvs.html","title":"开启ipvs","keywords":"","body":" 要保证有rr模块 启用 ipvs 后与 1.7 版本的配置差异如下： 要保证有rr模块 [root@node1 flannel]# lsmod | grep ip_vs ip_vs_sh 12688 0 ip_vs_wrr 12697 0 ip_vs_rr 12600 13 ip_vs 141092 19 ip_vs_rr,ip_vs_sh,ip_vs_wrr nf_conntrack 133387 7 ip_vs,nf_nat,nf_nat_ipv4,xt_conntrack,nf_nat_masquerade_ipv4,nf_conntrack_netlink,nf_conntrack_ipv4 libcrc32c 12644 3 ip_vs,nf_nat,nf_conntrack 没开启加载方式： modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack_ipv4 启用 ipvs 后与 1.7 版本的配置差异如下： 增加 –feature-gates=SupportIPVSProxyMode=true 选项，用于告诉 kube-proxy 开启 ipvs 支持，目前 ipvs 默认未开启 增加 ipvs-min-sync-period、–ipvs-sync-period、–ipvs-scheduler 三个参数用于调整 ipvs，具体参数值请自行查阅 ipvs 文档 增加 –masquerade-all 选项，以确保反向流量通过 打开 ipvs 需要安装 ipvsadm 软件， 在 node 中安装 yum install ipvsadm -y ipvsadm -L -n 重点说一下 –masquerade-all 选项: kube-proxy ipvs 是基于 NAT 实现的，当创建一个 service 后，kubernetes 会在每个节点上创建一个网卡，同时帮你将 Service IP(VIP) 绑定上，此时相当于每个 Node 都是一个 ds，而其他任何 Node 上的 Pod，甚至是宿主机服务(比如 kube-apiserver 的 6443)都可能成为 rs； 按照正常的 lvs nat 模型，所有 rs 应该将 ds 设置成为默认网关，以便数据包在返回时能被 ds 正确修改； 在 kubernetes 将 vip 设置到每个 Node 后，默认路由显然不可行，所以要设置 –masquerade-all 选项，以便反向数据包能通过。 注意：--masquerade-all 选项与 Calico 安全策略控制不兼容,请酌情使用 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"network/flannel.html":{"url":"network/flannel.html","title":"Flannel","keywords":"","body":" Flannel Flannel原理 Docker集成 CNI集成 Kubernetes集成 优点 缺点 Flannel Flannel通过给每台宿主机分配一个子网的方式为容器提供虚拟网络，它基于Linux TUN/TAP，使用UDP封装IP包来创建overlay网络，并借助etcd维护网络的分配情况。 Flannel原理 控制平面上host本地的flanneld负责从远端的ETCD集群同步本地和其它host上的subnet信息，并为POD分配IP地址。数据平面flannel通过Backend（比如UDP封装）来实现L3 Overlay，既可以选择一般的TUN设备又可以选择VxLAN设备。 { \"Network\": \"10.0.0.0/8\", \"SubnetLen\": 20, \"SubnetMin\": \"10.10.0.0\", \"SubnetMax\": \"10.99.0.0\", \"Backend\": { \"Type\": \"udp\", \"Port\": 7890 } } 除了UDP，Flannel还支持很多其他的Backend： udp：使用用户态udp封装，默认使用8285端口。由于是在用户态封装和解包，性能上有较大的损失 vxlan：vxlan封装，需要配置VNI，Port（默认8472）和GBP host-gw：直接路由的方式，将容器网络的路由信息直接更新到主机的路由表中，仅适用于二层直接可达的网络 aws-vpc：使用 Amazon VPC route table 创建路由，适用于AWS上运行的容器 gce：使用Google Compute Engine Network创建路由，所有instance需要开启IP forwarding，适用于GCE上运行的容器 ali-vpc：使用阿里云VPC route table 创建路由，适用于阿里云上运行的容器 Docker集成 source /run/flannel/subnet.env docker daemon --bip=${FLANNEL_SUBNET} --mtu=${FLANNEL_MTU} & CNI集成 CNI flannel插件会将flannel网络配置转换为bridge插件配置，并调用bridge插件给容器netns配置网络。比如下面的flannel配置 { \"name\": \"mynet\", \"type\": \"flannel\", \"delegate\": { \"bridge\": \"mynet0\", \"mtu\": 1400 } } 会被cni flannel插件转换为 { \"name\": \"mynet\", \"type\": \"bridge\", \"mtu\": 1472, \"ipMasq\": false, \"isGateway\": true, \"ipam\": { \"type\": \"host-local\", \"subnet\": \"10.1.17.0/24\" } } Kubernetes集成 使用flannel前需要配置kube-controller-manager --allocate-node-cidrs=true --cluster-cidr=10.244.0.0/16。 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 这会启动flanneld容器，并配置CNI网络插件： $ ps -ef | grep flannel | grep -v grep root 3625 3610 0 13:57 ? 00:00:00 /opt/bin/flanneld --ip-masq --kube-subnet-mgr root 9640 9619 0 13:51 ? 00:00:00 /bin/sh -c set -e -x; cp -f /etc/kube-flannel/cni-conf.json /etc/cni/net.d/10-flannel.conf; while true; do sleep 3600; done $ cat /etc/cni/net.d/10-flannel.conf { \"name\": \"cbr0\", \"type\": \"flannel\", \"delegate\": { \"isDefaultGateway\": true } } flanneld自动连接kubernetes API，根据node.Spec.PodCIDR配置本地的flannel网络子网，并为容器创建vxlan和相关的子网路由。 $ cat /run/flannel/subnet.env FLANNEL_NETWORK=10.244.0.0/16 FLANNEL_SUBNET=10.244.0.1/24 FLANNEL_MTU=1410 FLANNEL_IPMASQ=true $ ip -d link show flannel.1 12: flannel.1: mtu 1410 qdisc noqueue state UNKNOWN mode DEFAULT group default link/ether 8e:5a:0d:07:0f:0d brd ff:ff:ff:ff:ff:ff promiscuity 0 vxlan id 1 local 10.146.0.2 dev ens4 srcport 0 0 dstport 8472 nolearning ageing 300 udpcsum addrgenmode eui64 优点 配置安装简单，使用方便 与云平台集成较好，VPC的方式没有额外的性能损失 缺点 VXLAN模式对zero-downtime restarts支持不好 When running with a backend other than udp, the kernel is providing the data path with flanneld acting as the control plane. As such, flanneld can be restarted (even to do an upgrade) without disturbing existing flows. However in the case of vxlan backend, this needs to be done within a few seconds as ARP entries can start to timeout requiring the flannel daemon to refresh them. Also, to avoid interruptions during restart, the configuration must not be changed (e.g. VNI, --iface values). 参考文档 https://github.com/coreos/flannel https://coreos.com/flannel/docs/latest/ Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"network/weave.html":{"url":"network/weave.html","title":"Weave Net","keywords":"","body":" Weave Net Weave Kubernetes Weave Scope 优点 缺点 Weave Net Weave Net是一个多主机容器网络方案，支持去中心化的控制平面，各个host上的wRouter间通过建立Full Mesh的TCP链接，并通过Gossip来同步控制信息。这种方式省去了集中式的K/V Store，能够在一定程度上减低部署的复杂性，Weave将其称为“data centric”，而非RAFT或者Paxos的“algorithm centric”。 数据平面上，Weave通过UDP封装实现L2 Overlay，封装支持两种模式： 运行在user space的sleeve mode：通过pcap设备在Linux bridge上截获数据包并由wRouter完成UDP封装，支持对L2 traffic进行加密，还支持Partial Connection，但是性能损失明显。 运行在kernal space的 fastpath mode：即通过OVS的odp封装VxLAN并完成转发，wRouter不直接参与转发，而是通过下发odp 流表的方式控制转发，这种方式可以明显地提升吞吐量，但是不支持加密等高级功能。 Sleeve Mode: Fastpath Mode: 关于Service的发布，weave做的也比较完整。首先，wRouter集成了DNS功能，能够动态地进行服务发现和负载均衡，另外，与libnetwork 的overlay driver类似，weave要求每个POD有两个网卡，一个就连在lb/ovs上处理L2 流量，另一个则连在docker0上处理Service流量，docker0后面仍然是iptables作NAT。 Weave已经集成了主流的容器系统 Docker: https://www.weave.works/docs/net/latest/plugin/ Kubernetes: https://www.weave.works/docs/net/latest/kube-addon/ kubectl apply -f https://git.io/weave-kube CNI: https://www.weave.works/docs/net/latest/cni-plugin/ Prometheus: https://www.weave.works/docs/net/latest/metrics/ Weave Kubernetes kubectl apply -n kube-system -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\" 这会在所有Node上启动Weave插件以及Network policy controller： $ ps -ef | grep weave | grep -v grep root 25147 25131 0 16:22 ? 00:00:00 /bin/sh /home/weave/launch.sh root 25204 25147 0 16:22 ? 00:00:00 /home/weave/weaver --port=6783 --datapath=datapath --host-root=/host --http-addr=127.0.0.1:6784 --status-addr=0.0.0.0:6782 --docker-api= --no-dns --db-prefix=/weavedb/weave-net --ipalloc-range=10.32.0.0/12 --nickname=ubuntu-0 --ipalloc-init consensus=2 --conn-limit=30 --expect-npc 10.146.0.2 10.146.0.3 root 25669 25654 0 16:22 ? 00:00:00 /usr/bin/weave-npc 这样，容器网络为 所有容器都连接到weave网桥 weave网桥通过veth pair连到内核的openvswitch模块 跨主机容器通过openvswitch vxlan通信 policy controller通过配置iptables规则为容器设置网络策略 Weave Scope Weave Scope是一个容器监控和故障排查工具，可以方便的生成整个集群的拓扑并智能分组（Automatic Topologies and Intelligent Grouping）。 Weave Scope主要由scope-probe和scope-app组成 +--Docker host----------+ | +--Container------+ | .---------------. | | | | | Browser | | | +-----------+ | | |---------------| | | | scope-app |优点 去中心化 故障自动恢复 加密通信 Multicast networking 缺点 UDP模式性能损失较大 参考文档 https://github.com/weaveworks/weave https://www.weave.works/products/weave-net/ https://github.com/weaveworks/scope https://www.weave.works/guides/monitor-docker-containers/ http://www.sdnlab.com/17141.html Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"network/contiv.html":{"url":"network/contiv.html","title":"Contiv","keywords":"","body":" Contiv Kubernetes集成 Contiv Contiv是思科开源的容器网络方案，是一个用于跨虚拟机、裸机、公有云或私有云的异构容器部署的开源容器网络架构，并与主流容器编排系统集成。Contiv最主要的优势是直接提供了多租户网络，并支持L2(VLAN), L3(BGP), Overlay (VXLAN)以及思科自家的ACI。 主要特征 原生的Tenant支持，一个Tenant就是一个virtual routing and forwarding (VRF) 两种网络模式 L2 VLAN Bridged Routed network, e.g. vxlan, BGP, ACI Network Policy，如Bandwidth, Isolation等 Kubernetes集成 Ansible部署见https://github.com/kubernetes/contrib/tree/master/ansible/roles/contiv。 export VERSION=1.0.0-beta.3 curl -L -O https://github.com/contiv/install/releases/download/$VERSION/contiv-$VERSION.tgz tar xf contiv-$VERSION.tgz cd ~/contiv/contiv-$VERSION/install/k8s netctl --netmaster http://$netmaster:9999 global set --fwd-mode routing cd ~/contiv/contiv-$VERSION install/k8s/install.sh -n 10.87.49.77 -v b -w routing # check contiv pods export NETMASTER=http://10.87.49.77:9999 netctl global info # create a network # netctl network create --encap=vlan --pkt-tag=3280 --subnet=10.100.100.215-10.100.100.220/27 --gateway=10.100.100.193 vlan3280 netctl net create -t default --subnet=20.1.1.0/24 default-net # create BGP connections to each of the nodes netctl bgp create devstack-77 --router-ip=\"30.30.30.77/24\" --as=\"65000\" --neighbor-as=\"65000\" --neighbor=\"30.30.30.2\" netctl bgp create devstack-78 --router-ip=\"30.30.30.78/24\" --as=\"65000\" --neighbor-as=\"65000\" --neighbor=\"30.30.30.2\" netctl bgp create devstack-71 --router-ip=\"30.30.30.79/24\" --as=\"65000\" --neighbor-as=\"65000\" --neighbor=\"30.30.30.2\" # then create pod with label \"io.contiv.network\" 参考文档 http://contiv.github.io/ https://github.com/contiv/netplugin http://blogs.cisco.com/cloud/introducing-contiv-1-0 Kubernetes and Contiv on Bare-Metal with L3/BGP Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"network/calico.html":{"url":"network/calico.html","title":"Calico","keywords":"","body":" Calico Calico 架构 IP-in-IP Calico CNI Calico CNM Calico Kubernetes Calico 的不足 Calico Calico 是一个纯三层的数据中心网络方案（不需要 Overlay），并且与 OpenStack、Kubernetes、AWS、GCE 等 IaaS 和容器平台都有良好的集成。 Calico 在每一个计算节点利用 Linux Kernel 实现了一个高效的 vRouter 来负责数据转发，而每个 vRouter 通过 BGP 协议负责把自己上运行的 workload 的路由信息像整个 Calico 网络内传播——小规模部署可以直接互联，大规模下可通过指定的 BGP route reflector 来完成。 这样保证最终所有的 workload 之间的数据流量都是通过 IP 路由的方式完成互联的。Calico 节点组网可以直接利用数据中心的网络结构（无论是 L2 或者 L3），不需要额外的 NAT，隧道或者 Overlay Network。 此外，Calico 基于 iptables 还提供了丰富而灵活的网络 Policy，保证通过各个节点上的 ACLs 来提供 Workload 的多租户隔离、安全组以及其他可达性限制等功能。 Calico 架构 Calico 主要由 Felix、etcd、BGP client 以及 BGP Route Reflector 组成 Felix，Calico Agent，跑在每台需要运行 Workload 的节点上，主要负责配置路由及 ACLs 等信息来确保 Endpoint 的连通状态； etcd，分布式键值存储，主要负责网络元数据一致性，确保 Calico 网络状态的准确性； BGP Client（BIRD）, 主要负责把 Felix 写入 Kernel 的路由信息分发到当前 Calico 网络，确保 Workload 间的通信的有效性； BGP Route Reflector（BIRD），大规模部署时使用，摒弃所有节点互联的 mesh 模式，通过一个或者多个 BGP Route Reflector 来完成集中式的路由分发。 calico/calico-ipam，主要用作 Kubernetes 的 CNI 插件 IP-in-IP Calico 控制平面的设计要求物理网络得是 L2 Fabric，这样 vRouter 间都是直接可达的，路由不需要把物理设备当做下一跳。为了支持 L3 Fabric，Calico 推出了 IPinIP 的选项。 Calico CNI 见 https://github.com/projectcalico/cni-plugin。 Calico CNM Calico 通过 Pool 和 Profile 的方式实现了 docker CNM 网络： Pool，定义可用于 Docker Network 的 IP 资源范围，比如：10.0.0.0/8 或者 192.168.0.0/16； Profile，定义 Docker Network Policy 的集合，由 tags 和 rules 组成；每个 Profile 默认拥有一个和 Profile 名字相同的 Tag，每个 Profile 可以有多个 Tag，以 List 形式保存。 具体实现见 https://github.com/projectcalico/libnetwork-plugin，而使用方法可以参考 https://docs.projectcalico.org/master/getting-started/docker/。 Calico Kubernetes 对于使用 kubeadm 创建的 Kubernetes 集群，使用以下配置安装 calico 时需要配置 --pod-network-cidr=192.168.0.0/16 --service-cidr=10.96.0.0/12 （不能与 Calico 网络重叠） 然后运行 kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml 更详细的自定义配置方法见 https://docs.projectcalico.org/v3.0/getting-started/kubernetes。 这会在 Pod 中启动 Calico-etcd，在所有 Node 上启动 bird6、felix 以及 confd，并配置 CNI 网络为 calico 插件： # Calico 相关进程 $ ps -ef | grep calico | grep -v grep root 9012 8995 0 14:51 ? 00:00:00 /bin/sh -c /usr/local/bin/etcd --name=calico --data-dir=/var/etcd/calico-data --advertise-client-urls=http://$CALICO_ETCD_IP:6666 --listen-client-urls=http://0.0.0.0:6666 --listen-peer-urls=http://0.0.0.0:6667 root 9038 9012 0 14:51 ? 00:00:01 /usr/local/bin/etcd --name=calico --data-dir=/var/etcd/calico-data --advertise-client-urls=http://10.146.0.2:6666 --listen-client-urls=http://0.0.0.0:6666 --listen-peer-urls=http://0.0.0.0:6667 root 9326 9325 0 14:51 ? 00:00:00 bird6 -R -s /var/run/calico/bird6.ctl -d -c /etc/calico/confd/config/bird6.cfg root 9327 9322 0 14:51 ? 00:00:00 confd -confdir=/etc/calico/confd -interval=5 -watch --log-level=debug -node=http://10.96.232.136:6666 -client-key= -client-cert= -client-ca-keys= root 9328 9324 0 14:51 ? 00:00:00 bird -R -s /var/run/calico/bird.ctl -d -c /etc/calico/confd/config/bird.cfg root 9329 9323 1 14:51 ? 00:00:04 calico-felix # CNI 网络插件配置 $ cat /etc/cni/net.d/10-calico.conf { \"name\": \"k8s-pod-network\", \"cniVersion\": \"0.1.0\", \"type\": \"calico\", \"etcd_endpoints\": \"http://10.96.232.136:6666\", \"log_level\": \"info\", \"ipam\": { \"type\": \"calico-ipam\" }, \"policy\": { \"type\": \"k8s\", \"k8s_api_root\": \"https://10.96.0.1:443\", \"k8s_auth_token\": \"\" }, \"kubernetes\": { \"kubeconfig\": \"/etc/cni/net.d/calico-kubeconfig\" } } $ cat /etc/cni/net.d/calico-kubeconfig # Kubeconfig file for Calico CNI plugin. apiVersion: v1 kind: Config clusters: - name: local cluster: insecure-skip-tls-verify: true users: - name: calico contexts: - name: calico-context context: cluster: local user: calico current-context: calico-context Calico 的不足 既然是三层实现，当然不支持 VRF 不支持多租户网络的隔离功能，在多租户场景下会有网络安全问题 Calico 控制平面的设计要求物理网络得是 L2 Fabric，这样 vRouter 间都是直接可达的 参考文档 https://xuxinkun.github.io/2016/07/22/cni-cnm/ https://www.projectcalico.org/ http://blog.dataman-inc.com/shurenyun-docker-133/ Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"network/ovn-kubernetes.html":{"url":"network/ovn-kubernetes.html","title":"OVN","keywords":"","body":" Open Virtual Networking (OVN) Overlay模式 配置master 配置Node CNI插件原理 Underlay模式 OVN 安装方法 参考文档 Open Virtual Networking (OVN) ovn-kubernetes 提供了一个ovs OVN 网络插件，支持 underlay 和 overlay 两种模式。 underlay：容器运行在虚拟机中，而ovs则运行在虚拟机所在的物理机上，OVN将容器网络和虚拟机网络连接在一起 overlay：OVN通过logical overlay network连接所有节点的容器，此时ovs可以直接运行在物理机或虚拟机上 Overlay模式 配置master # start ovn /usr/share/openvswitch/scripts/ovn-ctl start_northd /usr/share/openvswitch/scripts/ovn-ctl start_controller # start ovnkube nohup sudo ovnkube -k8s-kubeconfig kubeconfig.yaml -net-controller \\ -loglevel=4 \\ -k8s-apiserver=\"http://$CENTRAL_IP:8080\" \\ -logfile=\"/var/log/openvswitch/ovnkube.log\" \\ -init-master=$NODE_NAME -cluster-subnet=\"$CLUSTER_IP_SUBNET\" \\ -service-cluster-ip-range=$SERVICE_IP_SUBNET \\ -nodeport \\ -nb-address=\"tcp://$CENTRAL_IP:6631\" \\ -sb-address=\"tcp://$CENTRAL_IP:6632\" 2>&1 & 配置Node nohup sudo ovnkube -k8s-kubeconfig kubeconfig.yaml -loglevel=4 \\ -logfile=\"/var/log/openvswitch/ovnkube.log\" \\ -k8s-apiserver=\"http://$CENTRAL_IP:8080\" \\ -init-node=\"$NODE_NAME\" \\ -nodeport \\ -nb-address=\"tcp://$CENTRAL_IP:6631\" \\ -sb-address=\"tcp://$CENTRAL_IP:6632\" -k8s-token=\"$TOKEN\" \\ -init-gateways \\ -service-cluster-ip-range=$SERVICE_IP_SUBNET \\ -cluster-subnet=$CLUSTER_IP_SUBNET 2>&1 & CNI插件原理 ADD操作 从ovn annotation获取ip/mac/gateway 在容器netns中配置接口和路由 添加ovs端口 ovs-vsctl add-port br-int veth_outside \\ --set interface veth_outside \\ external_ids:attached_mac=mac_address \\ external_ids:iface-id=namespace_pod \\ external_ids:ip_address=ip_address DEL操作 ovs-vsctl del-port br-int port Underlay模式 暂未实现。 OVN 安装方法 所有节点配置安装源并安装公共依赖 sudo apt-get install apt-transport-https echo \"deb https://packages.wand.net.nz $(lsb_release -sc) main\" | sudo tee /etc/apt/sources.list.d/wand.list sudo curl https://packages.wand.net.nz/keyring.gpg -o /etc/apt/trusted.gpg.d/wand.gpg sudo apt-get update sudo apt-get build-dep dkms sudo apt-get install python-six openssl python-pip -y sudo -H pip install --upgrade pip sudo apt-get install openvswitch-datapath-dkms -y sudo apt-get install openvswitch-switch openvswitch-common -y sudo -H pip install ovs Master 节点安装 ovn-central sudo apt-get install ovn-central ovn-common ovn-host -y Node 节点安装 ovn-host sudo apt-get install ovn-host ovn-common -y 参考文档 https://github.com/openvswitch/ovn-kubernetes Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"network/romana.html":{"url":"network/romana.html","title":"Romana","keywords":"","body":" Romana Kubernetes部署 工作原理 优点 缺点 Romana Romana是Panic Networks在2016年提出的开源项目，旨在解决Overlay方案给网络带来的开销。 Kubernetes部署 对使用kubeadm部署的Kubernetes集群： kubectl apply -f https://raw.githubusercontent.com/romana/romana/master/docs/kubernetes/romana-kubeadm.yml 对使用kops部署的Kubernetes集群: kubectl apply -f https://raw.githubusercontent.com/romana/romana/master/docs/kubernetes/romana-kops.yml 使用kops时要注意 设置网络插件使用CNI --networking cni 对于aws还提供romana-aws和romana-vpcrouter自动配置Node和Zone之间的路由 工作原理 layer 3 networking，消除overlay带来的开销 基于iptables ACL的网络隔离 基于hierarchy CIDR管理Host/Tenant/Segment ID 优点 纯三层网络，性能好 缺点 基于IP管理租户，有规模上的限制 物理设备变更或地址规划变更麻烦 参考文档 http://romana.io/ Romana basics Romana Github Romana 2.0 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"network/opencontrail.html":{"url":"network/opencontrail.html","title":"OpenContrail","keywords":"","body":"OpenContrail OpenContrail是Juniper推出的开源网络虚拟化平台，其商业版本为Contrail。 架构 OpenContrail主要由控制器和vRouter组成： 控制器提供虚拟网络的配置、控制和分析功能 vRouter提供分布式路由，负责虚拟路由器、虚拟网络的建立以及数据转发 vRouter支持三种模式 Kernel vRouter：类似于ovs内核模块 DPDK vRouter：类似于ovs-dpdk Netronome Agilio Solution (商业产品)：支持DPDK, SR-IOV and Express Virtio (XVIO) 参考文档 http://www.opencontrail.org/opencontrail-architecture-documentation/ http://www.opencontrail.org/network-virtualization-architecture-deep-dive/ Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"network/网络策略.html":{"url":"network/网络策略.html","title":"网络策略","keywords":"","body":" 网络策略 网络策略 网络策略（Network Policy）是一个特定的配置， 用于限制一组pod如何和另外一个pod或网络endpoint进行通信。 网络策略使用标签(label)来选择pods，并定义规则， 指定那些流量可以达到哪些pods。 前提： 要是网络策略生效， 必须要安装支持网络策略的网络插件，否则网络策略不会生效 默认情况下， 所有的pod都是非隔离，意味着它会放行所有来源的流量。 但是对于应用了网络策略的pod。它就变成隔离的， 它会拒绝所有没有被网络策略放行的连接。 示例： apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: test-network-policy namespace: default spec: podSelector: matchLabels: role: db policyTypes: - Ingress - Egress ingress: - from: - ipBlock: cidr: 172.17.0.0/16 except: - 172.17.1.0/24 - namespaceSelector: matchLabels: project: myproject - podSelector: matchLabels: role: frontend ports: - protocol: TCP port: 6379 egress: - to: - ipBlock: cidr: 10.0.0.0/24 ports: - protocol: TCP port: 5978 下面对这个网络策略进行说明： 强制必须有的字段： apiVersion，kind， metadata spec: 包含定义网络策略的所有相关信息 podSelector： 选择目的pods， 如果为空， 则选择所有的pods policyTypes： 可选值为Ingress, Egress或二者， 如果为空， 则Ingress会被设定， 当有egress规则时， Egress类型也会被设定 ingress：入向规则的白名单规则列表， 每条规则放行匹配到from和ports的流量。流量的源要么要么来自namespaceSelector或者来自podSelector的pod， 都会被匹配到。 egress： 出向规则的白名单规则列表。 每条规则放行匹配到to和ports的流量 ipBlock： ip块定义CIDR， except则把某段ip， 从大的ip 网段里排除掉 上述示例中的规则整体的意思是： 隔离pod标签为“role=db”， 命名空间为default的pods的入向和出向流量 允许pod标签为“role=frontend”， 命名空间为default的pods访问pod标签为“role=db”， 命名空间为default的pods的6379的TCP端口 允许namespace标签为“project=myproject”的pods访问pod标签为“role=db”， 命名空间为default的pods的6379的TCP端口 允许pod标签为“role=db”， 命名空间为default的pods访问10.0.0.0/24网段的TCP端口5978 当命名空间里没有任何的网络策略时， 默认情况下，所有pods的出流量和入流量都会被放行。 你可以可以定义一些规则来修改默认行为，比如： 默认拒绝所有的入流量 默认允许所有的入流量 默认拒绝所有的出流量 默认允许所有的出流量 默认拒绝所有的出流量和入流量 参考 官方文档之Network Policies Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"security/k8s-认证.html":{"url":"security/k8s-认证.html","title":"k8s-认证","keywords":"","body":"认证的几种方式 三种方式： 基本basic 令牌token 双向tls kubectl发出命令请求的流程 kubectl 通过以下顺序来找到 Kubeconfig 文件： 如果提供了 --kubeconfig 参数， Kubectl 就使用 --kubeconfig 参数提供的 Kubeconfig 文件 如果没有提供 --kubeconfig 参数，但设置了环境变量 $KUBECONFIG，则使用该环境变量提供的 Kubeconfig 文件 如果 --kubeconfig 参数和环境变量 $KUBECONFIG 都没有提供，Kubectl 就使用默认的 Kubeconfig 文件 $HOME/.kube/config 每次收到请求时，Apiserver 都会通过令牌链进行认证，直到某一个认证成功为止： x509 处理程序将验证 HTTP 请求是否是由 CA 根证书签名的 TLS 密钥进行编码的 bearer token 处理程序将验证 --token-auth-file 参数提供的 Token 文件是否存在 基本认证处理程序确保 HTTP 请求的基本认证凭证与本地的状态匹配 解析完 Kubeconfig 文件后，Kubectl 会确定当前要使用的上下文、当前指向的群集以及与当前用户关联的任何认证信息。如果用户提供了额外的参数（例如: --username），则优先使用这些参数覆盖 Kubeconfig 中指定的值。一旦拿到这些信息之后， Kubectl 就会把这些信息填充到将要发送的 HTTP 请求头中： x509 证书使用 tls.TLSConfig 发送（包括 CA 证书） bearer tokens 在 HTTP 请求头 Authorization 中发送 用户名和密码通过 HTTP 基本认证发送 OpenID 认证过程是由用户事先手动处理的，产生一个像 Bearer Token 一样被发送的 Token 认证 现在kubectl的请求已经成功发送了，接下来轮到 Kube-Apiserver 闪亮登场。 认证的定义 Kube-Apiserver 是客户端和系统组件用来保存和检索集群状态的主要接口。为了执行相应的功能，Kube-Apiserver 需要能够验证请求者是合法的，这个过程被称为认证。 认证的过程 当 Kube-Apiserver 第一次启动时，它会查看用户提供的所有 CLI 参数，并组合成一个合适的令牌列表。 如果提供了 --client-ca-file 参数，则会将 x509 客户端证书认证添加到令牌列表中；如果提供了 --token-auth-file 参数，则会将 breaer token 添加到令牌列表中。 每次收到请求时，Apiserver 都会通过令牌链进行认证，直到某一个认证成功为止： x509 处理程序将验证 HTTP 请求是否是由 CA 根证书签名的 TLS 密钥进行编码的 bearer token 处理程序将验证 --token-auth-file 参数提供的 Token 文件是否存在 基本认证处理程序确保 HTTP 请求的基本认证凭证与本地的状态匹配 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"security/基本认证.html":{"url":"security/基本认证.html","title":"基本认证","keywords":"","body":"为了使用这个方案，api－server需要使用－basic－auth－file＝选项来开启。HTTP_AUTH_FILE是个csv文件，每个用户入口都有下列格式：password，username，userid。目前，对AUTH_FILE的任意修都需要重新启动api－server。 例子： [root@master temp]# cat /etc/kubernetes/ssl/basic-auth.csv admin,admin_123,1 readonly,readonly_123,2 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"security/基于token的认证.html":{"url":"security/基于token的认证.html","title":"基于token的认证","keywords":"","body":"为了使用这个方案，api－server需要用－token－auth－file＝选项来开启。TOKEN_FILE是个csv文件，每个用户入口都有下列格式：token，user，userid，group。 Group的名字是随意的。 令牌文件的例子： [root@master temp]# head -c 16 /dev/urandom | od -An -t x | tr -d ' ' cdf6d36d697dae799728ac52531a43a8 [root@master temp]# vim /etc/kubernetes/ssl/bootstrap-token.csv [root@master temp]# cat /etc/kubernetes/ssl/bootstrap-token.csv cdf6d36d697dae799728ac52531a43a8,kubelet-bootstrap,10001,\"system:kubelet-bootstrap\" [root@master temp]# 基于令牌的身份验证面临的挑战就是，令牌是无期限的，而且对令牌清单做任何的修改都需要重新启动api－server。 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"security/k8s-授权.html":{"url":"security/k8s-授权.html","title":"k8s-授权","keywords":"","body":" 官方文档： 授权模式 鉴权的作用 原理图 官方文档： https://kubernetes.io/docs/reference/access-authn-authz/authorization/ 授权模式 webhook: 它与集群外的 HTTP(S) 服务交互。 ABAC: 它执行静态文件中定义的策略。 RBAC: 它使用 rbac.authorization.k8s.io API Group实现授权决策，允许管理员通过 Kubernetes API 动态配置策略。 Node: 它确保 Kubelet 只能访问自己节点上的资源。 鉴权的作用 鉴权的作用是，决定一个用户是否有权使用 Kubernetes API 做某些事情。它除了会影响 kubectl 等组件之外，还会对一些运行在集群内部并对集群进行操作的软件产生作用，例如使用了 Kubernetes 插件的 Jenkins，或者是利用 Kubernetes API 进行软件部署的 Helm。ABAC 和 RBAC 都能够对访问策略进行配置。 ABAC（Attribute Based Access Control）本来是不错的概念，但是在 Kubernetes 中的实现比较难于管理和理解（怪我咯），而且需要对 Master 所在节点的 SSH 和文件系统权限，而且要使得对授权的变更成功生效，还需要重新启动 API Server。 而 RBAC 的授权策略可以利用 kubectl 或者 Kubernetes API 直接进行配置。RBAC 可以授权给用户，让用户有权进行授权管理，这样就可以无需接触节点，直接进行授权管理。RBAC 在 Kubernetes 中被映射为 API 资源和操作。 原理图 在 RBAC 中定义了两个对象，用于描述在用户和资源之间的连接权限。 角色 角色是一系列的权限的集合，例如一个角色可以包含读取 Pod 的权限和列出 Pod 的权限， ClusterRole 跟 Role 类似，但是可以在集群中到处使用（ Role 是 namespace 一级的）。 角色绑定 RoleBinding 把角色映射到用户，从而让这些用户继承角色在 namespace 中的权限。ClusterRoleBinding 让用户继承 ClusterRole 在整个集群中的权限。 可参考： 关于 RoleBinding 和 ClusterRoleBinding Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"security/基于rbac的授权.html":{"url":"security/基于rbac的授权.html","title":"基于rbac的授权","keywords":"","body":" RBAC概述 Role and ClusterRole RoleBinding and ClusterRoleBinding 涉及到的资源 聚合clusterole 集群范围内的 ClusterRole 涉及到的Subjects 默认的 Roles 和 Role Bindings Auto-reconciliation user-facing roles 使用kubectl创建roleBiding/clusterRoleBinding Service Account Permissions 参考： RBAC概述 RBAC是Role-Based Access Control的简称，中文为基于角色的访问控制 RBAC使用“rbac.authorization.k8s.io”API组来驱动授权决策，允许管理员通过Kubernetes API动态配置策略。 从1.8开始，RBAC模式处于稳定版本，并由rbac.authorization.k8s.io/v1 API提供支持。 要启用RBAC，需要在 apiserver 中添加参数--authorization-mode=RBAC，如果使用的kubeadm安装的集群，1.6 版本以上的都默认开启了RBAC，可以通过查看 Master 节点上 apiserver 的静态Pod定义文件： # cat /etc/kubernetes/manifests/kube-apiserver.yaml ... - --authorization-mode=Node,RBAC ... 如果是二进制的方式搭建的集群，添加这个参数过后，需要重启 apiserver 服务。 Role and ClusterRole Role：在namespace内 ClusterRole：整集群内 如下Role在 namespace default里具有pods的读权限。 kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: default name: pod-reader rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] ClusterRole除了具备和Role一样可以配置的权限，还有如下几种： cluster-scoped resources (like nodes) non-resource endpoints (like “/healthz”) namespaced resources (like pods) across all namespaces (needed to run kubectl get pods –all-namespaces, for example) 如下ClusterRole具备在集群范围内的所有secret的读权限。 kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: # \"namespace\" omitted since ClusterRoles are not namespaced name: secret-reader rules: - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"get\", \"watch\", \"list\"] RoleBinding and ClusterRoleBinding 顾名思义，RoleBinding就是在某一namespace范围内，将Role授予某一个用户或一组用户（(users, groups, or service accounts)。 相对的，ClusterRoleBinding是在集群范围内授权的。 如下在namespace default里，将Role pod-reader授予给用户sure # This role binding allows \"sure\" to read pods in the \"default\" namespace. kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: read-pods namespace: default subjects: - kind: User name: sure # Name is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: pod-reader apiGroup: rbac.authorization.k8s.io RoleBinding也可以将集群范围的ClusterRole授予给某namespace里的用户，这样管理员可以配置整集群的ClusterRole，然后在多个namespace里复用。 例如，下面在namespace development里，将ClusterRole secret-read授予给用户dandan，这样dandan只能在namespace development里读取secret，对于其他namespace里的secret，仍然是没有权限的。 # This role binding allows \"dandan\" to read secrets in the \"development\" namespace. kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: read-secrets namespace: development # This only grants permissions within the \"development\" namespace. subjects: - kind: User name: dandan # Name is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: secret-reader apiGroup: rbac.authorization.k8s.io ClusterRoleBinding 则是在整个集群范围内给用户授予权限。下面配置将在集群范围内，允许manager组具有所有secret的读权限。 # This cluster role binding allows anyone in the \"rzpadmin\" group to read secrets in any namespace. kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: read-secrets-global subjects: - kind: Group name: rzpadmin # Name is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: secret-reader apiGroup: rbac.authorization.k8s.io 涉及到的资源 大部分资源是直接用Restful API URL的资源字符串来表示的，比如 pods。但某些k8s API还由子资源（subresource），例如容器的日志： GET /api/v1/namespaces/{namespace}/pods/{name}/log pods是一级资源（注意k8s没有把namespace作为资源），用pods表示；log是二级资源，用pods/log来表示。如下将创建一个允许读取pods和pods下日志的Role。 kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: default name: pod-and-pod-logs-reader rules: - apiGroups: [\"\"] resources: [\"pods\", \"pods/log\"] verbs: [\"get\", \"list\"] 也可以通过 resourceName 指定单个资源（而不是一类资源）。如下Role指定的是对 my-configmap 这个cm的在namespace default里的 get和update 权限。 kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: default name: configmap-updater rules: - apiGroups: [\"\"] resources: [\"configmaps\"] resourceNames: [\"my-configmap\"] verbs: [\"update\", \"get\"] 注意，指定了单个资源后，verbs里就不能由 list, watch, create, or deletecollection 了，因为这几个verbs对应的API URL里不会出现resource name。 聚合clusterole k8s 1.9版本以后，可以用 aggregationRule 来聚合其他ClusterRole，从而创建一个新的具有更多权限的ClusterRole。 聚合的方法是通过matchLabels（即`rbac.example.com/aggregate-to-monitoring: \"true\"``），来匹配所有metadata符合该label的ClusterRole。 aggregationRule不需要配置 rules 段，它是由controller收集所有匹配的ClusterRole的rules后填充的。 kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: monitoring aggregationRule: clusterRoleSelectors: - matchLabels: rbac.example.com/aggregate-to-monitoring: \"true\" rules: [] # Rules are automatically filled in by the controller manager. 注意，创建新的符合matchLabel的clusterRole，controller会将新的rules添加到aggregationRule。如下会将 monitoring-endpoints的rules添加到上面的ClusterRole monitoring。 kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: monitoring-endpoints labels: rbac.example.com/aggregate-to-monitoring: \"true\" # These rules will be added to the \"monitoring\" role. rules: - apiGroups: [\"\"] Resources: [\"services\", \"endpoints\", \"pods\"] verbs: [\"get\", \"list\", \"watch\"] 默认的面向用户的role使用了aggregationRule。这样admin可以自动拥有 CustomResourceDefinitions CRD的权限。 集群范围内的 ClusterRole admin会聚合所有label为rbac.authorization.k8s.io/aggregate-to-admin: \"true\"的ClusterRole 而 view 则会聚合所有label为rbac.authorization.k8s.io/aggregate-to-view: \"true\"的ClusterRole。 apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: admin annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: kubernetes.io/bootstrapping: rbac-defaults aggregationRule: clusterRoleSelectors: - matchLabels: rbac.authorization.k8s.io/aggregate-to-admin: \"true\" --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: view annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: kubernetes.io/bootstrapping: rbac-defaults aggregationRule: clusterRoleSelectors: - matchLabels: rbac.authorization.k8s.io/aggregate-to-view: \"true\" 配合下面的clusterrole使用 kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: aggregate-cron-tabs-edit labels: # Add these permissions to the \"admin\" and \"edit\" default roles. rbac.authorization.k8s.io/aggregate-to-admin: \"true\" rbac.authorization.k8s.io/aggregate-to-edit: \"true\" rules: - apiGroups: [\"stable.example.com\"] resources: [\"crontabs\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"] --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: aggregate-cron-tabs-view labels: # Add these permissions to the \"view\" default role. rbac.authorization.k8s.io/aggregate-to-view: \"true\" rules: - apiGroups: [\"stable.example.com\"] resources: [\"crontabs\"] verbs: [\"get\", \"list\", \"watch\"] 参考: https://kubernetes.io/docs/reference/access-authn-authz/rbac/#role-examples 涉及到的Subjects RoleBinding、ClusterRoleBinding将Role、ClusterRole绑定到 subject。subject可以是 user，group，serviceAccount。 user：k8s并不管理user，而是authenticator（例如我们用的是dex）在管理。user可以是字符串（如”jane”），email（如bob@example.com），ID。这取决于管理员配置认证的时候，--oidc-username-claim配置的是什么。我们这里用的是name。system:开头的用户是保留给k8s用的。 group：group是由 Authenticator modules 在管理。可以是像user一样的普通group，也可以是 system:开头的k8s的组。 serviceaccount：由k8s管理。sa区别于user，其代表的是service，用来供service之间访问用，例如service A调用 service B的API，那么可以为service A创建一个 sa ，然后赋予该sa访问 service B API的权限。sa类似appid的概念。sa可以用 kubectl get sa来查看。 注意group的两个例子: subjects: - kind: Group name: \"frontend-admins\" apiGroup: rbac.authorization.k8s.io --- # namespace qa下的sa subjects: - kind: Group name: system:serviceaccounts:qa apiGroup: rbac.authorization.k8s.io --- # 所有sa subjects: - kind: Group name: system:serviceaccounts apiGroup: rbac.authorization.k8s.io --- # 所有已认证用户 subjects: - kind: Group name: system:authenticated apiGroup: rbac.authorization.k8s.io --- # 所有未认证用户 subjects: - kind: Group name: system:unauthenticated apiGroup: rbac.authorization.k8s.io # 所有用户 subjects: - kind: Group name: system:authenticated apiGroup: rbac.authorization.k8s.io - kind: Group name: system:unauthenticated apiGroup: rbac.authorization.k8s.io 默认的 Roles 和 Role Bindings API Server内置了若干clusterRole和clusterRoleBinding，一般是以system:开头，label为kubernetes.io/bootstrapping=rbac-defaults。尽量不要去修改他们，否则会引起系统级别的问题，因为可能某些服务会因为权限不足而无法正常运行。 Auto-reconciliation 万一不小心把上面的clusterRole或者clusterRoleBinding给改错了呢？API Master具备 Auto-reconciliation的功能，即，API Master每次重启的时候，都会重新恢复默认的clusterRole/ClusterRoleBiding。 如果确实要改，可以修改其rbac.authorization.kubernetes.io/autoupdate为false，这样API Server就不会恢复其默认值了。 user-facing roles API Server还内置了一些不以system:开头的clusterRole: cluster-admin：集群超级管理员。resources、verbs匹配全是 * admin/edit/view：在某namespace中授权 使用kubectl创建roleBiding/clusterRoleBinding 创建某一namespace下的roleBinding: kubectl create rolebinding bob-admin-binding --clusterrole=admin --user=bob --namespace=acme kubectl create rolebinding myapp-view-binding --clusterrole=view --serviceaccount=acme:myapp --namespace=acme 创建clusterRoleBiding: kubectl create clusterrolebinding root-cluster-admin-binding --clusterrole=cluster-admin --user=root kubectl create clusterrolebinding kubelet-node-binding --clusterrole=system:node --user=kubelet kubectl create clusterrolebinding myapp-view-binding --clusterrole=view --serviceaccount=acme:myapp Service Account Permissions 默认RBAC不会给 kube-system 之外的sa授予权限。 https://kubernetes.io/docs/reference/access-authn-authz/rbac/#service-account-permissions 详细描述了权限管理从严格到宽松的各种sa授权方式。总之，管理越严格，管理员越忙。 参考： Kubernetes中的角色访问控制机制（RBAC）支持 Kubernetes-基于RBAC的授权 kubernetes RBAC认证简介 https://kubernetes.io/docs/reference/access-authn-authz/rbac/#role-examples https://kubernetes.io/docs/reference/access-authn-authz/rbac/#discovery-roles Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"monitor/监控概述.html":{"url":"monitor/监控概述.html","title":"监控概述","keywords":"","body":"监控分为三个层面 metrics logging trace metrics指的是某个度量,我们认知钟对这个度量有个基线值,当超出了这个基线值,我们就应该关注 logging当有引起我们关注的metrics的时候,这个时候我们就应该\"钻\"下去。我们应该看看在过去几小时/几分钟/几十秒内到底发生了一些什么事情，这个时候就是logging开始发挥作用，它应该可以给出我们想要的回放。 trace，当有了logging，可能我们觉得还是不够的，这个时候就应该trace，这个过程是非常消耗资源的。 上面3给过程，我们用去看病来举例 当我们身体不适了，去医院找医生看病，这个时候医生给我们量了体温发现38.5°，这个就是metrice。然后医生开始询问我们昨天到今天吃了些什么、做了些什么事，我们一一做了回答，这个就是logging。这个时候医生心里可能已经有数了，但是现在医闹问题，所以还不敢轻易下结论，他就说你去查查血看看有没有病毒性感染，这个就是trace。 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-13 22:35:54 "},"monitor/日志收集框架.html":{"url":"monitor/日志收集框架.html","title":"日志收集框架","keywords":"","body":" Kubernetes 日志收集 节点日志采集代理 sidecar 容器收集日志 使用 sidecar 运行日志采集 agent Kubernetes 日志收集 Kubernetes 集群本身不提供日志收集的解决方案，一般来说有主要的3种方案来做日志收集： 在节点上运行一个 agent 来收集日志 在 Pod 中包含一个 sidecar 容器来收集应用日志 直接在应用程序中将日志信息推送到采集后端 节点日志采集代理 通过在每个节点上运行一个日志收集的 agent 来采集日志数据，日志采集 agent 是一种专用工具，用于将日志数据推送到统一的后端。一般来说，这种 agent 用一个容器来运行，可以访问该节点上所有应用程序容器的日志文件所在目录。 由于这种 agent 必须在每个节点上运行，所以直接使用 DaemonSet 控制器运行该应用程序即可。在节点上运行一个日志收集的 agent 这种方式是最常见的一直方法，因为它只需要在每个节点上运行一个代理程序，并不需要对节点上运行的应用程序进行更改，对应用程序没有任何侵入性，但是这种方法也仅仅适用于收集输出到 stdout 和 stderr 的应用程序日志。 sidecar 容器收集日志 上面第一种方法可以看到有一个明显的问题就是采集的日志都是通过输出到容器的 stdout 和 stderr 里面的信息，这些信息会在本地的容器对应目录中保留成 JSON 日志文件，所以直接在节点上运行一个 agent 就可以采集到日志。但是如果应用程序的日志是输出到容器中的某个日志文件的话呢？这种日志数据显然只通过上面的方案是采集不到的了。 对于上面这种情况我们可以直接在 Pod 中启动另外一个 sidecar 容器，直接将应用程序的日志通过这个容器重新输出到 stdout，这样通过上面的节点日志收集方案又可以完成了。 由于这个 sidecar 容器的主要逻辑就是将应用程序中的日志进行重定向打印，所以背后的逻辑非常简单，开销很小，而且由于输出到了 stdout 或者 stderr，所以我们也可以使用 kubectl logs 来查看日志了。 示例: apiVersion: v1 kind: Pod metadata: name: counter spec: containers: - name: count image: busybox args: - /bin/sh - -c - > i=0; while true; do echo \"$i: $(date)\" >> /var/log/1.log; echo \"$(date) INFO $i\" >> /var/log/2.log; i=$((i+1)); sleep 1; done volumeMounts: - name: varlog mountPath: /var/log volumes: - name: varlog emptyDir: {} 由于 Pod 中容器的特性，我们可以利用另外一个 sidecar 容器去获取到另外容器中的日志文件，然后将日志重定向到自己的 stdout 流中，可以将上面的 YAML 文件做如下修改： apiVersion: v1 kind: Pod metadata: name: counter spec: containers: - name: count image: busybox args: - /bin/sh - -c - > i=0; while true; do echo \"$i: $(date)\" >> /var/log/1.log; echo \"$(date) INFO $i\" >> /var/log/2.log; i=$((i+1)); sleep 1; done volumeMounts: - name: varlog mountPath: /var/log - name: count-log-1 image: busybox args: [/bin/sh, -c, 'tail -n+1 -f /var/log/1.log'] volumeMounts: - name: varlog mountPath: /var/log - name: count-log-2 image: busybox args: [/bin/sh, -c, 'tail -n+1 -f /var/log/2.log'] volumeMounts: - name: varlog mountPath: /var/log volumes: - name: varlog emptyDir: {} 这样前面节点上的日志采集 agent 就可以自动获取这些日志信息，而不需要其他配置。 这种方法虽然可以解决上面的问题，但是也有一个明显的缺陷，就是日志不仅会在原容器文件中保留下来，还会通过 stdout 输出后占用磁盘空间，这样无形中就增加了一倍磁盘空间。 使用 sidecar 运行日志采集 agent 如果觉得在节点上运行一个日志采集的代理不够灵活的话，那么也可以创建一个单独的日志采集代理程序的 sidecar 容器，不过需要单独配置和应用程序一起运行。 不过这样虽然更加灵活，但是在 sidecar 容器中运行日志采集代理程序会导致大量资源消耗，因为如果有多少个要采集的 Pod，就需要运行多少个采集代理程序，另外还无法使用 kubectl logs 命令来访问这些日志，因为它们不受 kubelet 控制。 举个例子，可以使用的Stackdriver，它使用fluentd作为记录剂。以下是两个可用于实现此方法的配置文件。第一个文件包含配置流利的ConfigMap。 下面是 Kubernetes 官方的一个 fluentd 的配置文件示例，使用 ConfigMap 对象来保存： apiVersion: v1 kind: ConfigMap metadata: name: fluentd-config data: fluentd.conf: | type tail format none path /var/log/1.log pos_file /var/log/1.log.pos tag count.format1 type tail format none path /var/log/2.log pos_file /var/log/2.log.pos tag count.format2 type google_cloud 上面的配置文件是配置收集原文件 /var/log/1.log 和 /var/log/2.log 的日志数据，然后通过 google_cloud 这个插件将数据推送到 Stackdriver 后端去。 下面是我们使用上面的配置文件在应用程序中运行一个 fluentd 的容器来读取日志数据： apiVersion: v1 kind: Pod metadata: name: counter spec: containers: - name: count image: busybox args: - /bin/sh - -c - > i=0; while true; do echo \"$i: $(date)\" >> /var/log/1.log; echo \"$(date) INFO $i\" >> /var/log/2.log; i=$((i+1)); sleep 1; done volumeMounts: - name: varlog mountPath: /var/log - name: count-agent image: k8s.gcr.io/fluentd-gcp:1.30 env: - name: FLUENTD_ARGS value: -c /etc/fluentd-config/fluentd.conf volumeMounts: - name: varlog mountPath: /var/log - name: config-volume mountPath: /etc/fluentd-config volumes: - name: varlog emptyDir: {} - name: config-volume configMap: name: fluentd-config Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"monitor/k8s集成EFK作日志收集.html":{"url":"monitor/k8s集成EFK作日志收集.html","title":"k8s集成EFK作日志收集","keywords":"","body":" 概述 部署 Elasticsearch 集群 创建elasticsearch 的headless service 创建Elasticsearch 的 Pod 应用 pod模板的一些内容描述 几个初始化容器 测试是否安装成功 部署kibana 部署fluented 工作原理 配置 安装 configMap deployment 参考 概述 Kubernetes 中比较流行的日志收集解决方案是 Elasticsearch、Fluentd 和 Kibana（EFK）技术栈，也是官方现在比较推荐的一种方案。 Elasticsearch 是一个实时的、分布式的可扩展的搜索引擎，允许进行全文、结构化搜索，它通常用于索引和搜索大量日志数据，也可用于搜索许多不同类型的文档。 Kibana 是 Elasticsearch 的一个功能强大的数据可视化 Dashboard，Elasticsearch 通常与 Kibana 一起部署，Kibana 允许通过 web 界面来浏览 Elasticsearch 日志数据。 Fluentd是一个流行的开源数据收集器，我们将在 Kubernetes 集群节点上安装 Fluentd，通过获取容器日志文件、过滤和转换日志数据，然后将数据传递到 Elasticsearch 集群，在该集群中对其进行索引和存储。 部署 Elasticsearch 集群 这里我们使用3个 Elasticsearch Pod 来避免高可用下多节点集群中出现的“脑裂”问题，当一个或多个节点无法与其他节点通信时会产生“脑裂”，可能会出现几个主节点。 更多 Elasticsearch 集群脑裂问题，可以查看文档 https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html#split-brain 一个关键点是参数discover.zen.minimum_master_nodes=N/2+1，其中N是 Elasticsearch 集群中符合主节点的节点数，比如我们这里3个节点，意味着N应该设置为2。这样，如果一个节点暂时与集群断开连接，则另外两个节点可以选择一个新的主节点，并且集群可以在最后一个节点尝试重新加入时继续运行，在扩展 Elasticsearch 集群时，一定要记住这个参数。 创建elasticsearch 的headless service 首先创建一个名为 elasticsearch 的无头服务，新建文件 elasticsearch-svc.yaml，文件内容如下： kind: Service apiVersion: v1 metadata: name: elasticsearch namespace: logging labels: app: elasticsearch spec: selector: app: elasticsearch clusterIP: None ports: - port: 9200 name: rest - port: 9300 name: inter-node 定义了一个名为 elasticsearch 的 Service，指定标签app=elasticsearch，当我们将 Elasticsearch StatefulSet 与此服务关联时，服务将返回带有标签app=elasticsearch的 Elasticsearch Pods 的 DNS A 记录，然后设置clusterIP=None，将该服务设置成无头服务。最后，我们分别定义端口9200、9300，分别用于与 REST API 交互，以及用于节点间通信。 现在我们已经为 Pod 设置了无头服务和一个稳定的域名.elasticsearch.logging.svc.cluster.local，接下来我们通过 StatefulSet 来创建具体的 Elasticsearch 的 Pod 应用。 创建Elasticsearch 的 Pod 应用 Kubernetes StatefulSet 允许我们为 Pod 分配一个稳定的标识和持久化存储，Elasticsearch 需要稳定的存储来保证 Pod 在重新调度或者重启后的数据依然不变，所以需要使用 StatefulSet 来管理 Pod。 要了解更多关于 StaefulSet 的信息，可以查看官网关于 StatefulSet 的相关文档：https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/。 apiVersion: apps/v1 kind: StatefulSet metadata: name: es-cluster namespace: logging spec: serviceName: elasticsearch replicas: 3 selector: matchLabels: app: elasticsearch template: metadata: labels: app: elasticsearch spec: containers: - name: elasticsearch image: 192.168.3.27:8888/library/elasticsearch-oss:6.6.2 resources: limits: cpu: 1000m requests: cpu: 100m ports: - containerPort: 9200 name: rest protocol: TCP - containerPort: 9300 name: inter-node protocol: TCP volumeMounts: - name: data mountPath: /usr/share/elasticsearch/data env: - name: cluster.name value: k8s-logs - name: node.name valueFrom: fieldRef: fieldPath: metadata.name - name: discovery.zen.ping.unicast.hosts value: \"es-cluster-0.elasticsearch,es-cluster-1.elasticsearch,es-cluster-2.elasticsearch\" - name: discovery.zen.minimum_master_nodes value: \"2\" - name: ES_JAVA_OPTS value: \"-Xms512m -Xmx512m\" initContainers: - name: fix-permissions image: busybox command: [\"sh\", \"-c\", \"chown -R 1000:1000 /usr/share/elasticsearch/data\"] securityContext: privileged: true volumeMounts: - name: data mountPath: /usr/share/elasticsearch/data - name: increase-vm-max-map image: busybox command: [\"sysctl\", \"-w\", \"vm.max_map_count=262144\"] securityContext: privileged: true - name: increase-fd-ulimit image: busybox command: [\"sh\", \"-c\", \"ulimit -n 65536\"] securityContext: privileged: true volumeClaimTemplates: - metadata: name: data labels: app: elasticsearch spec: accessModes: [ \"ReadWriteOnce\" ] storageClassName: nfs-dynamic-class resources: requests: storage: 10Gi pod模板的一些内容描述 该部分是定义 StatefulSet 中的 Pod，我们这里使用一个-oss后缀的镜像，该镜像是 Elasticsearch 的开源版本，如果你想使用包含X-Pack之类的版本，可以去掉该后缀。然后暴露了9200和9300两个端口，注意名称要和上面定义的 Service 保持一致。然后通过 volumeMount 声明了数据持久化目录，下面我们再来定义 VolumeClaims。最后就是我们在容器中设置的一些环境变量了： cluster.name：Elasticsearch 集群的名称，我们这里命名成 k8s-logs。 node.name：节点的名称，通过metadata.name来获取。这将解析为 es-cluster-[0,1,2]，取决于节点的指定顺序。 discovery.zen.ping.unicast.hosts：此字段用于设置在 Elasticsearch 集群中节点相互连接的发现方法。我们使用 unicastdiscovery 方式，它为我们的集群指定了一个静态主机列表。由于我们之前配置的无头服务，我们的 Pod 具有唯一的 DNS 域es-cluster-[0,1,2].elasticsearch.logging.svc.cluster.local，因此我们相应地设置此变量。由于都在同一个 namespace 下面，所以我们可以将其缩短为es-cluster-[0,1,2].elasticsearch。要了解有关 Elasticsearch 发现的更多信息，请参阅 Elasticsearch 官方文档：https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery.html。 discovery.zen.minimum_master_nodes：我们将其设置为(N/2) + 1，N是我们的群集中符合主节点的节点的数量。我们有3个 Elasticsearch 节点，因此我们将此值设置为2（向下舍入到最接近的整数）。要了解有关此参数的更多信息，请参阅官方 Elasticsearch 文档：https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html#split-brain。 ES_JAVA_OPTS：这里我们设置为-Xms512m -Xmx512m，告诉JVM使用512 MB的最小和最大堆。您应该根据群集的资源可用性和需求调整这些参数。要了解更多信息，请参阅设置堆大小的相关文档：https://www.elastic.co/guide/en/elasticsearch/reference/current/heap-size.html。 几个初始化容器 这里我们定义了几个在主应用程序之前运行的 Init 容器，这些初始容器按照定义的顺序依次执行，执行完成后才会启动主应用容器。 第一个名为 fix-permissions 的容器用来运行 chown 命令，将 Elasticsearch 数据目录的用户和组更改为1000:1000（Elasticsearch 用户的 UID）。因为默认情况下，Kubernetes 用 root 用户挂载数据目录，这会使得 Elasticsearch 无法方法该数据目录，可以参考 Elasticsearch 生产中的一些默认注意事项相关文档说明：https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html#_notes_for_production_use_and_defaults。 第二个名为 increase-vm-max-map 的容器用来增加操作系统对mmap计数的限制，默认情况下该值可能太低，导致内存不足的错误，要了解更多关于该设置的信息，可以查看 Elasticsearch 官方文档说明：https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html。 最后一个初始化容器是用来执行ulimit命令增加打开文件描述符的最大数量的。 此外 Elastisearch Notes for Production Use 文档还提到了由于性能原因最好禁用 swap，当然对于 Kubernetes 集群而言，最好也是禁用 swap 分区的。 测试是否安装成功 添加成功后，可以看到 logging 命名空间下面的所有的资源对象： [root@master monitor]# kubectl get pod -n logging NAME READY STATUS RESTARTS AGE es-cluster-0 1/1 Running 42 19h es-cluster-1 1/1 Running 0 19h es-cluster-2 1/1 Running 0 18h fluentd-es-6xgxd 1/1 Running 0 16h fluentd-es-nt8d2 1/1 Running 0 16h kibana-5bff696ff4-mkp4l 1/1 Running 0 18h Pods 部署完成后，我们可以通过请求一个 REST API 来检查 Elasticsearch 集群是否正常运行。使用下面的命令将本地端口9200转发到 Elasticsearch 节点（如es-cluster-0）对应的端口： [root@master monitor]# kubectl port-forward es-cluster-0 9200:9200 --namespace=logging Forwarding from 127.0.0.1:9200 -> 9200 Forwarding from [::1]:9200 -> 9200 Handling connection for 9200 Handling connection for 9200 然后，在另外的终端窗口中，执行如下请求： [root@master ~]# curl http://localhost:9200/_cluster/state?pretty |more % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0{ \"cluster_name\" : \"k8s-logs\", \"compressed_size_in_bytes\" : 22679, \"cluster_uuid\" : \"p_AQ83sITaKreerPObVMag\", \"version\" : 3259, \"state_uuid\" : \"vpG1GH2bT9mSbwF9nqTlzw\", \"master_node\" : \"0SkArt-FTVih5fzWQkGEMw\", \"blocks\" : { }, \"nodes\" : { \"0SkArt-FTVih5fzWQkGEMw\" : { \"name\" : \"es-cluster-1\", \"ephemeral_id\" : \"wxjSdWb5SNqqSowx_d-UkQ\", \"transport_address\" : \"10.2.35.139:9300\", \"attributes\" : { } }, \"YeC_0S6eR7KeKcylEtneJg\" : { \"name\" : \"es-cluster-0\", \"ephemeral_id\" : \"9FaV_urNT8GCuOU32di89A\", \"transport_address\" : \"10.2.16.37:9300\", \"attributes\" : { } }, \"ynGIAwAUSOa8yfJs2sbWlw\" : { \"name\" : \"es-cluster-2\", \"ephemeral_id\" : \"gKFv1069QZ6S-ELVXNkWsQ\", \"transport_address\" : \"10.2.35.140:9300\", \"attributes\" : { } } }, ... ... ... 看到上面的信息就表明我们名为 k8s-logs 的 Elasticsearch 集群成功创建了3个节点：es-cluster-0，es-cluster-1，和es-cluster-2，当前主节点是 es-cluster-0。 部署kibana Elasticsearch 集群启动成功了，接下来我们可以来部署 Kibana 服务，新建一个名为 kibana.yaml 的文件，对应的文件内容如下： apiVersion: v1 kind: Service metadata: name: kibana namespace: logging labels: app: kibana spec: ports: - port: 5601 type: NodePort selector: app: kibana --- apiVersion: apps/v1 kind: Deployment metadata: name: kibana namespace: logging labels: app: kibana spec: selector: matchLabels: app: kibana template: metadata: labels: app: kibana spec: containers: - name: kibana image: 192.168.3.27:8888/library/kibana-oss:6.6.2 resources: limits: cpu: 1000m requests: cpu: 100m env: - name: ELASTICSEARCH_URL value: http://elasticsearch:9200 ports: - containerPort: 5601 上面我们定义了两个资源对象，一个 Service 和 Deployment，为了测试方便，我们将 Service 设置为了 NodePort 类型，Kibana Pod 中配置都比较简单，唯一需要注意的是我们使用 ELASTICSEARCH_URL 这个环境变量来设置Elasticsearch 集群的端点和端口，直接使用 Kubernetes DNS 即可，此端点对应服务名称为 elasticsearch，由于是一个 headless service，所以该域将解析为3个 Elasticsearch Pod 的 IP 地址列表。 # kubectl get svc --namespace=logging NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE elasticsearch ClusterIP None 9200/TCP,9300/TCP 19h kibana NodePort 10.1.184.32 5601:26299/TCP 18h 如果 Pod 已经是 Running 状态了，证明应用已经部署成功了，然后可以通过 NodePort 来访问 Kibana 这个服务，在浏览器中打开http://:26299 即可，如果看到如下欢迎界面证明 Kibana 已经成功部署到了 Kubernetes集群之中。 部署fluented Fluentd 是一个高效的日志聚合器，是用 Ruby 编写的，并且可以很好地扩展。对于大部分企业来说，Fluentd 足够高效并且消耗的资源相对较少，另外一个工具Fluent-bit更轻量级，占用资源更少，但是插件相对 Fluentd 来说不够丰富，所以整体来说，Fluentd 更加成熟，使用更加广泛，所以我们这里也同样使用 Fluentd 来作为日志收集工具。 工作原理 Fluentd 通过一组给定的数据源抓取日志数据，处理后（转换成结构化的数据格式）将它们转发给其他服务，比如 Elasticsearch、对象存储等等。Fluentd 支持超过300个日志存储和分析服务，所以在这方面是非常灵活的。主要运行步骤如下： 首先 Fluentd 从多个日志源获取数据 结构化并且标记这些数据 然后根据匹配的标签将数据发送到多个目标服务去 配置 一般来说我们是通过一个配置文件来告诉 Fluentd 如何采集、处理数据的，下面简单和大家介绍下 Fluentd 的配置方法。 安装 configMap # cat fluted-cm.yaml kind: ConfigMap apiVersion: v1 metadata: name: fluentd-config namespace: logging labels: addonmanager.kubernetes.io/mode: Reconcile data: system.conf: |- root_dir /tmp/fluentd-buffers/ containers.input.conf: |- @id fluentd-containers.log @type tail path /var/log/containers/*.log pos_file /var/log/es-containers.log.pos time_format %Y-%m-%dT%H:%M:%S.%NZ localtime tag raw.kubernetes.* format json read_from_head true # Detect exceptions in the log output and forward them as one log entry. @id raw.kubernetes @type detect_exceptions remove_tag_prefix raw message log stream stream multiline_flush_interval 5 max_bytes 500000 max_lines 1000 system.input.conf: |- # Logs from systemd-journal for interesting services. @id journald-docker @type systemd filters [{ \"_SYSTEMD_UNIT\": \"docker.service\" }] @type local persistent true read_from_head true tag docker @id journald-kubelet @type systemd filters [{ \"_SYSTEMD_UNIT\": \"kubelet.service\" }] @type local persistent true read_from_head true tag kubelet forward.input.conf: |- # Takes the messages sent over TCP @type forward output.conf: |- # Enriches records with Kubernetes metadata @type kubernetes_metadata @id elasticsearch @type elasticsearch @log_level info include_tag_key true host elasticsearch port 9200 logstash_format true request_timeout 30s @type file path /var/log/fluentd-buffers/kubernetes.system.buffer flush_mode interval retry_type exponential_backoff flush_thread_count 2 flush_interval 5s retry_forever retry_max_interval 30 chunk_limit_size 2M queue_limit_length 8 overflow_action block deployment apiVersion: v1 kind: ServiceAccount metadata: name: fluentd-es namespace: logging labels: k8s-app: fluentd-es kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: fluentd-es labels: k8s-app: fluentd-es kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile rules: - apiGroups: - \"\" resources: - \"namespaces\" - \"pods\" verbs: - \"get\" - \"watch\" - \"list\" --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: fluentd-es labels: k8s-app: fluentd-es kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile subjects: - kind: ServiceAccount name: fluentd-es namespace: logging apiGroup: \"\" roleRef: kind: ClusterRole name: fluentd-es apiGroup: \"\" --- apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-es namespace: logging labels: k8s-app: fluentd-es version: v2.2.0 kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile spec: selector: matchLabels: k8s-app: fluentd-es version: v2.2.0 template: metadata: labels: k8s-app: fluentd-es kubernetes.io/cluster-service: \"true\" version: v2.2.0 # This annotation ensures that fluentd does not get evicted if the node # supports critical pod annotation based priority scheme. # Note that this does not guarantee admission on the nodes (#40573). annotations: scheduler.alpha.kubernetes.io/critical-pod: '' spec: priorityClassName: system-node-critical serviceAccountName: fluentd-es containers: - name: fluentd-es image: 192.168.3.27:8888/library/fluentd-elasticsearch:v2.2.0 env: - name: FLUENTD_ARGS value: --no-supervisor -q resources: limits: memory: 500Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true - name: config-volume mountPath: /etc/fluent/config.d nodeSelector: beta.kubernetes.io/fluentd-ds-ready: \"true\" tolerations: - key: node-role.kubernetes.io/master operator: Exists effect: NoSchedule terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers - name: config-volume configMap: name: fluentd-config 将上面创建的 fluentd-config 这个 ConfigMap 对象通过 volumes 挂载到了 Fluentd 容器中，另外为了能够灵活控制哪些节点的日志可以被收集，所以我们这里还添加了一个 nodSelector 属性： nodeSelector: beta.kubernetes.io/fluentd-ds-ready: \"true\" 默认情况下 master 节点有污点，所以要想也收集 master 节点的日志，则需要添加上容忍： tolerations: - key: node-role.kubernetes.io/master operator: Exists effect: NoSchedule 还需要注意的是docker容器的根目录位置 创建完成后，查看对应的 Pods 列表，检查是否部署成功： # kubectl get pods -n logging NAME READY STATUS RESTARTS AGE es-cluster-0 1/1 Running 42 19h es-cluster-1 1/1 Running 0 19h es-cluster-2 1/1 Running 0 19h fluentd-es-6xgxd 1/1 Running 0 17h fluentd-es-nt8d2 1/1 Running 0 17h kibana-5bff696ff4-mkp4l 1/1 Running 0 18h 参考 https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"monitor/prometheus.html":{"url":"monitor/prometheus.html","title":"prometheus","keywords":"","body":"Prometheus 对于 K8S 的监控，我们选择 CNCF 旗下次于 K8S 毕业的项目 Prometheus 。 Prometheus 是一个非常灵活易于扩展的监控系统，它通过各种 exporter 暴露数据，并由 prometheus server 定时去拉数据，然后存储。 它自己提供了一个简单的前端界面，可在其中使用 PromQL 的语法进行查询，并进行图形化展示。 安装Prometheus Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"devops/在k8s集群上玩转CICD.html":{"url":"devops/在k8s集群上玩转CICD.html","title":"k8s+jenkins-pipline打造CI/CD","keywords":"","body":" 利用k8s + jenkins打造CI/CD CI/CD概念 传统的CI/CD流程 传统的CI/CD的基于jenkins master-slave的改进 基于k8s的CI/CD流程 传统方式的痛点 基于k8s的CI/CD的优势 前置条件: 玩转CI/CD步骤: 利用k8s + jenkins打造CI/CD CI/CD概念 持续集成（ContinuousIntegration，CI）：代码合并、构建、部署、测试都在一起，不断地执行这个过程，并对结果反馈。 持续部署（ContinuousDeployment，CD）：部署到测试环境、预生产环境、生产环境。 持续交付（ContinuousDelivery，CD）：将最终产品发布到生产环境，给用户使用。 传统的CI/CD流程 传统的CI/CD的基于jenkins master-slave的改进 Master/Slave相当于Server和agent的概念 Master提供web接口让用户来管理job和slave，job可以运行在master本机或者被分配到slave上运行。 一个master可以关联多个slave用来为不同的job或相同的job的不同配置来服务。 基于k8s的CI/CD流程 传统方式的痛点 传统的 Jenkins Master-Slave 一主多从会存在一些痛点，例如： 主Master发生单点故障时，整个流程都不可用了 每个Slave的配置环境不一样，来完成不同语言的编译打包等操作，但是这些差异化的配置导致管理起来非常不方便，维护起来也是比较费劲 资源分配不均衡，有的Slave要运行的job出现排队等待，而有的Slave处于空闲状态 资源有浪费，每 Slave可能是物理机或者虚拟机，当Slave处于空闲状态时，也不会完全释放掉资源 基于k8s的CI/CD的优势 正因为上面的这些痛点，我们希望有一种更高效更可靠的方式来完成这个 CI/CD 流程，而 Docker 技术能很好的解决这个痛点，又特别是在 k8s 集群环境下面能够更好来解决上面的问题，下图是基于 k8s 搭建 Jenkins master-slave 的示意图： 从图中可以看到 Jenkins Master 和 Jenkins Slave 以 Pod 形式运行在 k8s 的 Node 上，Master 运行在其中一个节点(也可以是statefulset)，并且将其配置数据存储到一个 PV 上去(这个需要共享存储)，Slave 被调度在各个节点上，并且它不是一直处于运行状态，它会按照jenkins pipeline或者其他配置 动态的创建运行完job后再自动删除。 这种模式的工作流程大致为： 当 Jenkins Master 接受到 Build 请求时，会根据配置的 Label 动态创建一个运行在 Pod 中的 Jenkins Slave 并注册到 Master 上，当运行完 Job 后，这个 Slave 会被注销并且这个 Pod 也会自动删除，恢复到最初状态。 这种模式带来的好处: 服务高可用: 当 Jenkins Master 出现故障时， 根据k8s自身的架构特点会自动创建一个新的 Jenkins Master 容器，并且将 之前创建好的 PV 分配给新创建的容器，保证数据不丢失，从而达到集群服务高可用。 或者直接将master部署为statefulset的形式, 让每个节点都部署一个从而达到高可用 动态伸缩 master节点可以部署为deployment + HPA的形式,当master的使用率过高或者过低时,动态扩展或收缩 合理使用资源，每次运行 Job 时，会自动创建一个 Jenkins Slave Job 完成后，Slave 自动注销并删除容器，资源自动释放， Kubernetes 根据每个资源的使用情况，动态分配 Slave 到空闲的节点上创建，合理利用资源 扩展性好 当 node资 源严重不足而导致 Job 排队等待时，可以较容易的添加一个 Node 到集群中，从而实现扩展。 前置条件: 已经部署好了一套K8S 已经搭建好了私有仓库 已经部署好了共享网络存储(如NFS/CEPH等)，可以创建PV/PVC/SC 玩转CI/CD步骤: 部署jenkins-master 配置jenkins-master 制作jenkins-slave 创建pipeline项目 编写jenkinsfile文件 验证 优化改进 最终交付物 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"devops/1.部署jenkins-master.html":{"url":"devops/1.部署jenkins-master.html","title":"1.部署jenkins-master","keywords":"","body":" 部署jenkins-master 环境说明: 创建相应的role、rolebinding、sa 创建jenkins master的image 创建拉取镜像的sercet 在k8s集群中部署jenkins master 部署pvc 部署jenkins 部署service 最终要达到的效果 部署jenkins-master 创建相应的role、rolebinding、sa 创建jenkins master的image 环境说明: k8s 1.11 docker: ce-18.09.0 storageclass: nfs-dynamic-class 私有仓库:harbor 192.168.3.27:8888 创建相应的role、rolebinding、sa 因为k8s 1.6引入了RBAC,所以在部署jenkins的时候也应该加入相应的元素 --- apiVersion: v1 kind: ServiceAccount metadata: name: jenkins --- kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: jenkins rules: - apiGroups: [\"extensions\", \"apps\"] resources: [\"deployments\"] verbs: [\"create\", \"delete\", \"get\", \"list\", \"watch\", \"patch\", \"update\"] - apiGroups: [\"\"] resources: [\"services\"] verbs: [\"create\", \"delete\", \"get\", \"list\", \"watch\", \"patch\", \"update\"] - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"] - apiGroups: [\"\"] resources: [\"pods/exec\"] verbs: [\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"] - apiGroups: [\"\"] resources: [\"pods/log\"] verbs: [\"get\",\"list\",\"watch\"] - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"get\"] --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: RoleBinding metadata: name: jenkins roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: jenkins subjects: - kind: ServiceAccount name: jenkins 创建jenkins master的image Dockerfile如下: FROM jenkins/jenkins:lts-alpine USER root 打包生成镜像 docker build -t 192.168.3.27:8888/ops/jenkins:lts-alpine . docker push 192.168.3.27:8888/ops/jenkins:lts-alpine 创建拉取镜像的sercet 因为是在k8s中使用docker拉取镜像,所以需要生成secret,详见 私有镜像仓库介绍 [root@master k8s-jenkins]# cat registry-pull-secret.yaml apiVersion: v1 kind: Secret metadata: name: registry-pull-secret namespace: default data: .dockerconfigjson: ewoJImF1dGhzIjogewoJCSIxOTIuMTY4LjMuMjc6ODg4OCI6IHsKCQkJImF1dGgiOiAiZVdaNmVEb3hjV0Y2UUZkVFdBPT0iCgkJfQoJfSwKCSJIdHRwSGVhZGVycyI6IHsKCQkiVXNlci1BZ2VudCI6ICJEb2NrZXItQ2xp ZW50LzE4LjA5LjAgKGxpbnV4KSIKCX0KfQ== kubectl create -f registry-pull-secret.yaml 在k8s集群中部署jenkins master 部署pvc 因为我已经配置好了storageclass,所以在这个地方直接使用的是PVC # kubectl get storageclass NAME PROVISIONER AGE nfs-dynamic-class fuseim.pri/ifs 46d apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfs-jenkins-pvc spec: accessModes: - ReadWriteOnce storageClassName: nfs-dynamic-class resources: requests: storage: 10Gi 部署jenkins 我在这个地方选择的是StatefulSet控制器, 当然也可以选择deployment的控制器 apiVersion: apps/v1beta1 kind: StatefulSet metadata: name: jenkins labels: name: jenkins spec: serviceName: jenkins replicas: 1 updateStrategy: type: RollingUpdate template: metadata: name: jenkins labels: name: jenkins spec: terminationGracePeriodSeconds: 10 serviceAccountName: jenkins imagePullSecrets: - name: registry-pull-secret containers: - name: jenkins image: 192.168.3.27:8888/ops/jenkins:lts-alpine imagePullPolicy: Always ports: - containerPort: 8080 - containerPort: 50000 resources: limits: cpu: 1 memory: 1Gi requests: cpu: 0.5 memory: 500Mi env: - name: LIMITS_MEMORY valueFrom: resourceFieldRef: resource: limits.memory divisor: 1Mi - name: JAVA_OPTS value: -Xmx$(LIMITS_MEMORY)m -XshowSettings:vm -Dhudson.slaves.NodeProvisioner.initialDelay=0 -Dhudson.slaves.NodeProvisioner.MARGIN=50 -Dhudson.slaves.NodeProvisioner.MARGIN0=0.85 volumeMounts: - name: jenkins-home mountPath: /var/jenkins_home livenessProbe: httpGet: path: /login port: 8080 initialDelaySeconds: 60 timeoutSeconds: 5 readinessProbe: httpGet: path: /login port: 8080 initialDelaySeconds: 60 timeoutSeconds: 5 volumes: - name: jenkins-home persistentVolumeClaim: claimName: nfs-jenkins-pvc 注意几个地方： 1. imagePullSecrets: - name: registry-pull-secret 2. image: 192.168.3.27:8888/ops/jenkins:lts-alpine 3. volumes: - name: jenkins-home persistentVolumeClaim: claimName: nfs-jenkins-pvc 部署service apiVersion: v1 kind: Service metadata: name: jenkins annotations: spec: type: NodePort selector: name: jenkins ports: - name: http port: 80 targetPort: 8080 protocol: TCP nodePort: 30001 - name: agent port: 50000 protocol: TCP 最终要达到的效果 [root@master k8s-jenkins]# kubectl get pvc,statefulset,pod,svc | grep jenkins NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/nfs-jenkins-pvc Bound pvc-219bc06f-f3a7-11e8-bc01-525400088c01 10Gi RWO nfs-dynamic-class 14d NAME DESIRED CURRENT AGE statefulset.apps/jenkins 1 1 14d NAME READY STATUS RESTARTS AGE pod/jenkins-0 1/1 Running 1 4d pod/nfs-client-provisioner-6db86bc775-5zbn7 1/1 Unknown 21 23d pod/nfs-client-provisioner-6db86bc775-lb945 1/1 Running 7 3d NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/jenkins NodePort 10.1.210.74 80:30001/TCP,50000:20990/TCP 14d service/kubernetes ClusterIP 10.1.0.1 443/TCP 55d 参考： https://github.com/jenkinsci/kubernetes-plugin/tree/fc40c869edfd9e3904a9a56b0f80c5a25e988fa1/src/main/kubernetes Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"devops/2.配置jenkins-master.html":{"url":"devops/2.配置jenkins-master.html","title":"2.配置jenkins-master","keywords":"","body":" 配置jenkins-master 配置所需的插件 配置全局凭据 用于svn的checkout代码的密码 用于harbor的pull的密码 kubectl的config 在jenkins中配置k8s集群的信息 配置jenkins-master 配置所需的插件 Kubernetes Continuous Deploy Kubernetes Extended Choice Parameter pipeline scm 配置全局凭据 用于svn的checkout代码的密码 用于harbor的pull的密码 kubectl的config 在jenkins中配置k8s集群的信息 在jenkins的系统管理-系统设置中配置 需要做一个测试,测试成功说明配置正确。 其他的暂时不用配置，会在后面的jenkinsfile中说明。 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"devops/3.制作jenkins-slave.html":{"url":"devops/3.制作jenkins-slave.html","title":"3.制作jenkins-slave","keywords":"","body":" 制作jenkins-slave for java的镜像 制作基础镜像os2 制作基于os2的jenkins-slave 制作jenkins-slave for java的镜像 制作基础镜像os2 FROM debian:stretch MAINTAINER suredandan xrzp@qq.com ENV TIMEZONE=Asia/Shanghai \\ LANG=zh_CN.UTF-8 RUN echo \"${TIMEZONE}\" > /etc/timezone \\ && echo \"$LANG UTF-8\" > /etc/locale.gen \\ && apt-get update -q \\ && ln -sf /usr/share/zoneinfo/${TIMEZONE} /etc/localtime \\ && mkdir -p /home/jenkins/.jenkins \\ && mkdir -p /home/jenkins/agent \\ && mkdir -p /usr/share/jenkins # COPY chhostname.sh /usr/local/bin/chhostname.sh # java/locale/DinD/svn/jnlp RUN DEBIAN_FRONTEND=noninteractive apt-get install -yq vim wget curl apt-utils dialog locales apt-transport-https build-essential bzip2 ca-certificates sudo jq unzip zip gnupg2 software-pr operties-common \\ && update-locale LANG=$LANG \\ && locale-gen $LANG \\ && DEBIAN_FRONTEND=noninteractive dpkg-reconfigure locales \\ && curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/debian/gpg | sudo apt-key add - \\ && add-apt-repository \"deb [arch=amd64] https://mirrors.aliyun.com/docker-ce/linux/debian $(lsb_release -cs) stable\" \\ && apt-get update -y \\ && apt-get install -y docker-ce \\ && apt-get install -y subversion \\ && groupadd -g 10000 jenkins \\ && useradd -c \"Jenkins user\" -d $HOME -u 10000 -g 10000 -m jenkins \\ && usermod -a -G docker jenkins \\ && sed -i '/^root/a\\jenkins ALL=(ALL:ALL) NOPASSWD:ALL' /etc/sudoers USER root WORKDIR /home/jenkins docker build -t 192.168.3.27:8888/ops/os2 . docker push 192.168.3.27:8888/ops/os2 制作基于os2的jenkins-slave [root@master jenkins_slave_2]# ls -l total 247672 -rw-r--r-- 1 root root 8799579 Dec 1 21:22 apache-maven-3.5.3-bin.tar.gz -rw------- 1 root root 6347 Dec 5 11:47 config -rw-r--r-- 1 root root 514 Dec 4 23:40 Dockerfile drwxr-xr-x 8 10 143 255 Sep 6 2017 jdk -rw-r--r-- 1 root root 189736377 Dec 1 16:29 jdk-8u151-linux-x64.tar.gz -rwxr-xr-x 1 root root 2033 Dec 4 13:33 jenkins-slave -rwxr-xr-x 1 root root 54277604 Dec 1 16:28 kubectl drwxr-xr-x 6 root root 99 Dec 1 21:22 maven -rw-r--r-- 1 root root 770802 Dec 1 16:24 slave.jar Dockerfile如下: FROM 192.168.3.27:8888/ops/os2 MAINTAINER suredandan xrzp@qq.com RUN mkdir -p /usr/local/maven \\ && mkdir -p /usr/local/jdk \\ && mkdir -p /root/.kube COPY jdk /usr/local/jdk COPY maven /usr/local/maven COPY kubectl /usr/local/bin/kubectl COPY jenkins-slave /usr/local/bin/jenkins-slave COPY slave.jar /usr/share/jenkins COPY config /root/.kube/ ENV JAVA_HOME=/usr/local/jdk \\ MAVEN_HOME=/usr/local/maven \\ PATH=/usr/local/jdk/bin:/usr/local/maven/bin:$PATH docker build -t 192.168.3.27:8888/ops/jenkins-slave4 . docker push 192.168.3.27:8888/ops/jenkins-slave4 说明: 目录jdk是jdk-8u151-linux-x64.tar.gz解压后的目录,目的是让每个slave都在镜像中就有jdk环境 目录maven是apache-maven-3.5.3-bin.tar.gz解压后的目录,目的是让每个slave都有mvn环境 kubectl是二进制的kubectl文件 jenkins-slave 是 github的jenkinsci 提供的执行文件 slave.jar 是部署了jenkins 之后jenkins自己提供的一个jar包 一般路径为：http://yourserver:port/jnlpJars/slave.jar 在本文环境下的路径为: http://192.168.3.28:30001/jnlpJars/slave.jar config文件是k8s集群下 /root/.kube/config Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"devops/4.创建pipeline项目.html":{"url":"devops/4.创建pipeline项目.html","title":"4.创建pipeline项目","keywords":"","body":" 创建pipeline项目 在新建任务的地方创建pipeline的任务 创建后进去编辑任务的信息: 创建pipeline项目 在新建任务的地方创建pipeline的任务 创建后进去编辑任务的信息: 进入项目配置的地方,只需要写一个要打包项目的svn地址,因为我们将采用jenkinsfile的方式,将所有的流程信息都写入到这个配置文件中。当我们编译这个项目的时候，首先是去SVN上获取到jenkinsfile，然后根据里面编写的流程再来进行流程的执行。 在SVN的项目下,有个文件夹叫:deploy,下面的文件如下: Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"devops/5.编写jenkinsfile文件.html":{"url":"devops/5.编写jenkinsfile文件.html","title":"5.编写jenkinsfile文件","keywords":"","body":"编写jenkinsfile 最重要的一步来了! 首先看看jenkinsfile里面的信息 podTemplate(label: 'jnlp-slave', cloud: 'kubernetes', containers: [ containerTemplate( name: 'jnlp', image: '192.168.3.27:8888/ops/jenkins-slave4', alwaysPullImage: true ), ], volumes: [ hostPathVolume(mountPath: '/var/run/docker.sock', hostPath: '/var/run/docker.sock'), hostPathVolume(mountPath: '/usr/bin/docker', hostPath: '/usr/bin/docker') ], imagePullSecrets: ['registry-pull-secret'] ) { node(\"jnlp-slave\"){ stage('SVN Checkout'){ checkout([$class: 'SubversionSCM', additionalCredentials: [], excludedCommitMessages: '', excludedRegions: '', excludedRevprop: '', excludedUsers: '', filterChangelog: false, ignoreDirPropChanges: false, includedRegions: '', locations: [[cancelProcessOnExternalsFail: true, credentialsId: '0ead2dca-f930-453a-b142-389c89801271', depthOption: 'infinity', ignoreExternalsOption: true, local: '.', remote: 'https://192.168.xxx.xxx:8443/svn/xxxxx/trunk/05.Code/devPortal/webapp']], quietOperation: true, workspaceUpdater: [$class: 'UpdateUpdater']]) } stage('Unit Testing'){ echo \"Unit Testing...\" } stage('Maven Build'){ echo \"Maven Build...\" sh \"/usr/local/maven/bin/mvn clean package --settings=/usr/local/maven/conf/settings.xml -Dmaven.test.skip=true\" } stage('Build and Push Image'){ sh ''' docker login -u yfzx -p abcdefg123456 192.168.3.27:8888 docker build -t 192.168.3.27:8888/project/devportal -f deploy/Dockerfile . docker push 192.168.3.27:8888/project/devportal ''' } stage('Deploy to K8S'){ sh ''' kubectl --kubeconfig=/root/.kube/config apply -f deploy/deploy.yaml ''' } stage('Testing'){ echo \"Testing...\" } } } Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"devops/6.验证k8s-jenkinsfile.html":{"url":"devops/6.验证k8s-jenkinsfile.html","title":"6.验证k8s-jenkinsfile","keywords":"","body":"验证 : [root@master jenkins_slave_2]# kubectl get deploy,pod NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deployment.extensions/devportal-web 1 1 1 1 8d deployment.extensions/nfs-client-provisioner 1 1 1 1 46d NAME READY STATUS RESTARTS AGE pod/devportal-web-b8645fdfc-kwb6f 1/1 Running 0 3d pod/devportal-web-b8645fdfc-vrzcg 1/1 Unknown 0 5d pod/jenkins-0 1/1 Running 1 4d pod/nfs-client-provisioner-6db86bc775-5zbn7 1/1 Unknown 21 23d pod/nfs-client-provisioner-6db86bc775-lb945 1/1 Running 7 3d Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"devops/7.优化改进.html":{"url":"devops/7.优化改进.html","title":"7.优化改进","keywords":"","body":"优化改进 制作镜像的过程 最初打包是把slave和基础镜像放在一起的,编译一次大概花一个小时 镜像的大小 # docker images REPOSITORY TAG IMAGE ID CREATED SIZE 192.168.3.27:8888/ops/jenkins-slave4 nojdkmvn f23c25c4f035 8 days ago 994MB 192.168.3.27:8888/ops/jenkins-slave4 latest 28b9adee04a7 8 days ago 1.39GB 代码仓库的共享 安全性 stage('Build and Push Image'){ sh ''' docker login -u yfzx -p abcdefg123456 192.168.3.27:8888 docker build -t 192.168.3.27:8888/project/devportal -f deploy/Dockerfile . docker push 192.168.3.27:8888/project/devportal ''' } stage('Build and Push Image'){ withCredentials([usernamePassword(credentialsId: '703d4800-fda1-4efc-beb4-26812f68002d', passwordVariable: 'harborpassword', usernameVariable: 'harborusername')]) { sh ''' docker login -u ${harborusername} -p ${harborpassword} 192.168.3.27:8888 docker build -t 192.168.3.27:8888/project/devportal -f deploy/Dockerfile . docker push 192.168.3.27:8888/project/devportal ''' } } Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"helm/helm.html":{"url":"helm/helm.html","title":"helm","keywords":"","body":"参考： 初试 Kubernetes 集群中使用 Helm 搭建 Spinnaker 平台 在 KubeCon 大会上备受关注的 K8S Helm 到底是什么？ Helm简介 K8S集群中使用Helm管理应用分发 helm-安装与使用 http://blog.51cto.com/13995002/2287546 Kubernetes上如何使用Helm 使用Helm部署Prometheus和Grafana监控Kubernetes Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"helm/helm安装.html":{"url":"helm/helm安装.html","title":"helm安装及简单使用","keywords":"","body":" helm的安装 一、helm介绍 1.什么是 Helm Charts 2.为什么需要 Helm Charts 二、安装helm 1.获取安装包 2.安装server 使用helm部署一个应用 创建服务账号 创建集群的角色绑定 为应用程序设置serviceAccount 仓库的添加与删除 搜索Helm应用 查看应用 安装应用 使用fetch进行安装 更新release 查看指定release的历史部署版本信息 版本回滚 四、参考 helm的安装 一、helm介绍 1.什么是 Helm Charts Helm Charts是 Kubernetes 项目中的一个子项目（https://github.com/kubernetes/helm） 目的是提供 Kubernetes 的包管理平台。Helm 能够帮你管理 Kubernetes 的应用集合。Helm Charts 能够帮你定义，安装，升级最复杂的 Kubernetes 应用集合。Helm Charts 很容易创建，做版本化，共享和发布，最新版本的 Helm 由 CNCF 进行维护，目前在业界已经有大量的公司在使用 Helm，其中包括谷歌，微软，Bitnami 等大型企业。 Helm 由两部分组成： Helm 客户端。它用来创建，拉取，搜索和验证 Charts，初始化 Tiller 服务。 Tiller server。跑在 Kubernetes 集群之内，管理这些应用的发布。 2.为什么需要 Helm Charts 很多Kubernetes 的使用者在部署一些通用组件时，例如数据库，CI 工具，缓存组件，负载均衡等等，通常会需求一些业界统一的最佳实践进行应用的配置。 目前市面上有很多仓库提供这样的最佳实践，例如：Kubernetes 的contrib repository，Helm Charts Repository (https://github.com/kubernetes/charts)，Bitnami Charts Repository. 面对如此众多的来源，用户更希望有一个统一的入口去管理所有 Helm Charts仓库，于是就有了 Helm 私有仓库的诞生。 它的好处在于： 管理复杂的应用集合Charts 能够描述最复杂的应用，提供可重复，幂等性的安装，以及提供统一的认证中心服务。 容易升级为团队提供实时的镜像升级，以及自定义 webhook，解决镜像升级的痛点。 企业内部共享Charts能够很容易的进行版本化，共享，在企业内部提供私有Heml 仓库服务，解决了从官方源拉镜像速度奇慢的痛点。 回滚使用 Helm 可以方便的进行应用的回滚，回到之前的 Release 版本。 目前在Kubernetes Charts repository有两个主要的目录Stable 和Incubator，Stable 里有近百种应用，例如：Artifactory, Jenkins，Mysql，MariaDB，Consul,Gitlab, Grafana,Nginx 等等。Incubator 里也有cassandra，Kafka，zk 等等知名软件，并且 Stable 仓库会持续的更新，提供更为便捷的应用配置管理。 二、安装helm 1.获取安装包 在官网获取helm的最新版本或者其他版本 https://github.com/helm/helm/releases 我在这儿选择了最新的版本helm-v2.11.0 [root@master src]# wget https://storage.googleapis.com/kubernetes-helm/helm-v2.11.0-linux-amd64.tar.gz [==================================================>] 19,149,273 6.58MB/s in 2.8s 2018-10-19 15:27:50 (6.58 MB/s) - ‘helm-v2.11.0-linux-amd64.tar.gz’ saved [19149273/19149273] 然后就是解压，将二进制包放在/usr/local/bin或者/etc/kubernetes/bin下 需要说明的是，这一步其实就是安装helm的client端 2.安装server 接下来就是初始化helm server 因为gfw的原因，所以需要要使用阿里云提供的镜像仓库，可以去 https://dev.aliyun.com/ 查看 初始化命令： helm init --upgrade -i \\ registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.11.0 \\ --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts 上面的命令实质就是在k8s上部署了一个deployment，这个deployment的名字 tiller，它其实就是helm的服务端。 查看一下这个pod # kubectl get pod -n kube-system |grep till tiller-deploy-cd5cf5bb6-xbjfl 1/1 Running 0 2m 服务端和客户端安装好之后，看看版本: # helm version Client: &version.Version{SemVer:\"v2.11.0\", GitCommit:\"2e55dbe1fdb5fdb96b75ff144a339489417b146b\", GitTreeState:\"clean\"} Server: &version.Version{SemVer:\"v2.11.0\", GitCommit:\"2e55dbe1fdb5fdb96b75ff144a339489417b146b\", GitTreeState:\"clean\"} 注意：如果tiller的镜像不能正常的拉取或启动的话，会显示如下： [root@master bin]# helm version Client: &version.Version{SemVer:\"v2.11.0\", GitCommit:\"2e55dbe1fdb5fdb96b75ff144a339489417b146b\", GitTreeState:\"clean\"} Error: could not find a ready tiller pod 提示： 所谓client端其实就是那个helm二进制包，用来执行一些列的 helm xxx 命令的 所谓server端其实就是那个pod（helm tiller） 另外，需要在每个node节点需要安装socat ``` yum install -y socat 否则，会报类似的错误： unable to do port forwarding: socat not found. Error: cannot connect to Tiller ``` 使用helm部署一个应用 创建服务账号 # kubectl create serviceaccount --namespace kube-system tiller serviceaccount \"tiller\" created 创建集群的角色绑定 # kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller clusterrolebinding.rbac.authorization.k8s.io \"tiller-cluster-rule\" created 为应用程序设置serviceAccount # kubectl patch deploy --namespace kube-system tiller-deploy -p '{\"spec\":{\"template\":{\"spec\":{\"serviceAccount\":\"tiller\"}}}}' deployment.extensions \"tiller-deploy\" patched 仓库的添加与删除 查看仓库 # helm repo list NAME URL stable https://kubernetes-charts.storage.googleapis.com local http://127.0.0.1:8879/charts 删除仓库 helm repo remove stable 添加仓库 helm repo add ali-apphub https://apphub.aliyuncs.com 搜索Helm应用 # helm search jenkins NAME CHART VERSION APP VERSION DESCRIPTION stable/jenkins 0.13.5 2.73 Open source continuous integration server. It s... # helm repo list NAME URL stable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts local http://127.0.0.1:8879/charts 查看应用 helm inspect stable/jenkins helm inspect values stable/jenkins 安装应用 # helm install stable/jenkins 使用fetch进行安装 有时候需要修改一些参数,所以可以先把安装包fetch下来,修改后再进行安装 helm fetch stable/redis cd 对应的目录中 helm install -n redis-sure -f values.yaml . 更新release helm upgrade mysql -f mysql/values.yaml --set resources.requests.memory=1024Mi mysql 查看指定release的历史部署版本信息 helm hist mysql 版本回滚 helm rollback --debug mysql 1 四、参考 官方文档 https://docs.helm.sh/using_helm Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"helm/helm-FAQ.html":{"url":"helm/helm-FAQ.html","title":"helm安装FAQ","keywords":"","body":" 安装 FAQ 下载 安装 入门 升级 卸载 安装 FAQ 本节跟踪安装或开始使用 Helm 时遇到的一些经常遇到的问题。 欢迎你的帮助 来更好的提供此文档。要添加，更正或删除信息，提出问题 issue 或向我们发送 PR 请求。 下载 我想知道更多关于我的下载选项。 问：我无法获得最新 Helm 的 GitHub 发布。他们在哪？ 答：我们不再使用 GitHub 发布版本。二进制文件现在存储在 GCS 公共存储区中 GCS public bucket。 问：为什么没有 Debian/Fedora/... Helm 的原生的软件包？ 我们很乐意提供这些信息，或者指向可靠的提供商。如果你对帮助感兴趣，我们很乐意。这就是 Homebrew 式的开始。 问：你为什么要提供一个 curl ...|bash 脚本？ 答：我们的 repo 库（scripts/get）中有一个脚本可以作为 curl ..|bash 脚本执行。这些传输全部受 HTTPS 保护，并且脚本会对其获取的包进行一些审计。但是，脚本具有任何 shell 脚本的所有常见危险。 我们提供它是因为它很有用，但我们建议用户先仔细阅读脚本。并且，我们真正喜欢的是 Helm 的的打包版本。 安装 我正在尝试安装 Helm/Tiller，但有些地方出了问题。 问：我如何将 Helm 客户端文件放在~/.helm 以外的地方？ 设置 $HELM_HOME 环境变量，然后运行 helm init： export HELM_HOME=/some/path helm init --client-only 注意，如果你有现有的 repo 存储库，则需要通过 helm repo add.... 重新添加它们。 问：我如何配置 Helm，但不安装 Tiller？ 答：默认情况下，helm init 将确认本​​地 $HELM_HOME 配置，然后在群集上安装 Tiller。要本地配置，但不安装 Tiller，请使用 helm init --client-only。 问：如何在集群上手动安装 Tiller？ 答：Tiller 是作为 Kubernetes deployment 安装的。可以通过运行 helm init --dry-run --debug 获取 manifest，然后通过 kubectl 手动安装 。建议不要删除或更改该 deployment 中的标签 labels，因为它们有时支持脚本和工具需要用到。 问：为什么安装 Tiller 期间报错误 Error response from daemon: target is unknown？ 答：有用户报告无法在使用 Docker 1.13.0 的 Kubernetes 实例上安装 Tiller。造成这种情况的根本原因是 Docker 中的一个错误，它使得一个版本与早期版本的 Docker 推送到 Docker 注册表的镜像不兼容。 该问题在发布后不久就已修复，并在 Docker 1.13.1-RC1 和更高版本中提供。 入门 我成功安装了 Helm/Tiller，但我使用时碰到问题。 问：使用 Helm 时，收到错误 “客户端传输中断” E1014 02:26:32.885226 16143 portforward.go:329] an error occurred forwarding 37008 -> 44134: error forwarding port 44134 to pod tiller-deploy-2117266891-e4lev_kube-system, uid : unable to do port forwarding: socat not found. 2016/10/14 02:26:32 transport: http2Client.notifyError got notified that the client transport was broken EOF. Error: transport is closing 答：这通常表明 Kubernetes 未设置为允许端口转发。 通常情况下，缺少的部分是 socat。如果正在运行 CoreOS，我们被告知它可能在安装时配置错误。CoreOS 团队建议阅读以下内容： https://coreos.com/kubernetes/docs/latest/kubelet-wrapper.html 以下是一些解决的问题案例，可以帮助开始使用： https://github.com/kubernetes/helm/issues/1371 https://github.com/kubernetes/helm/issues/966 Q：使用 Helm 时, 报错误 \"lookup XXXXX on 8.8.8.8:53: no such host\" Error: Error forwarding ports: error upgrading connection: dial tcp: lookup kube-4gb-lon1-02 on 8.8.8.8:53: no such host 答：我们在 Ubuntu 和 Kubeadm 多节点群集中有这个问题。问题原因是节点期望某些 DNS 记录可以通过全局 DNS 获得。在上游解决此问题之前，可以按照以下方式解决该问题。在每个控制平面节点上： 添加条目到 /etc/hosts，将主机名映射到其 public IP 安装 dnsmasq（例如 apt install -y dnsmasq） 删除 k8s api 服务容器（kubelet 会重新创建它） 然后 systemctl restart docker（或重新启动节点）请 / etc/resolv.conf 更改 请参阅此问题以获取更多信息：https://github.com/kubernetes/helm/issues/1455 问：在 GKE（Google Container Engine）上，报错 \"No SSH tunnels currently open\" Error: Error forwarding ports: error upgrading connection: No SSH tunnels currently open. Were the targets able to accept an ssh-key for user \"gke-[redacted]\"? 错误消息的另一个形式是： Unable to connect to the server: x509: certificate signed by unknown authority 答：这个问题是你的本地 Kubernetes 配置文件必须具有正确的凭据。 在 GKE 上创建集群时，它将提供凭证，包括 SSL 证书和证书颁发机构信息。这些需要存储在一个 Kubernetes 配置文件中（默认：~/.kube/config，这样 kubectl 和 helm 可以访问它们）。 问：当我运行 Helm 命令时，出现有关隧道 tunnel 或代理 proxy 的错误 答：Helm 使用 Kubernetes 代理服务连接到 Tiller 服务器。如果命令 kubectl proxy 不适用，Helm 也不行。通常，错误与缺失的 socat 服务有关。 问：Tiller 崩溃 当我在 Helm 上运行命令时，Tiller 崩溃时会出现如下错误： Tiller is listening on :44134 Probes server is listening on :44135 Storage driver is ConfigMap Cannot initialize Kubernetes connection: the server has asked for the client to provide credentials 2016-12-20 15:18:40.545739 I | storage.go:37: Getting release \"bailing-chinchilla\" (v1) from storage panic: runtime error: invalid memory address or nil pointer dereference [signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x8053d5] goroutine 77 [running]: panic(0x1abbfc0, 0xc42000a040) /usr/local/go/src/runtime/panic.go:500 +0x1a1 k8s.io/helm/vendor/k8s.io/kubernetes/pkg/client/unversioned.(*ConfigMaps).Get(0xc4200c6200, 0xc420536100, 0x15, 0x1ca7431, 0x6, 0xc42016b6a0) /home/ubuntu/.go_workspace/src/k8s.io/helm/vendor/k8s.io/kubernetes/pkg/client/unversioned/configmap.go:58 +0x75 k8s.io/helm/pkg/storage/driver.(*ConfigMaps).Get(0xc4201d6190, 0xc420536100, 0x15, 0xc420536100, 0x15, 0xc4205360c0) /home/ubuntu/.go_workspace/src/k8s.io/helm/pkg/storage/driver/cfgmaps.go:69 +0x62 k8s.io/helm/pkg/storage.(*Storage).Get(0xc4201d61a0, 0xc4205360c0, 0x12, 0xc400000001, 0x12, 0x0, 0xc420200070) /home/ubuntu/.go_workspace/src/k8s.io/helm/pkg/storage/storage.go:38 +0x160 k8s.io/helm/pkg/tiller.(*ReleaseServer).uniqName(0xc42002a000, 0x0, 0x0, 0xc42016b800, 0xd66a13, 0xc42055a040, 0xc420558050, 0xc420122001) /home/ubuntu/.go_workspace/src/k8s.io/helm/pkg/tiller/release_server.go:577 +0xd7 k8s.io/helm/pkg/tiller.(*ReleaseServer).prepareRelease(0xc42002a000, 0xc42027c1e0, 0xc42002a001, 0xc42016bad0, 0xc42016ba08) /home/ubuntu/.go_workspace/src/k8s.io/helm/pkg/tiller/release_server.go:630 +0x71 k8s.io/helm/pkg/tiller.(*ReleaseServer).InstallRelease(0xc42002a000, 0x7f284c434068, 0xc420250c00, 0xc42027c1e0, 0x0, 0x31a9, 0x31a9) /home/ubuntu/.go_workspace/src/k8s.io/helm/pkg/tiller/release_server.go:604 +0x78 k8s.io/helm/pkg/proto/hapi/services._ReleaseService_InstallRelease_Handler(0x1c51f80, 0xc42002a000, 0x7f284c434068, 0xc420250c00, 0xc42027c190, 0x0, 0x0, 0x0, 0x0, 0x0) /home/ubuntu/.go_workspace/src/k8s.io/helm/pkg/proto/hapi/services/tiller.pb.go:747 +0x27d k8s.io/helm/vendor/google.golang.org/grpc.(*Server).processUnaryRPC(0xc4202f3ea0, 0x28610a0, 0xc420078000, 0xc420264690, 0xc420166150, 0x288cbe8, 0xc420250bd0, 0x0, 0x0) /home/ubuntu/.go_workspace/src/k8s.io/helm/vendor/google.golang.org/grpc/server.go:608 +0xc50 k8s.io/helm/vendor/google.golang.org/grpc.(*Server).handleStream(0xc4202f3ea0, 0x28610a0, 0xc420078000, 0xc420264690, 0xc420250bd0) /home/ubuntu/.go_workspace/src/k8s.io/helm/vendor/google.golang.org/grpc/server.go:766 +0x6b0 k8s.io/helm/vendor/google.golang.org/grpc.(*Server).serveStreams.func1.1(0xc420124710, 0xc4202f3ea0, 0x28610a0, 0xc420078000, 0xc420264690) /home/ubuntu/.go_workspace/src/k8s.io/helm/vendor/google.golang.org/grpc/server.go:419 +0xab created by k8s.io/helm/vendor/google.golang.org/grpc.(*Server).serveStreams.func1 /home/ubuntu/.go_workspace/src/k8s.io/helm/vendor/google.golang.org/grpc/server.go:420 +0xa3 答：请检查 Kubernetes 的安全设置。 Tiller 中的崩溃几乎总是由于未能与 Kubernetes API 服务器进行协商而导致的结果（此时，Tiller 功能不正常，因此崩溃并退出）。 通常，这是认证失败的结果，因为运行 Tiller 的 Pod 没有正确的令牌 token。 要解决这个问题，你需要修改 Kubernetes 配置。确保 --service-account-private-key-file 从 controller-manager 和 --service-account-key-file 从 API 服务器指向同一个 X509 RSA 密钥。 升级 我的 Helm 原来工作正常，然后我升级了。现在它工作不正常。 问：升级后，我收到错误 “Client version is incompatible”。怎么问题？ Tiller 和 Helm 必须协商一个通用版本，以确保他们可以安全地进行通信而不会违反 API 假设。该错误意味着版本差异太大而无法安全地继续。通常，需要为此手动升级 Tiller。 该安装指南 Installation Guide 有关于安全 Helm 升级和 Tiller 的详细信息。 版本号的规则如下： 预发布版本与其他一切不兼容。Alpha.1 与... 不相容 Alpha.2。 修补程序版本兼容：1.2.3 与 1.2.4 兼容 少量修订不兼容：1.2.0 与 1.3.0 不兼容，但我们可能在未来放宽这一限制。 主要版本不兼容：1.0.0 与 2.0.0 不兼容。 卸载 我正在尝试删除某些东西。 问：当我删除 Tiller deployment 时，为何所有安装的 release 信息还在集群里？ 安装 release 信息存储在 kube-system 名称空间内的 ConfigMaps 中。需要手动删除它们以删除记录或使用 helm delete --purge。 问：我想删除我的本地 Helm。它的所有文件在哪里？ 包括helm二进制文件，Helm存储了一些文件在$HELM_HOME，默认位于~/.helm。 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"helm/try-helmv3.html":{"url":"helm/try-helmv3.html","title":"helmV3版本试用及新特性","keywords":"","body":" helm v3的特性 V3带来的变化 Release 不再是全局资源 Values 支持 JSON Schema 校验器 推送 Charts 到容器镜像仓库中 代码复用 - Library Chart 支持 其他的一些变化 安装过程 使用过程 添加仓库 安装应用 其他一些命令 roadmap 记录一下helm v3版本的新特性以及试用情况 helm v2与v3可以共存,大家都用的是~/.helm helm v3的特性 最大特性就是去掉了tiller，使用`~/.kube/config作为与集群的通信的桥梁 在 Helm 2 中，Tiller 是作为一个 Deployment 部署在 kube-system 命名空间中，很多情况下，我们会为 Tiller 准备一个 ServiceAccount ，这个 ServiceAccount 通常拥有集群的所有权限。用户可以使用本地 Helm 命令，自由地连接到 Tiller 中并通过 Tiller 创建、修改、删除任意命名空间下的任意资源。 然而在多租户场景下，这种方式也会带来一些安全风险，我们即要对这个 ServiceAccount 做很多剪裁，又要单独控制每个租户的控制，这在当前的 Tiller 模式下看起来有些不太可能。 于是在 Helm 3 中，Tiller 被移除了。新的 Helm 客户端会像 kubectl 命令一样，读取本地的 kubeconfig 文件，使用我们在 kubeconfig 中预先定义好的权限来进行一系列操作。这样做法即简单，又安全。 虽然 Tiller 文件被移除了，但 Release 的信息仍在集群中以 ConfigMap 的方式存储，因此体验和 Helm 2 没有区别。 V3带来的变化 Release 不再是全局资源 在 Helm 2 中，Tiller 自身部署往往在 kube-system 下，虽然不一定是 cluster-admin 的全局管理员权限，但是一般都会有 kube-system 下的权限。 当 Tiller 想要存储一些信息的时候，它被设计成在 kube-system 下读写 ConfigMap 。 在 Helm 3 中，Helm 客户端使用 kubeconfig 作为认证信息直接连接到 Kubernetes APIServer，不一定拥有 cluster-admin 权限或者写 kube-system 的权限，因此它只能将需要存储的信息存在当前所操作的 Kubernetes Namespace 中，继而 Release 变成了一种命名空间内的资源。 Helm v2存储的release 都在tiller 所在的namespace,而且是以cm进行存储的。 Helm v3，所有的信息都存储在release对应的namespace 下，而且以secret存储。这是v2和v3很不相同的地方。 Values 支持 JSON Schema 校验器 Helm Charts 是一堆 Go Template 文件、一个变量文件 Values 和一些 Charts 描述文件的组合。Go Template 和 Kubernetes 资源描述文件的内容十分灵活，在开发迭代过程中，很容易出现一些变量未定义的问题。Helm 3 引入了 JSON Schema 校验，它支持用一长串 DSL 来描述一个变量文件的格式、检查所有输入的变量的格式。 当我们运行 helm install 、 helm upgrade 、 helm lint 、 helm template 命令时，JSON Schema 的校验会自动运行，如果失败就会立即报错。 推送 Charts 到容器镜像仓库中 互联网上有一些 Helm Charts 托管平台，负责分发常见的开源应用，例如 Helm Hub、KubeApps Hub。 在企业环境中，用户需要私有化的 Helm Charts 托管。最常见的方案是部署 ChartMuseum，使其对接一些云存储。当然也有一些较为小众的方案，比如 App-Regsitry 直接复用了 Docker 镜像仓库 Registry V2，再比如 helm-s3 直接通过云存储 S3 存取 Charts 文件。 如今在 Helm 3，Helm 直接支持了推送 Charts 到容器镜像仓库的能力，希望支持满足 OCI 标准的所有 Registry。但这部分还没有完全被开发完，会在未来的 alpha 版本中被完善。 docker run -dp 5000:5000 --restart=always --name registry registry:2 然后下载一个chart包helm fetch stable/wordpress # helmv3 chart save wordpress localhost:5000/wordpress:latest Name: wordpress Version: 0.6.13 Meta: sha256:83c48dd3c01a2952066ead67023ea14963a88db4287650baad5ea1ddd8ff9590 Content: sha256:248c8c68f4f614003c8b1a9d78787e5f07e979e9b996981df993cf380f498c97 latest: saved # ./helmv3 chart list REF NAME VERSION DIGEST SIZE CREATED localhost:5000/wordpress:latest wordpress 0.6.13 248c8c6 12.0 KiB 11 seconds # helmv3 chart push localhost:5000/wordpress:latest The push refers to repository [localhost:5000/wordpress] Name: wordpress Version: 0.6.13 Meta: sha256:83c48dd3c01a2952066ead67023ea14963a88db4287650baad5ea1ddd8ff9590 Content: sha256:248c8c68f4f614003c8b1a9d78787e5f07e979e9b996981df993cf380f498c97 latest: pushed to remote (2 layers, 12.6 KiB total) 代码复用 - Library Chart 支持 Helm 3 中引入了一种新的 Chart 类型，名为 Library Chart 。它不会部署出一些具体的资源，只能被其他的 Chart 所引用，提高代码的可用复用性。当一个 Chart 想要使用该 Library Chart 内的一些模板时，可以在 Chart.yaml 的 dependencies 依赖项中指定。 其他的一些变化 Go Import 的路径由 k8s.io/helm 变成了 helm.sh/helm 简化了 Chart 内置变量 Capabilities 的一些属性 helm install 不再默认生成一个 Release 的名称，除非指定了 --generate-name 移除了用于本地临时搭建 Chart Repository 的 helm serve 命令 helm delete 更名为 helm uninstall ， helm inspect 更名为 helm show ， helm fetch 更名为 helm pull ，但以上旧的命令当前仍能使用 requirements.yaml 被整合到了 Chart.yaml 中，但格式保持不变 安装过程 安装过程变得非常简单了,只需要安装client即可 # wget https://cloudnativeapphub.oss-cn-hangzhou.aliyuncs.com/helm-v3.0.0-alpha.1-linux-amd64.tar.gz --2019-07-10 09:50:39-- https://cloudnativeapphub.oss-cn-hangzhou.aliyuncs.com/helm-v3.0.0-alpha.1-linux-amd64.tar.gz 正在解析主机 cloudnativeapphub.oss-cn-hangzhou.aliyuncs.com (cloudnativeapphub.oss-cn-hangzhou.aliyuncs.com)... 118.31.219.223 正在连接 cloudnativeapphub.oss-cn-hangzhou.aliyuncs.com (cloudnativeapphub.oss-cn-hangzhou.aliyuncs.com)|118.31.219.223|:443... 已连接。 已发出 HTTP 请求，正在等待回应... 200 OK 长度：12711766 (12M) [application/x-gzip] 正在保存至: “helm-v3.0.0-alpha.1-linux-amd64.tar.gz” 100%[=======================================================================================================================>] 12,711,766 1.31MB/s 用时 9.1s 2019-07-10 09:50:53 (1.34 MB/s) - 已保存 “helm-v3.0.0-alpha.1-linux-amd64.tar.gz” [12711766/12711766]) # ls helm-v3.0.0-alpha.1-linux-amd64.tar.gz # tar -zxf helm-v3.0.0-alpha.1-linux-amd64.tar.gz # cd linux-amd64/ # mv helm /usr/local/bin/helmv3 使用过程 添加仓库 与V2是共享的! # helm repo list NAME URL local http://127.0.0.1:8879/charts microsoft https://mirror.azure.cn/kubernetes/charts/ # helmv3 repo add ali-apphub https://apphub.aliyuncs.com \"ali-apphub\" has been added to your repositories # helm repo list NAME URL local http://127.0.0.1:8879/charts microsoft https://mirror.azure.cn/kubernetes/charts/ ali-apphub https://apphub.aliyuncs.com 安装应用 helmV2安装的应用helmV3看不见，反之亦然！ 先使用v2安装一次 # helm install ali-apphub/guestbook --set service.type=NodePort NAME: invisible-worm LAST DEPLOYED: Wed Jul 10 09:57:57 2019 NAMESPACE: default STATUS: DEPLOYED RESOURCES: ==> v1/Deployment NAME READY UP-TO-DATE AVAILABLE AGE invisible-worm-guestbook 0/2 0 0 0s redis-master 0/1 0 0 0s redis-slave 0/2 0 0 0s ==> v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE invisible-worm-guestbook LoadBalancer 10.101.48.210 3000:25883/TCP 0s redis-master ClusterIP 10.105.253.249 6379/TCP 0s redis-slave ClusterIP 10.105.98.176 6379/TCP 0s NOTES: 1. Get the application URL by running these commands: NOTE: It may take a few minutes for the LoadBalancer IP to be available. You can watch the status of by running 'kubectl get svc -w invisible-worm-guestbook --namespace default' export SERVICE_IP=$(kubectl get svc --namespace default invisible-worm-guestbook -o jsonpath='{.status.loadBalancer.ingress[0].ip}') echo http://$SERVICE_IP:3000 安装完成后查看信息 # helm status invisible-worm LAST DEPLOYED: Wed Jul 10 09:57:57 2019 NAMESPACE: default STATUS: DEPLOYED RESOURCES: ==> v1/Deployment NAME READY UP-TO-DATE AVAILABLE AGE invisible-worm-guestbook 2/2 2 2 13m redis-master 1/1 1 1 13m redis-slave 2/2 2 2 13m ==> v1/Pod(related) NAME READY STATUS RESTARTS AGE invisible-worm-guestbook-cbc8b5c8-bwwg4 2/2 Running 0 13m invisible-worm-guestbook-cbc8b5c8-xb492 2/2 Running 0 13m redis-master-9954bc48d-w4kvk 2/2 Running 0 13m redis-slave-75f9c8bc49-msgnm 2/2 Running 0 13m redis-slave-75f9c8bc49-wzn5q 2/2 Running 0 13m ==> v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE invisible-worm-guestbook NodePort 10.101.48.210 3000:25883/TCP 13m redis-master ClusterIP 10.105.253.249 6379/TCP 13m redis-slave ClusterIP 10.105.98.176 6379/TCP 13m NOTES: 1. Get the application URL by running these commands: NOTE: It may take a few minutes for the LoadBalancer IP to be available. You can watch the status of by running 'kubectl get svc -w invisible-worm-guestbook --namespace default' export SERVICE_IP=$(kubectl get svc --namespace default invisible-worm-guestbook -o jsonpath='{.status.loadBalancer.ingress[0].ip}') echo http://$SERVICE_IP:3000 而用V3是看不见这个已经安装的应用的 # helmv3 list NAME NAMESPACE REVISION UPDATED STATUS CHART 再使用v3安装一次 # helmv3 install ali-apphub/guestbook --set service.type=NodePort Error: must either provide a name or specify --generate-name V3的一个特性就是必需要给应用娶一个名字，或者使用--generate-name选项来自动生成一个名字. 加上名字,重新安装 # helmv3 install gusetbook ali-apphub/guestbook --set service.type=NodePort NAME: gusetbook LAST DEPLOYED: 2019-07-10 10:13:20.098413164 +0800 CST m=+5.744019735 NAMESPACE: sure STATUS: deployed NOTES: 1. Get the application URL by running these commands: NOTE: It may take a few minutes for the LoadBalancer IP to be available. You can watch the status of by running 'kubectl get svc -w gusetbook-guestbook --namespace sure' export SERVICE_IP=$(kubectl get svc --namespace sure gusetbook-guestbook -o jsonpath='{.status.loadBalancer.ingress[0].ip}') echo http://$SERVICE_IP:3000 安装完成后查看信息: # helmv3 status gusetbook NAME: gusetbook LAST DEPLOYED: 2019-07-10 10:13:20.098413164 +0800 CST NAMESPACE: sure STATUS: deployed NOTES: 1. Get the application URL by running these commands: NOTE: It may take a few minutes for the LoadBalancer IP to be available. You can watch the status of by running 'kubectl get svc -w gusetbook-guestbook --namespace sure' export SERVICE_IP=$(kubectl get svc --namespace sure gusetbook-guestbook -o jsonpath='{.status.loadBalancer.ingress[0].ip}') echo http://$SERVICE_IP:3000 其他一些命令 helm delete 更名为 helm uninstall helm inspect 更名为 helm show helm fetch 更名为 helm pull 但以上旧的命令当前仍能使用 roadmap alpha1: 移除Tiller， 提供Library charts, 存储格式改成secret， 开始OCI集成工作 alpha2: 更好的 OCI 集成，Lua 模板支持 alpha3: 重构更新策略（可能是客户端侧进行，也可能是服务端侧进行） Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"3rdui/rancher的使用.html":{"url":"3rdui/rancher的使用.html","title":"rancher的使用","keywords":"","body":"安装rancher server docker run -d --restart=unless-stopped -p 80:80 -p 443:443 rancher/rancher:latest PS: docker run xxx -p 外部访问端口:容器内部暴露的端口 -P 是暴露所有的端口 可以用暴露所有端口的方式来检查为啥自己设置的不对 导入已有的集群 创建角色 # kubectl create clusterrolebinding cluster-admin-binding --clusterrole cluster-admin --user [USER_ACCOUNT] Error from server (AlreadyExists): clusterrolebindings.rbac.authorization.k8s.io \"cluster-admin-binding\" already exists kubectl create clusterrolebinding cluster-admin-binding --clusterrole cluster-admin --user admin 这一句其实就是做的下面这个操作 apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-crb roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - name: admin apiGroup: rbac.authorization.k8s.io 在每个node节点上执行 [root@node1 ~]# curl --insecure -sfL https://192.168.3.27/v3/import/74gqt8ksjkv4fwjj67x24zs69d59mxpsddbs4b8g4frh7tgmcj9pcg.yaml | kubectl apply -f - namespace \"cattle-system\" configured serviceaccount \"cattle\" unchanged clusterrolebinding.rbac.authorization.k8s.io \"cattle-admin-binding\" configured secret \"cattle-credentials-c9e9c9a\" created clusterrole.rbac.authorization.k8s.io \"cattle-admin\" configured deployment.extensions \"cattle-cluster-agent\" created daemonset.extensions \"cattle-node-agent\" created 正确导入后的状态 执行完成后，就是pengding的状态，过一会儿后就是active的状态了,说明已经正确导入了. 进去到某个集群中 在这儿发现有个etcd的报错 去查询etcd的状态 [root@master ~]# etcdctl --endpoints=https://192.168.3.27:2379,https://192.168.3.28:2379,https://192.168.3.3:2379 \\ > --ca-file=/opt/kubernetes/ssl/ca.pem \\ > --cert-file=/opt/kubernetes/ssl/etcd.pem \\ > --key-file=/opt/kubernetes/ssl/etcd-key.pem cluster-health member 2df8ed452a15b52f is healthy: got healthy result from https://192.168.3.28:2379 failed to check the health of member c569b5ea1d5fd6b2 on https://192.168.3.3:2379: Get https://192.168.3.3:2379/health: dial tcp 192.168.3.3:2379: getsockopt: connection refused member c569b5ea1d5fd6b2 is unreachable: [https://192.168.3.3:2379] are all unreachable member d4a4a6a1a95915b1 is healthy: got healthy result from https://192.168.3.27:2379 cluster is degraded 发现192.168.3.3上的etcd的状态不对 去.3上发现etcd没有启动起来,重启etcd [root@node2 ~]# ps -ef | grep etcd root 18233 18163 0 11:48 pts/2 00:00:00 grep --color=auto etcd root 19228 1 0 Dec24 ? 00:00:33 /opt/kubernetes/bin/flanneld -etcd-endpoints=https://192.168.3.27:2379,https://192.168.3.28:2379,https://192.168.3.3:2379 -etcd-prefix=/coreos.com/network --etcd-cafile=/opt/kubernetes/ssl/ca.pem --etcd certfile=/opt/kubernetes/ssl/flanneld.pem --etcd-keyfile=/opt/kubernetes/ssl/flanneld-key.pem [root@node2 ~]# systemctl start etcd ,再次检查状态,就OK了. [root@master ~]# etcdctl --endpoints=https://192.168.3.27:2379,https://192.168.3.28:2379,https://192.168.3.3:2379 --ca-file=/opt/kubernetes/ssl/ca.pem --cert-file=/opt/kubernetes/ssl/e tcd.pem --key-file=/opt/kubernetes/ssl/etcd-key.pem cluster-healthmember 2df8ed452a15b52f is healthy: got healthy result from https://192.168.3.28:2379 member c569b5ea1d5fd6b2 is healthy: got healthy result from https://192.168.3.3:2379 member d4a4a6a1a95915b1 is healthy: got healthy result from https://192.168.3.27:2379 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"kubeedge/kubeedge介绍.html":{"url":"kubeedge/kubeedge介绍.html","title":"kubeedge","keywords":"","body":" kubeedge介绍 架构 云端组件 边缘端组件 Kubeedge的优势 丰富的应用程序 介绍 kubeedge介绍 KubeEdge 是华为开源用来打通云与边的一套解决方案。它可将k8s容器编排和管理扩展到边缘端设备。 它基于Kubernetes构建，为网络和应用程序提供核心基础架构支持，并在云端和边缘端部署应用，同步元数据。KubeEdge 还支持 MQTT 协议，允许开发人员编写客户逻辑，并在边缘端启用设备通信的资源约束。 架构 架构如图: kubeedge分为云端(cloud)和边缘端(edge) KubeEdge 由以下组件构成： 云端组件 CloudHub: CloudHub 是一个 Web Socket 服务端，负责监听云端的变化, 缓存并发送消息到 EdgeHub。 EdgeController: EdgeController 是一个扩展的 Kubernetes 控制器，管理边缘节点和 Pods 的元数据确保数据能够传递到指定的边缘节点。 边缘端组件 Edged: Edged 是运行在边缘节点的代理，用于管理容器化的应用程序。 EdgeHub: EdgeHub 是一个 Web Socket 客户端，负责与边缘计算的云服务（例如 KubeEdge 架构图中的 Edge Controller）交互，包括同步云端资源更新、报告边缘主机和设备状态变化到云端等功能。 MetaManager: MetaManager 是消息处理器，位于 Edged 和 Edgehub 之间，它负责向轻量级数据库（SQLite）存储/检索元数据。 EventBus: EventBus 是一个与 MQTT 服务器（mosquitto）交互的 MQTT 客户端，为其他组件提供订阅和发布功能。 DeviceTwin: DeviceTwin 负责存储设备状态并将设备状态同步到云，它还为应用程序提供查询接口。 Kubeedge的优势 打通边缘计算 通过在边缘端运行业务逻辑，可以在本地保护和处理大量数据。KubeEdge 减少了边和云之间的带宽请求，加快响应速度，并保护客户数据隐私。 简化开发 开发人员可以编写常规的基于 http 或 mqtt 的应用程序，容器化并在边缘或云端任何地方运行。 Kubernetes 原生支持k8s 使用 KubeEdge 用户可以在边缘节点上编排应用、管理设备并监控应用程序/设备状态，就如同在云端操作 Kubernetes 集群一样。 丰富的应用程序 用户可以轻松地将复杂的机器学习、图像识别、事件处理等高层应用程序部署到边缘端。 介绍 KubeEdge 由以下组件构成: Edged: Edged 是运行在边缘节点的代理，用于管理容器化的应用程序。 EdgeHub: EdgeHub 是一个 Web Socket 客户端，负责与边缘计算的云服务（例如 KubeEdge 架构图中的 Edge Controller）交互，包括同步云端资源更新、报告边缘主机和设备状态变化到云端等功能。 CloudHub: CloudHub 是一个 Web Socket 服务端，负责监听云端的变化, 缓存并发送消息到 EdgeHub。 EdgeController: EdgeController 是一个扩展的 Kubernetes 控制器，管理边缘节点和 Pods 的元数据确保数据能够传递到指定的边缘节点。 EventBus: EventBus 是一个与 MQTT 服务器（mosquitto）交互的 MQTT 客户端，为其他组件提供订阅和发布功能。 DeviceTwin: DeviceTwin 负责存储设备状态并将设备状态同步到云，它还为应用程序提供查询接口。 MetaManager: MetaManager 是消息处理器，位于 Edged 和 Edgehub 之间，它负责向轻量级数据库（SQLite）存储/检索元数据。 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"kubeedge/go环境配置.html":{"url":"kubeedge/go环境配置.html","title":"go环境配置","keywords":"","body":" golang环境配置 go的环境一个实例 git clone kubeedge的代码 golang环境配置 goroot /usr/local/go gopath mkdir -p /sure/goproject cd /sure/goproject mkdir src pkg bin 下载解压 cd /softdb wget https://dl.google.com/go/go1.12.10.linux-amd64.tar.gz tar -C /usr/local -zxvf go1.12.10.linux-amd64.tar.gz 编辑环境变量 vim /etc/profile export GOROOT=/usr/local/go export PATH=$PATH:$GOROOT/bin export GOPATH=/sure/goproject source /etc/profile go的环境一个实例 go_project // go_project为GOPATH目录 -- bin -- myApp1 // 编译生成 -- myApp2 // 编译生成 -- myApp3 // 编译生成 -- pkg -- src -- myApp1 // project1 -- models -- controllers -- others -- main.go -- myApp2 // project2 -- models -- controllers -- others -- main.go -- myApp3 // project3 -- models -- controllers -- others -- main.go git clone kubeedge的代码 git clone https://github.com/kubeedge/kubeedge.git $GOPATH/src/github.com/kubeedge/kubeedge 指定版本v0.3.0-beta.0 克隆 git clone --branch v0.3.0-beta.0 https://github.com/kubeedge/kubeedge.git $GOPATH/src/github.com/kubeedge/kubeedge 指定版本1.1 git clone --branch release-1.1 https://github.com/kubeedge/kubeedge.git $GOPATH/src/github.com/kubeedge/kubeedge Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-11 08:58:54 "},"kubeedge/配置kubeedge-0.2.html":{"url":"kubeedge/配置kubeedge-0.2.html","title":"配置kubeedge-V0.2","keywords":"","body":" 配置kubeedge 获取代码 代码层级 编译二进制文件 编译cloud和edge二进制文件 编译cloud二进制文件 编译edge二进制文件 部署云端(cloud) 部署边缘端(edge) 配置kubeedge kubeedge分为云端(cloud)和边缘端(edge) 获取代码 git clone https://github.com/kubeedge/kubeedge.git $GOPATH/src/github.com/kubeedge/kubeedge cd $GOPATH/src/github.com/kubeedge/kubeedge 代码层级 [root@k8s-master kubeedge]# tree -L 1 . ├── build ├── cloud ├── common ├── CONTRIBUTING.md ├── docs ├── edge ├── external-dependency.md ├── Gopkg.lock ├── Gopkg.toml ├── LICENSE ├── MAINTAINERS ├── Makefile ├── README.md ├── README_zh.md ├── tests └── vendor 主要看3个：cloud、edge、build 编译二进制文件 编译cloud和edge二进制文件 cd $GOPATH/src/github.com/kubeedge/kubeedge make 编译cloud二进制文件 cd $GOPATH/src/github.com/kubeedge/kubeedge make all WHAT=cloud 等同于: cd $GOPATH/src/github.com/kubeedge/kubeedge/cloud/edgecontroller make 编译edge二进制文件 cd $GOPATH/src/github.com/kubeedge/kubeedge make all WHAT=edge 等同于: cd $GOPATH/src/github.com/kubeedge/kubeedge/edge make 部署云端(cloud) 云端是k8s的扩展，使用了CRD对k8s进行扩展，本文将以二进制的方式进行云端的部署 因为cloud是k8s的扩展,所以需要与k8s的master进行通信，我们一般都做了双向tls，所以为了简便，可以先暴露非安全的IP和端口用来测试 vi /etc/kubernetes/manifests/kube-apiserver.yaml # Add the following flags in spec: containers: -command section - --insecure-port=8080 - --insecure-bind-address=0.0.0.0 当然，生产中，我们不建议这么用，我们还是老老实实的在control.yaml中配置kubeconfig的路径即可. 进入 [root@k8s-master cloud]# tree -L 3 . ├── edgecontroller │ ├── cmd │ │ └── edgecontroller.go │ ├── conf │ │ ├── controller.yaml │ │ ├── edgecontroller.tar │ │ ├── logging.yaml │ │ └── modules.yaml │ ├── edgecontroller │ ├── edgecontroller.bak │ ├── edgecontroller.log │ ├── Makefile │ └── pkg │ ├── cloudhub │ └── controller └── README.md 进入到$GOPATH/src/github.com/kubeedge/kubeedge/cloud/edgecontroller/conf 修改controller.yaml的相关内容 [root@k8s-master conf]# cat controller.yaml controller: kube: master: https://192.168.3.6:443 #配置了这个就不需要再使用非安全端口和非安全ip了 namespace: \"\" content_type: \"application/vnd.kubernetes.protobuf\" qps: 5 burst: 10 node_update_frequency: 10 kubeconfig: \"/root/.kube/config\" #这个地方就写kubeconfig的地址 cloudhub: address: 0.0.0.0 port: 10000 ca: /sure/cert/rootCA.crt # 可以通过脚本生成,只需要注意路径即可 cert: /sure/cert/edge.crt # 可以通过脚本生成,只需要注意路径即可 key: /sure/cert/edge.key # 可以通过脚本生成,只需要注意路径即可 keepalive-interval: 20 write-timeout: 20 node-limit: 10 部署边缘端(edge) 此方式将部署 cloud 端到 k8s 集群，所以需要登录到 k8s 的 master 节点上（或者其他可以用 kubectl 操作集群的机器）。 存放在 github.com/kubeedge/kubeedge/build/cloud 里的各个编排文件和脚本会被用到。所以需要先将这些文件放到可以用 kubectl 操作的地方。 首先， 确保 k8s 集群可以拉到 edge controller 镜像。如果没有， 可以构建一个，然后推到集群能拉到的 registry 上。 cd $GOPATH/src/github.com/kubeedge/kubeedge make cloudimage 这一步如果已经找到edgecontroller的镜像，就可以不用做了 然后，需要生成 tls 证书。这步成功的话，会生成 06-secret.yaml。 cd build/cloud ../tools/certgen.sh buildSecret | tee ./06-secret.yaml 接着，按照编排文件的文件名顺序创建各个 k8s 资源。在创建之前，应该检查每个编排文件内容，以确保符合特定的集群环境。 for resource in $(ls *.yaml); do kubectl create -f $resource; done 最后，基于08-service.yaml.example，创建一个适用于集群环境的 service， 将 cloud hub 暴露到集群外，让 edge core 能够连到。 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"install/kubeedge管理平台初始化环境安装步骤.html":{"url":"install/kubeedge管理平台初始化环境安装步骤.html","title":"kubeedge套装安装-v0.2","keywords":"","body":" 环境说明 节点规划 软件版本 一、执行master节点的安装 1.解压安装包 2.修改kubeedge.ini 3.修改或者检查 /etc/hosts 4.清理环境 5.执行安装master 6.安装结束的提示 二、node节点的安装 1.清理环境 2.配置环境 3.添加node到k8s集群中 4.如果还要添加其他节点,则需要再重复执行以上即可. 三、k8s集群检查及其他配置 1.节点状态检查 2.已经安装的pod如下: 3.修改apiserver的配置 4.修改kube-proxy的配置 5.添加用户admin/admin可以访问k8s的权限的配置 5.1添加basic-auth.csv到/etc/kubernetes/pki/下 5.2 在kube-apiserver中加入如下配置 5.3 创建clusterrole与user的绑定 5.4 java代码中要注意的地方: 四、部署其他组件 1.部署harbor 1.1进入目录将docker-composer复制到/usr/bin目录下 1.2解压harbor的离线安装包 1.3进入harbor目录,修改配置文件 1.4执行安装 1.5 配置harbor仓库 1.6docker的相关配置 1.7 和k8s的结合使用 2.部署ingress 2.1创建 2.2验证 3.部署nginx用于文件的下载 3.1创建 3.2验证 五、部署edgecontroller 1.进入相关目录 2.修改05-configmap.yaml 3.依次执行: 3.1 执行x-02-certgen.sh 3.2 执行x-03-create-06-secret.sh 3.3 执行x-04-doit.sh 4.检查edgecontroller pod的状态 六.边缘节点的配置(测试用) 1.在k8s master创建node 2.在边缘节点上拉取edge_code和conf和拉取密钥 3.修改 conf/edge.yaml 4.边缘节点后台运行edge_core 5.在master查看节点状态 七、注意事项 1.docker根目录的位置与大小 2.harbor的默认安装目录 3.kubectl 自动补全 环境说明 节点规划 节点 IP 角色 备注 k8s-master 192.168.3.94 master,harbor仓库 k8s-node1 192.168.3.124 node edge-node1 192.168.3.9 edgenode 软件版本 操作系统版本: cetos7.5 内核版本: 3.10.0-862.el7.x86_64 软件安装目录: /usr/local/src KubeEdge 版本: 0.2.1 kubernetes 版本: 1.14.1 一、执行master节点的安装 注意,本文档中 192.168.3.94 为 k8s master节点 192.168.3.124为 k8s node 节点 环境为centos7 1.解压安装包 tar -zxf k8s-deploy.tar.gz cd k8s-deploy/ chmod +x installk8s.sh 2.修改kubeedge.ini [root@host-192-168-3-94 k8s-deploy]# cat kubeedge.ini POD_NETWORK_CIDR=10.244.0.0/16 SERVICE_CIDR=10.2.0.0/16 APISERVER_ADVERTISE_ADDRESS=192.168.3.94 3.修改或者检查 /etc/hosts 4.清理环境 如果以前有安装过k8s的集群,需要执行卸载环境,保证环境是干净的 5.执行安装master 会提示/ect/hosts的检查和kubeedge.ini中apiserver的检查 确实安装后,就开始正式安装master节点了. 安装过程中,会做如下的操作: 关闭操作系统的防火墙 添加bridgesupport 关闭selinux 二进制方式安装docker 导入k8s集群需要的镜像 安装kubelet 安装kubernetes master节点 配置kube-config 安装网络插件flannel 6.安装结束的提示 很快就会安装结束,除非有问题.. 安装完成后,会有下面所示的提示: 在其他node节点,在完成初始化环境后,只需要执行以上的命令即可加入到k8s集群中去. 二、node节点的安装 1.清理环境 如果以前有安装过k8s的集群,需要执行卸载环境,保证环境是干净的 2.配置环境 3.添加node到k8s集群中 根据在master节点最后的提示,执行命令 kubeadm join 192.168.3.94:6443 --token ezgyei.st500hn6bmneez2a \\ --discovery-token-ca-cert-hash sha256:f9f23f026ece8d4a995528a24efcc95b28eaa25ca8b0c07b2e49dee91b32d6bc 安装完成后有如上提示: 4.如果还要添加其他节点,则需要再重复执行以上即可. 三、k8s集群检查及其他配置 1.节点状态检查 安装完master节点和node节点后,检查节点的状态 [root@host-192-168-3-94 k8s-deploy]# kubectl get cs NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-0 Healthy {\"health\":\"true\"} [root@host-192-168-3-94 k8s-deploy]# kubectl get node NAME STATUS ROLES AGE VERSION host-192-168-3-124 Ready 2m3s v1.14.1 host-192-168-3-94 Ready master 10m v1.14.1 2.已经安装的pod如下: [root@host-192-168-3-94 k8s-deploy]# kubectl get pod --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-fb8b8dccf-42lhz 1/1 Running 0 11m kube-system coredns-fb8b8dccf-lv97r 1/1 Running 0 11m kube-system etcd-host-192-168-3-94 1/1 Running 0 10m kube-system kube-apiserver-host-192-168-3-94 1/1 Running 0 10m kube-system kube-controller-manager-host-192-168-3-94 1/1 Running 0 10m kube-system kube-flannel-ds-amd64-6gcjj 1/1 Running 0 11m kube-system kube-flannel-ds-amd64-cw8b6 1/1 Running 0 2m42s kube-system kube-proxy-jxc8q 1/1 Running 0 2m42s kube-system kube-proxy-mnnv5 1/1 Running 0 11m kube-system kube-scheduler-host-192-168-3-94 1/1 Running 0 10m 3.修改apiserver的配置 cd /etc/kubernetes/manifests vim kube-apiserver.yaml 添加: - --service-node-port-range=8000-40000 - --insecure-port=8080 - --insecure-bind-address=0.0.0.0 修改完成保存后,会自动重启 4.修改kube-proxy的配置 给kube-proxy加上nodeselector,防止边缘节点上自动启动kube-proxy nodeSelector: kubernetes.io/arch: amd64 修改完成后查询 [root@host-192-168-3-94 manifests]# kubectl get daemonsets. -n kube-system NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE kube-flannel-ds-amd64 2 2 2 2 2 beta.kubernetes.io/arch=amd64 22m kube-flannel-ds-arm 0 0 0 0 0 beta.kubernetes.io/arch=arm 22m kube-flannel-ds-arm64 0 0 0 0 0 beta.kubernetes.io/arch=arm64 22m kube-flannel-ds-ppc64le 0 0 0 0 0 beta.kubernetes.io/arch=ppc64le 22m kube-flannel-ds-s390x 0 0 0 0 0 beta.kubernetes.io/arch=s390x 22m kube-proxy 2 2 2 2 2 22m [root@host-192-168-3-94 manifests]# kubectl edit daemonsets kube-proxy -n kube-system daemonset.extensions/kube-proxy edited [root@host-192-168-3-94 manifests]# [root@host-192-168-3-94 manifests]# kubectl get daemonsets. -n kube-system NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE kube-flannel-ds-amd64 2 2 2 2 2 beta.kubernetes.io/arch=amd64 25m kube-flannel-ds-arm 0 0 0 0 0 beta.kubernetes.io/arch=arm 25m kube-flannel-ds-arm64 0 0 0 0 0 beta.kubernetes.io/arch=arm64 25m kube-flannel-ds-ppc64le 0 0 0 0 0 beta.kubernetes.io/arch=ppc64le 25m kube-flannel-ds-s390x 0 0 0 0 0 beta.kubernetes.io/arch=s390x 25m kube-proxy 2 2 2 2 2 kubernetes.io/arch=amd64 25m 查看NODE SELECTOR这一列是否已经有了加上的nodeselector 5.添加用户admin/admin可以访问k8s的权限的配置 5.1添加basic-auth.csv到/etc/kubernetes/pki/下 [root@host-192-168-3-94 src]# cat /etc/kubernetes/pki/basic-auth.csv admin,admin,1 5.2 在kube-apiserver中加入如下配置 - --basic-auth-file=/etc/kubernetes/pki/basic-auth.csv 5.3 创建clusterrole与user的绑定 apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-crb roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - name: admin apiGroup: rbac.authorization.k8s.io 5.4 java代码中要注意的地方: 四、部署其他组件 1.部署harbor 1.1进入目录将docker-composer复制到/usr/bin目录下 # cd /usr/local/src # chmod a+x docker-compose-Linux-x86_64 # mv docker-compose-Linux-x86_64 /usr/local/bin/docker-compose # docker-compose -version docker-compose version 1.24.0, build 0aa59064 1.2解压harbor的离线安装包 [root@host-192-168-3-94 src]# tar -zxf harbor-offline-installer-v1.7.5.tgz [root@host-192-168-3-94 src]# 1.3进入harbor目录,修改配置文件 注意:最好把harbor对应的admin的默认密码修改了! [root@host-192-168-3-94 src]# cd harbor/ [root@host-192-168-3-94 harbor]# cp harbor.cfg harbor.cfg.bak [root@host-192-168-3-94 harbor]# vim harbor.cfg [root@host-192-168-3-94 harbor]# diff harbor.cfg harbor.cfg.bak 8c8 hostname = reg.mydomain.com [root@host-192-168-3-94 harbor]# 1.4执行安装 [root@host-192-168-3-94 harbor]# ./install.sh ... ... ... ✔ ----Harbor has been installed and started successfully.---- Now you should be able to visit the admin portal at http://edgehub.acedge.cn:8888. For more details, please visit https://github.com/goharbor/harbor . 1.5 配置harbor仓库 通过浏览器上登录harbor仓库,做如下操作: 创建用户 修改用户权限 创建项目 修改项目所有者 1.6docker的相关配置 修改/etc/docker/daemon.json添加 # cat /etc/docker/daemon.json { \"insecure-registries\" : [\"edgehub.acedge.cn:8888\",\"192.168.3.XXXXXXXXX:8888\"] } 1.7 和k8s的结合使用 docker登录harbor的地址生成相关信息 docker先登录harbor仓库后,会在/root/.docker/config.json自动生成登录的信息,类似: 将这个密码做base64转换 # cat /root/.docker/config.json | base64 -w 0 ewoJImF1dGhzIjogewoJCSIxOTIuMTY4LjMuNiI6IHsKCQkJImF1dGgiOiAiWVdSdGFXNDZVM1JoY2lveU1ERTAiCgkJfQoJfSwKCSJIdHRwSGVhZGVycyI6IHsKCQkiVXNlci1BZ2VudCI6ICJEb2NrZXItQ2xpZW50LzE4LjA2LjEtY2UgKGxpbnV4KSIKCX0KfQ== 生成secret apiVersion: v1 kind: Secret metadata: name: harborsecret data: .dockerconfigjson: ewoJImF1dGhzIjogewoJCSIxOTIuMTY4LjMuNiI6IHsKCQkJImF1dGgiOiAiWVdSdGFXNDZVM1JoY2lveU1ERTAiCgkJfQoJfSwKCSJIdHRwSGVhZGVycyI6IHsKCQkiVXNlci1BZ2VudCI6ICJEb2NrZXItQ2xpZW50Lz E4LjA2LjEtY2UgKGxpbnV4KSIKCX0KfQ== type: kubernetes.io/dockerconfigjson 在deployment或者pod中配置拉取镜像的的imagePullSecrets imagePullSecrets: - name: harborsecret 2.部署ingress 2.1创建 [root@host-192-168-3-94 src]# tar -zxf addons.tar.gz [root@host-192-168-3-94 src]# cd addons/ [root@host-192-168-3-94 addons]# pwd /usr/local/src/addons [root@host-192-168-3-94 addons]# ls dl edgecontroller harbor_soft ingress storageclass [root@host-192-168-3-94 addons]# kubectl create -f ingress/ daemonset.extensions/traefik-ingress-lb created serviceaccount/ingress created clusterrolebinding.rbac.authorization.k8s.io/ingress created service/traefik-web-ui created ingress.extensions/traefik-web-ui created [root@host-192-168-3-94 addons]# 2.2验证 [root@host-192-168-3-94 addons]# kubectl get ingresses -n kube-system NAME HOSTS ADDRESS PORTS AGE traefik-web-ui k8s-ingress-ui.com 在访问的主机hosts里面添加192.168.3.124 k8s-ingress-ui.com 通过浏览器访问 3.部署nginx用于文件的下载 3.1创建 [root@host-192-168-3-94 addons]# pwd /usr/local/src/addons [root@host-192-168-3-94 addons]# ls dl edgecontroller harbor_soft ingress storageclass [root@host-192-168-3-94 addons]# kubectl create -f dl ingress.extensions/dl-file-url created deployment.extensions/nginx-test created service/nginx-test created persistentvolume/dl-url created persistentvolumeclaim/dl-url-pvc created [root@host-192-168-3-94 addons]# 3.2验证 [root@host-192-168-3-94 addons]# kubectl get ingresses NAME HOSTS ADDRESS PORTS AGE dl-file-url static.acedge.cn 80 30s 在访问的主机hosts里面添加192.168.3.124 static.acedge.cn 在节点的 /data4dlurl 任意添加一个文件 [root@host-192-168-3-124 data4dlurl]# cd /data4dlurl/ [root@host-192-168-3-124 data4dlurl]# echo \"just 4 download! \" > hello.txt [root@host-192-168-3-124 data4dlurl]# 通过浏览器访问 五、部署edgecontroller 1.进入相关目录 [root@host-192-168-3-94 edgecontroller]# pwd /usr/local/src/addons/edgecontroller [root@host-192-168-3-94 edgecontroller]# ll 总用量 64 -rw-r--r-- 1 root root 58 5月 9 21:38 01-namespace.yml -rw-r--r-- 1 root root 91 5月 9 21:38 02-serviceaccount.yaml -rw-r--r-- 1 root root 381 5月 9 21:38 03-clusterrole.yaml -rw-r--r-- 1 root root 333 5月 9 21:38 04-clusterrolebinding.yaml -rw-r--r-- 1 root root 882 5月 9 21:38 05-configmap.yaml -rw-r--r-- 1 root root 906 5月 9 21:38 05-configmap.yaml.bak -rw-r--r-- 1 root root 2195 5月 9 21:38 07-deployment.yaml -rw-r--r-- 1 root root 297 5月 9 21:38 08-service.yaml -rw-r--r-- 1 root root 258 5月 9 21:38 08-service.yaml.example -rwxr-xr-x 1 root root 1597 5月 9 21:38 certgen.sh -rw-r--r-- 1 root root 1140 5月 9 21:38 README.md -rw-r--r-- 1 root root 1255 5月 9 21:38 test-nginx.yml -rwxr-xr-x 1 root root 32 5月 9 21:38 x-02-certgen.sh -rwxr-xr-x 1 root root 48 5月 9 21:38 x-03-create-06-secret.sh -rwxr-xr-x 1 root root 171 5月 9 21:38 x-04-doit.sh -rwxr-xr-x 1 root root 350 5月 9 21:38 x-05-clean.sh 2.修改05-configmap.yaml 修改05-configmap.yaml中的master对应的地址,其他不用改 3.依次执行: x-02-certgen.sh ：用于生产密钥.路径在/etc/kubeedge/ca和/etc/kubeedge/certs x-03-create-06-secret.sh ：用于生成edgecontroller的secret.yaml x-04-doit.sh ：用于创建kubeedge对应的各种资源 3.1 执行x-02-certgen.sh [root@host-192-168-3-94 edgecontroller]# sh x-02-certgen.sh Generating RSA private key, 2048 bit long modulus ..........................+++ .........................................................................................................................................+++ e is 65537 (0x10001) Signature ok subject=/C=CN/ST=Sichuan/L=Chengdu/O=KubeEdge/CN=kubeedge.io Getting CA Private Key [root@host-192-168-3-94 edgecontroller]# [root@host-192-168-3-94 edgecontroller]# ls -l /etc/kubeedge/ca -rw-r--r-- 1 root root 1976 5月 8 20:14 ca.crt -rw-r--r-- 1 root root 3311 5月 8 20:14 ca.key -rw-r--r-- 1 root root 17 5月 10 11:39 ca.srl [root@host-192-168-3-94 edgecontroller]# ls -l /etc/kubeedge/certs/ -rw-r--r-- 1 root root 1513 5月 10 11:39 edge.crt -rw-r--r-- 1 root root 985 5月 10 11:39 edge.csr -rw-r--r-- 1 root root 1675 5月 10 11:39 edge.key 3.2 执行x-03-create-06-secret.sh [root@host-192-168-3-94 edgecontroller]# sh x-03-create-06-secret.sh apiVersion: v1 kind: Secret metadata: name: edgecontroller namespace: kubeedge labels: k8s-app: kubeedge kubeedge: edgecontroller stringData: ca.crt: | -----BEGIN CERTIFICATE----- xxxxxxpapapa -----END CERTIFICATE----- cloud.crt: | -----BEGIN CERTIFICATE----- xxxxxxpapapa -----END CERTIFICATE----- cloud.key: | -----BEGIN RSA PRIVATE KEY----- xxxxxxpapapa -----END RSA PRIVATE KEY----- 执行完这个会生成一个文件 06-secret.yaml 3.3 执行x-04-doit.sh [root@host-192-168-3-94 edgecontroller]# sh x-04-doit.sh namespace/kubeedge created serviceaccount/edgecontroller created clusterrole.rbac.authorization.k8s.io/edgecontroller created clusterrolebinding.rbac.authorization.k8s.io/edgecontroller created configmap/edgecontroller created secret/edgecontroller created deployment.apps/edgecontroller created service/edgecontroller created 4.检查edgecontroller pod的状态 [root@host-192-168-3-94 edgecontroller]# kubectl get pod -n kubeedge NAME READY STATUS RESTARTS AGE edgecontroller-7c894ddf45-zrvg2 1/1 Running 0 85s [root@host-192-168-3-94 edgecontroller]# kubectl logs edgecontroller-7c894ddf45-zrvg2 -n kubeedge 没有出现网络的 i/o time out的错误就说明对了. 用telnet也可以检查 [root@host-192-168-3-94 src]# telnet 192.168.3.124 10000 Trying 192.168.3.124... Connected to 192.168.3.124. Escape character is '^]'. 六.边缘节点的配置(测试用) 在这以 192.168.3.9这个服务器作为edgenode作为示例 1.在k8s master创建node [root@host-192-168-3-94 src]# cat node.json { \"kind\": \"Node\", \"apiVersion\": \"v1\", \"metadata\": { \"name\": \"host-192-168-3-9\", \"labels\": { \"name\": \"edge-node\" } } } 2.在边缘节点上拉取edge_code和conf和拉取密钥 将master节点上通过sh x-02-certgen.sh生成的密钥拉取过来即可 [root@host-192-168-3-94 edgecontroller]# ls -l /etc/kubeedge/certs/ -rw-r--r-- 1 root root 1513 5月 10 11:39 edge.crt -rw-r--r-- 1 root root 985 5月 10 11:39 edge.csr -rw-r--r-- 1 root root 1675 5月 10 11:39 edge.key 最终准备的文件如下: [root@host-192-168-3-9 running]# pwd /root/running [root@host-192-168-3-9 running]# ll 总用量 98560 drwxr-xr-x 2 root root 84 5月 10 12:16 conf -rwxr-xr-x 1 root root 100910464 5月 10 11:56 edge_core -rw-r--r-- 1 root root 1513 5月 10 12:17 edge.crt -rw-r--r-- 1 root root 985 5月 10 12:17 edge.csr -rw-r--r-- 1 root root 1675 5月 10 12:17 edge.key 3.修改 conf/edge.yaml 需要修改的地方已经用框标识 示例: [root@host-192-168-3-9 conf]# diff edge.yaml edge.yaml.bak 11,13c11,13 url: wss://192.168.3.140:10000/e632aba927ea4ac2b575ec1603d56f10/host-192-168-3-107/events > certfile: /etc/kubeedge/edge/certs/edge.crt > keyfile: /etc/kubeedge/edge/certs/edge.key 24c24 node-id: host-192-168-3-107 28c28 hostname-override: host-192-168-3-107 4.边缘节点后台运行edge_core nohup ./edge_core & 注意,在生产环境建议修改log的等级,或者重定向日志的输出到 /dev/null 5.在master查看节点状态 七、注意事项 1.docker根目录的位置与大小 默认的目录为/var/lib/docker,这个路径在根目录上,可以考虑先创建一个lvm来挂载这个目录,方便今后的扩容 systemctl stop docker cd /var/lib cp -rf docker docker.bak cp -rf docker /xxx/ rm -rf docker ln -s /xxx/docker docker systemctl start docker docker info 2.harbor的默认安装目录 harbor的默认安装目录为/data 可以考虑先创建一个lvm来挂载这个目录,方便今后的扩容 3.kubectl 自动补全 yum install -y bash-completion source /usr/share/bash-completion/bash_completion source Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"kubeedge/mqtt-and-coap.html":{"url":"kubeedge/mqtt-and-coap.html","title":"MQTT-Vs-CoAP","keywords":"","body":"基础通信协议 基础通信协议方面就有两个专门的竞争协议：消息队列遥测传输( MQTT )，和受约束的应用协议(CoAP)。 它们都设计为轻量级，并仔细使用稀缺的网路资源。两者都在正确的环境中使用，但问题是，由于物联网的快速发展，一般人们不知道这些协议是什么？或何时使用？ 什么是MQTT MQTT的全名为Message Queuing Telemetry Transport，为IBM和Eurotech共同制定出来的protocol，在MQTT的官网可以看到一开始它对MQTT的介绍： http://mqtt.org/ MQTT is a machine-to-machine (M2M)/\"Internet of Things\" connectivity protocol. It was designed as an extremely lightweight publish/subscribe messaging transport. 简单来说，它是为了物联网而设计的protocol，并且它是透过publish/subscribe的方式来做讯息传送。由于是为了物联网而设计的协定，因此它所需要的网路频宽是很低的，而所需要的硬体资源也是低的。 MQTT很像Twitter。这是一个「发布和订阅」协议。 Publish/Subscribe： 在看MQTT之前，最好要先知道Publish/Subscribe的讯息传送机制为何，这样之后在看其协定时，才会更快上手。 Publish/Subscribe有三种主要的组成元件，分别为Publisher、Subscriber以及Topic。 Publisher为讯息的来源，它会将讯息发送给Topic，而Subscriber向Topic注册，表示他们想要接收此Topic的讯息；因此当有某个Publisher对Topic发送讯息时，只要是有对此Topic注册的Subscriber，都会收到此则讯息。 它们的关系如下图： MQTT特性 了解了Publish/Subscribe的机制之后，接着看看MQTT有哪些特性： Publish/Subscribe的讯息传送模式，来提供一对多的讯息分配。 使用TCP/IP来提供基本的网路连结。 三种讯息传送服务的qualities： \"At most once\"，最多一次，讯息遗失或是重复发送的状况可能会发生；这种quality适合应用在环境感测，不在意资料是否会遗失，因为下一次的资料取样很快就会被published出来。 \"At least once\"，至少一次，这种quality保证讯息会送达，只是可能会发生重复发送讯息的状况。 \"Exactly once\"，确定一次，确认讯息只会送到一次。这种quality适合用在计费系统，系统只要有重复收到资料、或是资料遗失状况发生，就会造成系统错误。 由于他的header固定长度为2byte，因此可以减少封包传送时的额外负载，并减少所需的网路频宽。 当异常断线发生时，会使用最后遗嘱(Last Will and Testament)的机制，通知各个感兴趣的client。 MQTT现况： MQTT现阶段，并不是一个标准化的Protocol，还在持续改进中，目前为MQTT V3.1。不过IBM已于2013年，已经将它交给OASIS进行标准化了，并且一直以来IBM对此协定采开放、免授权费的方式，让它能够被散布，因此相信不久的将来，会成为一个主流的Protocol。 而目前支援MQTT的Client API，有Eclipse Phno Project有对MQTT client支援，其支援C、Java、Javascript、C++等等的语言，可说是支援度很高的Project。而目前已经在应用MQTT的，最知名的应该就是Facebook Message App了吧，可以参考此篇文章。 小结： 上面提到的，低频宽、低硬体需求的特性，讯息传递为Publish/Subscribe的方式，正好可以用来实现Push Notification的机制，并且能达到手持装置省电的需求，接下来会先从其Protocol开始了解，并用Client Api跑些范例来应用此Protocol。 什么是CoAP CoAP(The Constrained Application Protocol) 目前已是IETF标准(RFC 7252) ，提出一个类似HTTP/TCP设计，但是属于轻量版的HTTP/UDP，使得其有利于感测节点进行网路传输。 CoAP主要特点： CoAP是主从(Client/Server)架构，感测节点多半为CoAP Server提供资源，由CoAP Client请求读取/控制资源状态。CoAP使用UDP (port: 5683)，对于资料是否要重传或传送顺序(Reordering)全交由上层应用层来决定，对于资源有限的MCU则不需要有完整TCP/IP协定实作 而CoAP同HTTP一样具有REST(Representational State Transfer)设计风格，也支援GET/PUT/POST/DELETE及URIs的请求方式。 CoAP采用二进位整数格式且封包标头4个byte而非HTTP使用字串格式(ASCII code)，所以封包传送时的额外负担小且不必像HTTP一样得进行耗时的字串解析处理。 CoAP QoS : CoAP讯息分为Confirmable或Non-Confirmable。Confirmable要求接收端须回送ACK，若没有收到ACK则重送一次。若送的是Non-Confirmable讯息，则送出端不在乎接收端是否收到。 CoAP加密使用DTLS (Datagram Transport Layer Security) 通知机制: CoAP扩展了HTTP GET，加入了一个observe flag，使得CoAP Server能主动回传，CoAP Client所observe的资源状态。 NAT Issue:若感测节点在NAT后方，则必须一开始先送出请求到外部，使路由器可以接受来自外面CoAP Client的请求，例如请求资源清单。 DTLS加密过程使用预设密钥PSK或椭圆曲线Diffie-Hellman ECDH算法，这种方法同样也是DLMS中使用的加密方法。存在的问题是，在使用ECDH时会受到DDoS的攻击，使用大的证书使得服务端瘫痪 CoAP vs MQTT 比较 MQTT MQTT的特点是可以保持长连接，具有一定的实时性，云端向设备端发送消息，设备端可以在最短的时间内接收到并作出响应，所以MQTT更适合需要实时控制的场合，更适合执行器。要保持长连接，那么就要时不时地发送心跳包，这就不会省电了。所以低功耗的场合并不适合MQTT。MQTT的长连接需要建立在TCP的基础上，TCP协议的复杂性决定了对设备的要求是比较高一些的，相比UDP。 CoAP CoAP的特点是低功耗，数据发完就可以休眠了。所以CoAP更适合数据采集的场合，更适合纯粹的传感器设备，特别是电池供电的传感器设备。基于UDP协议，对设备的要求比较简单。华为出的NB-IoT芯片就只支持UDP和CoAP，华为的决策告诉我们CoAP和NB-IoT是一对。 总结 都是公开标准且都是基于IP层的协定 封包标头小且采用binary格式 CoAP属于一对一通讯，MQTT则是多对多 若考虑感测节点在NAT后方的情况，由于MQTT的架构因为有中央broker的角色，MQTT Client本来就持续连接在broker，所以可以直接推播讯息，没有NAT问题。 MQTT是多个客户端通过一个中央代理传递消息的多对多协议。它通过让客户端发布消息、代理决定消息路由和复制来解耦生产者和消费者。虽然MQTT持久性有一些支持，但它是最好的实时通讯总线。 CoAP基本上是一个在Client和Server之间传递状态信息的单对单协议。虽然它支持观察资源，但是CoAP最适合状态转移模型，而不是单纯的基于事件。 MQTT Clients与Broker之间保持TCP长连接，这个在NAT环境中也不会有问题。CoAP Clients与Server都要接收和发送UDP包。在NAT环境下使用CoAP，需要使用“隧道掘进”或者端口转发(内网穿透)，否则像LWM2M（轻量级M2M）一样，首先初始化设备到‘头端’( head-end )的连接. MQTT不支持带有类型或者其它帮助Clients理解的标签消息。MQTT消息可用于任意目的，但前提是所有的Clients必须知道消息格式。而CoAP则相反，它内置内容协商和发现支持，这样允许设备彼此窥测以找到交换数据的方式。 协议 核心特点 下层协议 应用场合 硬件要求 MQTT 长连接 TCP 实时控制/执行器 较高 CoAP 低功耗 UDP 数据采集/传感器 较低 来自网上的一个对比： 物联网应用层通讯协定标准比较 机器对机器(Machine-to-Machine， M2M)通讯是物联网的一个重要运作概念。随着物联网的应用日益兴盛，M2M流量会持续增加，故针对M2M Traffic特征及其应用，M2M通讯技术应运而生。 由于物联网架构下，感测节点本身多半采用MCU，且以电池供电，故这些新的M2M协定必须考量，在有限的硬件能力及功耗等条件下，使得M2M Traffic在进行网路传输时，有较高的Throughput、低延迟、低电力耗损，甚至提供不同的QoS (Quality of Service)。 目前各家提供连结物联网装置的云端资料服务平台 AWS IoT、Evrythng、Xively、ThingSpeak、ThingWorx等及晶片厂提供的云平台，如联发科的MCS、ARM mbed Device Connector等，都广泛支援CoAP及MQTT协定，故将选择此两种协定来进行说明与比较。 通信协议 restful 传输层 publish/subscribe request/Response 加密 qos 封包标头 是否开源 CoAP √ UDP √ √ DTLS √ 4 libcoap MQTT × TCP √ × SSL √ 2 paho/mosquitto/mqtt.js/emq HTTP √ TCP × √ SSL × ~ 然而CoAP Client要取得位于NAT后方的感测节点资料，则须要在路由器上，设上设定virtual server，或port forwarding之类才能使用，不然就必须另外有第三方伺服器存在，让感测节点先连出才行。 需要注意的是，CoAP的网络层只有IPv6，所以如果没有物联网网关Gateway，一个IPv4的云端是无法直接访问CoAP终端的，只能由终端发起去访问固定的云端服务器。在大量的物联网终端情况下，保持连接或者会话是不现实的。因此物联网Gateway就非常重要了，需要服务器通过Gateway集成专有的寻址应用，反向建立与在NAT后的终端连接。 对比 什么时候使用它们？ 可能都在问的问题是，「如果他们很相似，我应该在什么时候使用哪一个；又在什么时候，使用哪一个？」 由于发布/订阅体系结构与中间商中介，MQTT是广域网（WAN，互联网）上的设备之间的通信的理想选择。它在带宽有限的情况下是最有用的，例如远程现场站点或其他缺乏强大网络的区域。 MQTT是Azure和Amazon服务产品的一部分，因此它具有很多已建立的架构，使其易于适应当前的开发人员。 CoAP的强项是与HTTP的兼容性。如果您有一个基于Web服务的现有系统，那么在CoAP中添加是一个很好的选择。它建立在用户数据报协议（UDP）上，这在一些资源有限的环境中是有用的。由于UDP允许广播和多播，您可以使用较少的带宽潜在地传输到多个主机。这使得它对于设备需要快速交流的本地网络环境很好，这对于一些M2M设置是传统的。 如果物联网开发人员正在使用将利用现有Web服务器架构的设备，开发人员将使用CoAP。但是，如果开发者正在构建一个设备真正“仅报告”的东西 - 也就是说，它被丢弃在网络上，只需要将数据报告回服务器 - CoAP将会更好。其他用途，如云架构，可能最好用MQTT完成。 MQTT和CoAP的未来 随着时间的推移，对于其他协议，使用或行业采用趋向于向更自由和包容的平台迁移，除非非包容性平台更好。 MQTT和CoAP都是开放标准，任何人都可以实现。 CoAP由标准机构启动，而不是由私有公司（包括IBM）设计的MQTT。 CoAP被设计为处理资源有限的环境，可能是它成为赢家，但是目前MQTT似乎处于领先地位。 MQTT背后有显着的动力 - 大云玩家已经选择了这一势头，或者至少选择它。此外，许多商业用例需要MQTT（存储和转发，集中式主机）的功能。然而，一种可能性是，一些围绕HTTP（例如移动应用程序开发）进行标准化的软件开发可以开始利用CoAP来处理外围设备，并与后端通信，以帮助减少不良连接带宽。 最终，这些协议可以通过互联网有效部署在不同的应用程序中。我们知道有特定的使用案例，其中每个都是最好的，但是我们也知道，物联网和物联网设备将会在复杂性和普及性方面继续发展。对于开发人员来说，了解应用程序的关键差异不仅可以实现更好的初始部署，而且可以为今后的开发工作奠定坚实的基础。 参考： https://itknowledgeexchange.techtarget.com/iot-agenda/iot-developers-confused-mqtt-coap/ https://blog.csdn.net/iotisan/article/category/7184968 TCP/IP协议模型： Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-22 17:31:35 "},"kubeedge/详解MQTT.html":{"url":"kubeedge/详解MQTT.html","title":"详解MQTT","keywords":"","body":" MQTT简介 MQTT(Message Queuing Telemetry Transport)是基于二进制消息的发布/订阅编程模式的消息协议。MQTT的出现是因为传统的http不适合物联网的场景，所以用的是发布/订阅（Publish/Subscribe）模式取代请求/回答（Request/Response）模式 遵循的设计原则 精简，不添加可有可无的功能。 发布/订阅（Pub/Sub）模式，方便消息在传感器之间传递。 允许用户动态创建主题，零运维成本。 把传输量降到最低以提高传输效率。 把低带宽、高延迟、不稳定的网络等因素考虑在内。 支持连续的会话控制。 理解客户端计算能力可能很低。 提供服务质量管理。 假设数据不可知，不强求传输数据的类型与格式，保持灵活性。 发布/订阅模式 与请求/回答这种同步模式不同，发布/定义模式解耦了发布消息的客户（发布者）与订阅消息的客户（订阅者）之间的关系，这意味着发布者和订阅者之间并不需要直接建立联系。打个比方，http是打电话给客户(需要响应才能通话)，而mqtt是发邮件给客户(异步，不需要响应)。 这种模式带来了这些好处： 发布者与订阅者不比了解彼此，只要认识同一个消息代理即可。 发布者和订阅者不需要交互，发布者无需等待订阅者确认而导致锁定。 发布者和订阅者不需要同时在线，可以自由选择时间来消费消息。 主题 MQTT是通过主题对消息进行分类的，本质上就是一个UTF-8的字符串，不过可以通过反斜杠表示多个层级关系。主题并不需要创建，直接使用就是了。 主题还可以通过通配符进行过滤。其中，+可以过滤一个层级，而*只能出现在主题最后表示过滤任意级别的层级。 举个例子： building-b/floor-5：代表B楼5层的设备。 +/floor-5：代表任何一个楼的5层的设备。 building-b/*：代表B楼所有的设备。 注意，MQTT允许使用通配符订阅主题，但是并不允许使用通配符广播。 服务质量 为了满足不同的场景，MQTT支持三种不同级别的服务质量（Quality of Service，QoS）为不同场景提供消息可靠性： 级别0：尽力而为。消息发送者会想尽办法发送消息，但是遇到意外并不会重试。 级别1：至少一次。消息接收者如果没有知会或者知会本身丢失，消息发送者会再次发送以保证消息接收者至少会收到一次，当然可能造成重复消息。 级别2：恰好一次。保证这种语义肯待会减少并发或者增加延时，不过丢失或者重复消息是不可接受的时候，级别2是最合适的。 级别2所提供的不重不丢很多情况下是最理想的，不过往返多次的确认一定对并发和延迟带来影响。 级别1提供的至少一次语义在日志处理这种场景下是完全OK的，所以像Kafka这类的系统利用这一特点减少确认从而大大提高了并发。 级别0适合鸡肋数据场景 消息类型 MQTT拥有14种不同的消息类型： CONNECT：客户端连接到MQTT代理 CONNACK：连接确认 PUBLISH：新发布消息 PUBACK：新发布消息确认，是QoS 1给PUBLISH消息的回复 PUBREC：QoS 2消息流的第一部分，表示消息发布已记录 PUBREL：QoS 2消息流的第二部分，表示消息发布已释放 PUBCOMP：QoS 2消息流的第三部分，表示消息发布完成 SUBSCRIBE：客户端订阅某个主题 SUBACK：对于SUBSCRIBE消息的确认 UNSUBSCRIBE：客户端终止订阅的消息 UNSUBACK：对于UNSUBSCRIBE消息的确认 PINGREQ：心跳 PINGRESP：确认心跳 DISCONNECT：客户端终止连接前优雅地通知MQTT代理 MQTT的实现 推荐参考两种mqtt协议的实现： mosquitto EMQ 参考 MQTT Version 5.0 Introduction to MQTT Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-22 17:15:31 "},"tips/k8s-tips.html":{"url":"tips/k8s-tips.html","title":"k8s-使用技巧","keywords":"","body":" 命令补全 命令补全 yum install -y bash-completion source /usr/share/bash-completion/bash_completion source # Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"tips/k8s上下文介绍.html":{"url":"tips/k8s上下文介绍.html","title":"k8s上下文介绍","keywords":"","body":" 1.生成新的kubeconfig文件 2.创建集群入口 3.指定集群证书文件 4.创建环境配置文件context 5.将kubeconfig文件 copy到 /root/.kube/config 6.激活上下文 在有证书的机器上 1.生成新的kubeconfig文件 kubectl config set-credentials devuser \\ --client-certificate=/opt/kubernetes/ssl/devuser.pem \\ --client-key=/opt/kubernetes/ssl/devuser-key.pem \\ --embed-certs=true \\ --kubeconfig=kubeconfig 特别注意 embed-certs选项，如果为false，代表里面的秘钥会以路径的方式生成，这个对于非本地的config是不可取的 2.创建集群入口 kubectl config set-cluster k8s --server=https://172.18.53.221:6443 --kubeconfig=kubeconfig 3.指定集群证书文件 kubectl config set-cluster k8s --certificate-authority=/opt/kubernetes/ssl/ca.pem --embed-certs=true --kubeconfig=kubeconfig kubernetes.pem 是集群的证书，有些地方应该是ca 要参考apiserver的配置文件的client-ca-file 一项 [Service] ExecStart=/opt/kubernetes/bin/kube-apiserver \\ --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction \\ --bind-address=172.18.53.221 \\ --insecure-bind-address=0.0.0.0 \\ --insecure-port=8080 \\ --authorization-mode=Node,RBAC \\ --runtime-config=rbac.authorization.k8s.io/v1 \\ --kubelet-https=true \\ --anonymous-auth=false \\ --basic-auth-file=/opt/kubernetes/ssl/basic-auth.csv \\ --enable-bootstrap-token-auth \\ --token-auth-file=/opt/kubernetes/ssl/bootstrap-token.csv \\ --service-cluster-ip-range=10.1.0.0/16 \\ --service-node-port-range=20000-40000 \\ --tls-cert-file=/opt/kubernetes/ssl/kubernetes.pem \\ --tls-private-key-file=/opt/kubernetes/ssl/kubernetes-key.pem \\ --client-ca-file=/opt/kubernetes/ssl/ca.pem \\ --service-account-key-file=/opt/kubernetes/ssl/ca-key.pem \\ --etcd-cafile=/opt/kubernetes/ssl/ca.pem \\ --etcd-certfile=/opt/kubernetes/ssl/kubernetes.pem \\ --etcd-keyfile=/opt/kubernetes/ssl/kubernetes-key.pem \\ --etcd-servers=https://172.18.53.221:2379,https://172.18.53.223:2379,https://172.18.53.224:2379 \\ --enable-swagger-ui=true \\ --allow-privileged=true \\ --audit-log-maxage=30 \\ --audit-log-maxbackup=3 \\ --audit-log-maxsize=100 \\ --audit-log-path=/opt/kubernetes/log/api-audit.log \\ --event-ttl=1h \\ --v=2 \\ --logtostderr=false \\ --log-dir=/opt/kubernetes/log \\ --enable-swagger-ui=true 各种ca，tls，请参考：https://blog.csdn.net/qq_34463875/article/details/78042852 4.创建环境配置文件context kubectl config set-context k8s \\ --cluster=k8s \\ --user=devuser \\ --namespace=kube-system \\ --kubeconfig=kubeconfig 注意： 可以加上namespace作为默认的namespace是选项 5.将kubeconfig文件 copy到 /root/.kube/config mv kubeconfig /root/.kube/config 6.激活上下文 kubectl config use-context k8s 切换之后看到的新的上下文应该为如下： [root@nazeebodan ~]# kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority-data: REDACTED server: https://172.18.53.221:6443 name: k8s contexts: - context: cluster: k8s namespace: kube-system user: devuser name: k8s current-context: k8s kind: Config preferences: {} users: - name: devuser user: client-certificate-data: REDACTED client-key-data: REDACTED 7、检查kubectl get pod ======================================== 登录dashboard的时候，会有2个选项 3.1获取sa的名字 kubectl get secret -n kube-system|grep admin-user 3.2 获取密钥 kubectl describe secret admin-user-token-ml8d2 -n kube-system 将获取到的密码注入到config文件 kubectl config set-credentials admin --token=eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLTZoZnhoIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJhOGJiNmIwNC05MjBkLTExZTgtODc2MS0wMDE2M2UwNDYzYTciLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.EZF855UNZfr7k2RQtZ5p2f-1bkn9C-EEBdk7Oh0ss08wQnxE4Y2FqZDfzU9YkT87XDYMxh_OKLWqSMPzw6ssGlQNFOQL8h6Y3IfkE4Q2gExzvQ8O9Ilqgj6o-9oRpE7R_GDY8x0OCSh6IwZWE7WSfaelpzVTLKfZpSafQJGU9FcdW-7pxD1dwo1bognoSlg6wVlLcaH3Tk0JahANsZNFnxp5dgTla1ijjgA9x5VDDo59YBWRZawArcoI3ha1DNxSfjb9ylB1VkJWbKT7x7DwG0KAhqhlb6lMvBFxJyK-PPihtPm69oCvQuctiMjAlRu5En0OCwsxbp5Xum8Q4ZPuEQ --kubeconfig=config.bak 将集群入口信息写入到config kubectl config set-cluster kubernetes --server=https://172.18.53.221:6443 --kubeconfig=config.bak 将认证信息写入到config kubectl config set-cluster kubernetes --certificate-authority=/opt/kubernetes/ssl/ca.pem --embed-certs=true -- kubeconfig=config.bak 创建环境配置文件 kubectl config set-context kubernetes --cluster=kubernetes --user=admin --kubeconfig=config.bak 激活上下文 kubectl config use-context kubernetes --kubeconfig=config.bak 题外话： 1、创建serviceaccount 2、将角色绑定到serviceaccout apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kube-system 理一下user和sa的关系 1. 通过ca创建user-->生成证书-->创建角色(资源+操作)-->创建绑定(绑定里面就有目标用户了)-->绑定完成后就有相应的权限了 给user生成新的kubeconfig文件 创建集群入口 指定集群证书文件 创建环境配置文件(指定user和namespace) 激活上下文 2.如果想让user访问某pod 如果要让user访问某pod，那么pod有相应的sa，如果有特殊需求，那么先去建立，如下： 创建secret-->创建sa-->绑定pod ==>要访问这个pod说明就需要有相应的sa对应的token 接着 将获取到的密码注入到config文件，步骤： 给user生成新的kubeconfig文件 创建集群入口 指定集群证书文件 创建环境配置文件(指定user和namespace) 激活上下文，拿新生产的kubeconfig就可以用了。 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"tips/kubectl-tips.html":{"url":"tips/kubectl-tips.html","title":"kubectl使用技巧","keywords":"","body":" kubectl 命令技巧大全 Kubectl 自动补全 Kubectl 上下文和配置 创建对象 显示和查找资源 更新资源 修补资源 编辑资源 Scale 资源 删除资源 与运行中的 Pod 交互 与节点和集群交互 资源类型 格式化输出 Kubectl 详细输出和调试 参考 kubectl 命令技巧大全 Kubctl 命令是操作 kubernetes 集群的最直接和最 skillful 的途径，这个60多MB大小的二进制文件，到底有啥能耐呢？请看下文： Kubectl 自动补全 $ source Kubectl 上下文和配置 设置 kubectl 命令交互的 kubernetes 集群并修改配置信息。参阅 使用 kubeconfig 文件进行跨集群验证 获取关于配置文件的详细信息。 $ kubectl config view # 显示合并后的 kubeconfig 配置 # 同时使用多个 kubeconfig 文件并查看合并后的配置 $ KUBECONFIG=~/.kube/config:~/.kube/kubconfig2 kubectl config view # 获取 e2e 用户的密码 $ kubectl config view -o jsonpath='{.users[?(@.name == \"e2e\")].user.password}' $ kubectl config current-context # 显示当前的上下文 $ kubectl config use-context my-cluster-name # 设置默认上下文为 my-cluster-name # 向 kubeconf 中增加支持基本认证的新集群 $ kubectl config set-credentials kubeuser/foo.kubernetes.com --username=kubeuser --password=kubepassword # 使用指定的用户名和 namespace 设置上下文 $ kubectl config set-context gce --user=cluster-admin --namespace=foo \\ && kubectl config use-context gce 创建对象 Kubernetes 的清单文件可以使用 json 或 yaml 格式定义。可以以 .yaml、.yml、或者 .json 为扩展名。 $ kubectl create -f ./my-manifest.yaml # 创建资源 $ kubectl create -f ./my1.yaml -f ./my2.yaml # 使用多个文件创建资源 $ kubectl create -f ./dir # 使用目录下的所有清单文件来创建资源 $ kubectl create -f https://git.io/vPieo # 使用 url 来创建资源 $ kubectl run nginx --image=nginx # 启动一个 nginx 实例 $ kubectl explain pods,svc # 获取 pod 和 svc 的文档 # 从 stdin 输入中创建多个 YAML 对象 $ cat 显示和查找资源 # Get commands with basic output $ kubectl get services # 列出所有 namespace 中的所有 service $ kubectl get pods --all-namespaces # 列出所有 namespace 中的所有 pod $ kubectl get pods -o wide # 列出所有 pod 并显示详细信息 $ kubectl get deployment my-dep # 列出指定 deployment $ kubectl get pods --include-uninitialized # 列出该 namespace 中的所有 pod 包括未初始化的 # 使用详细输出来描述命令 $ kubectl describe nodes my-node $ kubectl describe pods my-pod $ kubectl get services --sort-by=.metadata.name # List Services Sorted by Name # 根据重启次数排序列出 pod $ kubectl get pods --sort-by='.status.containerStatuses[0].restartCount' # 获取所有具有 app=cassandra 的 pod 中的 version 标签 $ kubectl get pods --selector=app=cassandra rc -o \\ jsonpath='{.items[*].metadata.labels.version}' # 获取所有节点的 ExternalIP $ kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type==\"ExternalIP\")].address}' # 列出属于某个 PC 的 Pod 的名字 # “jq”命令用于转换复杂的 jsonpath，参考 https://stedolan.github.io/jq/ $ sel=${$(kubectl get rc my-rc --output=json | jq -j '.spec.selector | to_entries | .[] | \"\\(.key)=\\(.value),\"')%?} $ echo $(kubectl get pods --selector=$sel --output=jsonpath={.items..metadata.name}) # 查看哪些节点已就绪 $ JSONPATH='{range .items[*]}{@.metadata.name}:{range @.status.conditions[*]}{@.type}={@.status};{end}{end}' \\ && kubectl get nodes -o jsonpath=\"$JSONPATH\" | grep \"Ready=True\" # 列出当前 Pod 中使用的 Secret $ kubectl get pods -o json | jq '.items[].spec.containers[].env[]?.valueFrom.secretKeyRef.name' | grep -v null | sort | uniq 更新资源 $ kubectl rolling-update frontend-v1 -f frontend-v2.json # 滚动更新 pod frontend-v1 $ kubectl rolling-update frontend-v1 frontend-v2 --image=image:v2 # 更新资源名称并更新镜像 $ kubectl rolling-update frontend --image=image:v2 # 更新 frontend pod 中的镜像 $ kubectl rolling-update frontend-v1 frontend-v2 --rollback # 退出已存在的进行中的滚动更新 $ cat pod.json | kubectl replace -f - # 基于 stdin 输入的 JSON 替换 pod # 强制替换，删除后重新创建资源。会导致服务中断。 $ kubectl replace --force -f ./pod.json # 为 nginx RC 创建服务，启用本地 80 端口连接到容器上的 8000 端口 $ kubectl expose rc nginx --port=80 --target-port=8000 # 更新单容器 pod 的镜像版本（tag）到 v4 $ kubectl get pod mypod -o yaml | sed 's/\\(image: myimage\\):.*$/\\1:v4/' | kubectl replace -f - $ kubectl label pods my-pod new-label=awesome # 添加标签 $ kubectl annotate pods my-pod icon-url=http://goo.gl/XXBTWq # 添加注解 $ kubectl autoscale deployment foo --min=2 --max=10 # 自动扩展 deployment “foo” 修补资源 使用策略合并补丁并修补资源。 $ kubectl patch node k8s-node-1 -p '{\"spec\":{\"unschedulable\":true}}' # 部分更新节点 # 更新容器镜像； spec.containers[*].name 是必须的，因为这是合并的关键字 $ kubectl patch pod valid-pod -p '{\"spec\":{\"containers\":[{\"name\":\"kubernetes-serve-hostname\",\"image\":\"new image\"}]}}' # 使用具有位置数组的 json 补丁更新容器镜像 $ kubectl patch pod valid-pod --type='json' -p='[{\"op\": \"replace\", \"path\": \"/spec/containers/0/image\", \"value\":\"new image\"}]' # 使用具有位置数组的 json 补丁禁用 deployment 的 livenessProbe $ kubectl patch deployment valid-deployment --type json -p='[{\"op\": \"remove\", \"path\": \"/spec/template/spec/containers/0/livenessProbe\"}]' 编辑资源 在编辑器中编辑任何 API 资源。 $ kubectl edit svc/docker-registry # 编辑名为 docker-registry 的 service $ KUBE_EDITOR=\"nano\" kubectl edit svc/docker-registry # 使用其它编辑器 Scale 资源 $ kubectl scale --replicas=3 rs/foo # Scale a replicaset named 'foo' to 3 $ kubectl scale --replicas=3 -f foo.yaml # Scale a resource specified in \"foo.yaml\" to 3 $ kubectl scale --current-replicas=2 --replicas=3 deployment/mysql # If the deployment named mysql's current size is 2, scale mysql to 3 $ kubectl scale --replicas=5 rc/foo rc/bar rc/baz # Scale multiple replication controllers 删除资源 $ kubectl delete -f ./pod.json # 删除 pod.json 文件中定义的类型和名称的 pod $ kubectl delete pod,service baz foo # 删除名为“baz”的 pod 和名为“foo”的 service $ kubectl delete pods,services -l name=myLabel # 删除具有 name=myLabel 标签的 pod 和 serivce $ kubectl delete pods,services -l name=myLabel --include-uninitialized # 删除具有 name=myLabel 标签的 pod 和 service，包括尚未初始化的 $ kubectl -n my-ns delete po,svc --all # 删除 my-ns namespace 下的所有 pod 和 serivce，包括尚未初始化的 与运行中的 Pod 交互 $ kubectl logs my-pod # dump 输出 pod 的日志（stdout） $ kubectl logs my-pod -c my-container # dump 输出 pod 中容器的日志（stdout，pod 中有多个容器的情况下使用） $ kubectl logs -f my-pod # 流式输出 pod 的日志（stdout） $ kubectl logs -f my-pod -c my-container # 流式输出 pod 中容器的日志（stdout，pod 中有多个容器的情况下使用） $ kubectl run -i --tty busybox --image=busybox -- sh # 交互式 shell 的方式运行 pod $ kubectl attach my-pod -i # 连接到运行中的容器 $ kubectl port-forward my-pod 5000:6000 # 转发 pod 中的 6000 端口到本地的 5000 端口 $ kubectl exec my-pod -- ls / # 在已存在的容器中执行命令（只有一个容器的情况下） $ kubectl exec my-pod -c my-container -- ls / # 在已存在的容器中执行命令（pod 中有多个容器的情况下） $ kubectl top pod POD_NAME --containers # 显示指定 pod 和容器的指标度量 与节点和集群交互 $ kubectl cordon my-node # 标记 my-node 不可调度 $ kubectl drain my-node # 清空 my-node 以待维护 $ kubectl uncordon my-node # 标记 my-node 可调度 $ kubectl top node my-node # 显示 my-node 的指标度量 $ kubectl cluster-info # 显示 master 和服务的地址 $ kubectl cluster-info dump # 将当前集群状态输出到 stdout $ kubectl cluster-info dump --output-directory=/path/to/cluster-state # 将当前集群状态输出到 /path/to/cluster-state # 如果该键和影响的污点（taint）已存在，则使用指定的值替换 $ kubectl taint nodes foo dedicated=special-user:NoSchedule 资源类型 下表列出的是 kubernetes 中所有支持的类型和缩写的别名。 资源类型 缩写别名 clusters componentstatuses cs configmaps cm daemonsets ds deployments deploy endpoints ep event ev horizontalpodautoscalers hpa ingresses ing jobs limitranges limits namespaces ns networkpolicies nodes no statefulsets persistentvolumeclaims pvc persistentvolumes pv pods po podsecuritypolicies psp podtemplates replicasets rs replicationcontrollers rc resourcequotas quota cronjob secrets serviceaccount sa services svc storageclasses thirdpartyresources 格式化输出 要以特定的格式向终端窗口输出详细信息，可以在 kubectl 命令中添加 -o 或者 -output 标志。 输出格式 描述 -o=custom-columns= 使用逗号分隔的自定义列列表打印表格 -o=custom-columns-file= 使用 文件中的自定义列模板打印表格 -o=json 输出 JSON 格式的 API 对象 -o=jsonpath= 打印 jsonpath 表达式中定义的字段 -o=jsonpath-file= 打印由 文件中的 jsonpath 表达式定义的字段 -o=name 仅打印资源名称 -o=wide 以纯文本格式输出任何附加信息，对于 Pod ，包含节点名称 -o=yaml 输出 YAML 格式的 API 对象 Kubectl 详细输出和调试 使用 -v 或 --v 标志跟着一个整数来指定日志级别。这里 描述了通用的 kubernetes 日志约定和相关的日志级别。 详细等级 描述 --v=0 总是对操作人员可见。 --v=1 合理的默认日志级别，如果您不需要详细输出。 --v=2 可能与系统的重大变化相关的，有关稳定状态的信息和重要的日志信息。这是对大多数系统推荐的日志级别。 --v=3 有关更改的扩展信息。 --v=4 调试级别详细输出。 --v=6 显示请求的资源。 --v=7 显示HTTP请求的header。 --v=8 显示HTTP请求的内容。 参考 Kubectl 概览 JsonPath 手册 本文是对官方文档的中文翻译，原文地址：https://kubernetes.io/docs/user-guide/kubectl-cheatsheet/ Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"tips/k8s查漏补缺.html":{"url":"tips/k8s查漏补缺.html","title":"k8s查漏补缺","keywords":"","body":"--restart-policy参数 带有参数 --restart-policy=Always 的资源将被部署为 Deployment 带有参数 --restart-policy=Never 的资源将被部署为 Pod。 同时 Kubectl 也会检查是否需要触发其他操作，例如：记录命令（用来进行回滚或审计） API 版本协商与 API 组 为了更容易地消除字段或者重新组织资源结构，Kubernetes 支持多个 API 版本，每个版本都在不同的 API 路径下，例如 /api/v1 或者 /apis/extensions/v1beta1。不同的 API 版本表明不同的稳定性和支持级别，更详细的描述可以参考 Kubernetes API 概述。 API 组主要作用是对类似资源进行分类，以便使得 Kubernetes API 更容易扩展。 API 的组别在 REST 路径或者序列化对象的 apiVersion 字段中指定。 例如：Deployment 的 API 组名是 apps，最新的 API 版本是 v1beta2，这就是为什么要在 Deployment manifests 顶部输入 apiVersion: apps/v1beta2。 为了提高性能，Kubectl 将 OpenAPI 模式缓存到了 ~/.kube/cache 目录。 如果想了解 API 发现的过程，可以尝试删除该目录并在运行 kubectl 命令时将 -v 参数的值设为最大值，然后将会看到所有试图找到这些 API 版本的 HTTP 请求。 kubectl 通过以下顺序来找到 Kubeconfig 如果提供了 --kubeconfig 参数， Kubectl 就使用 --kubeconfig 参数提供的 Kubeconfig 文件。 如果没有提供 --kubeconfig 参数，但设置了环境变量 $KUBECONFIG，则使用该环境变量提供的 Kubeconfig 文件。 如果 --kubeconfig 参数和环境变量 $KUBECONFIG 都没有提供，Kubectl 就使用默认的 Kubeconfig 文件 $HOME/.kube/config。 kubectl发出的HHP请求头中的内容 解析完 Kubeconfig 文件后，Kubectl 会确定当前要使用的上下文、当前指向的群集以及与当前用户关联的任何认证信息。如果用户提供了额外的参数（例如: --username），则优先使用这些参数覆盖 Kubeconfig 中指定的值。一旦拿到这些信息之后， Kubectl 就会把这些信息填充到将要发送的 HTTP 请求头中： x509 证书使用 tls.TLSConfig 发送（包括 CA 证书） bearer tokens 在 HTTP 请求头 Authorization 中发送 用户名和密码通过 HTTP 基本认证发送 OpenID 认证过程是由用户事先手动处理的，产生一个像 Bearer Token 一样被发送的 Token 每次收到请求时，Apiserver 都会通过令牌链进行认证，直到某一个认证成功为止： x509 处理程序将验证 HTTP 请求是否是由 CA 根证书签名的 TLS 密钥进行编码的 bearer token 处理程序将验证 --token-auth-file 参数提供的 Token 文件是否存在 基本认证处理程序确保 HTTP 请求的基本认证凭证与本地的状态匹配 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"tips/参考资料汇总.html":{"url":"tips/参考资料汇总.html","title":"参考资料汇总","keywords":"","body":" 好文系列 相关网站 文档手册 相关项目 课程培训 出版书籍 好文系列 《Kubernetes与云原生应用》系列之Kubernetes的系统架构与设计理念 ＊＊＊＊＊ 容器设计模式 如果有10000台机器，你想怎么玩？（一）概述 ＊＊＊＊＊ K8S Meetup 深圳站演讲回顾（二）：『才云』CTO 邓德源《基于 Kubernetes 的容器云平台落地实践》 ＊＊＊＊＊ 相关网站 docker专业介绍的网站dockerinfo docker专业介绍的网站dockone Stuq课程－从理论到生产环境实战：掌握Docker大规模部署和管理 kubernetes中文社区 k8s官方博客 K8sMeetup中国站 文档手册 kubernetes中文文档 ＊＊＊＊＊ 各个发行版的release note kubernetes官方指南 ＊＊＊＊＊ coreos的k8s相关文档 etcd3学习笔记 http://blog.fleeto.us/ 相关项目 专注于k8s工具链的开源仓库 awesome-kubernetes community repo 通过这个github 仓库可以了解最新的社区动态 ，和社区工具的发展 GitHub - rootsongjc/kubernetes-handbook: For kubernetes users and fans, let's rock it! ＊＊＊＊＊ Kubernetes Handbook （Kubernetes指南） 课程培训 才云k8s技术培训 Kubernetes 架构及原理 Kubernetes 层级化资源模型 Kubernetes 基础资源操作实践 基于Kubernetes的分布式 Tensor Flow 系统 https://www.udacity.com/course/scalable-microservices-with-kubernetes--ud615 Kubernetes完全教程-资料 Kubernetes完全教程-公开课视频 预备课：Docker 以及 Docker 网络 第一课：Kubernetes 架构概述 第二课：Kubernetes 的安装和运维 第三课：Kubernetes 的网络和存储 第四课：Kubernetes 的 API Spec 以及安全机制 第五课：Kubernetes 的应用管理（案例） 第六课：Kubernetes 的日志监控与故障排除 第七课：Kubernetes 的扩展开发 出版书籍 《容器与容器云》 《Kubernetes实战 》 《Kubernetes权威指南:​从Docker到Kuberne​tes实践全接触(第2版)》 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"tips/升级内核.html":{"url":"tips/升级内核.html","title":"centos7内核升级方法","keywords":"","body":" 在线安装的方式升级kernel 安装最新版本的kernel 设置grub,然后重启 查看系统上的所有可以内核 设置 grub2 验证 更新工具包 编译的方式升级内核 参考资料 相关术语 在线安装的方式升级kernel 安装最新版本的kernel # 载入公钥 rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org # 安装ELRepo rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm # 载入elrepo-kernel元数据 yum --disablerepo=\\* --enablerepo=elrepo-kernel repolist # 查看可用的rpm包 yum --disablerepo=\\* --enablerepo=elrepo-kernel list kernel* # 安装最新版本的kernel yum --disablerepo=\\* --enablerepo=elrepo-kernel install -y kernel-ml.x86_64 设置grub,然后重启 查看系统上的所有可以内核 awk -F\\' '$1==\"menuentry \" {print i++ \" : \" $2}' /etc/grub2.cfg 设置 grub2 1.通过 grub2-set-default 0 命令设置： 其中 0 来自上一步的 awk 命令： grub2-set-default 0 2. 编辑 /etc/default/grub 文件 设置 GRUB_DEFAULT=0，表示使用上一步的 awk 命令显示的编号为 0 的内核作为默认内核： # vi /etc/default/grub > GRUB_TIMEOUT=5 > GRUB_DISTRIBUTOR=\"$(sed 's, release .*$,,g' /etc/system-release)\" > GRUB_DEFAULT=0 > GRUB_DISABLE_SUBMENU=true > GRUB_TERMINAL_OUTPUT=\"console\" 3 生成 grub 配置文件并重启 通过 gurb2-mkconfig 命令创建 grub2 的配置文件，然后重启： grub2-mkconfig -o /boot/grub2/grub.cfg reboot 验证 通过 uname -r 查看 更新工具包 # 删除旧版本工具包 yum remove kernel-tools-libs.x86_64 kernel-tools.x86_64 # 安装新版本工具包 yum --disablerepo=\\* --enablerepo=elrepo-kernel install -y kernel-ml-tools.x86_64 编译的方式升级内核 [root@docker-dev ~]# yum groupinstall \"Development Tools\" -y #安装开发工具包 [root@docker-dev ~]# yum install -y elfutils-libelf-devel bc openssl-devel ncurses-devel wget ntpdate vim net-tools #安装常用软件 [root@docker-dev ~]# ntpdate -u times.aliyun.com #同步时间，如果时间不对编译可能会出问题 [root@docker-dev ~]# wget https://cdn.kernel.org/pub/linux/kernel/v4.x/linux-4.9.39.tar.xz #下载内核 [root@docker-dev ~]# tar -xf linux-4.9.39.tar.xz -C /usr/src/ #解压内核 [root@docker-dev ~]# cd /usr/src/linux-4.9.39/ [root@docker-dev linux-4.9.39]# cp /boot/config-3.10.0-514.el7.x86_64 .config [root@docker-dev linux-4.9.39]# sh -c 'yes \"\" | make oldconfig' [root@docker-dev linux-4.9.39]# make -j30 bzImage #测试机是32核，这里用30，尽可能加快编译速度 [root@docker-dev linux-4.9.39]# make -j30 modules [root@docker-dev linux-4.9.39]# make -j30 modules_install [root@docker-dev linux-4.9.39]# make install [root@docker-dev linux-4.9.39]# awk -F\\' '$1==\"menuentry \" {print $2}' /etc/grub2.cfg [root@docker-dev linux-4.9.39]# grub2-set-default 0 && init 6 [root@docker-dev ~]# uname -a Linux docker-dev 4.9.39 #1 SMP Mon Jul 24 11:28:25 CST 2017 x86_64 x86_64 x86_64 GNU/Linux 参考资料 内核官网:https://www.kernel.org/ 相关术语 kernel-ml kernel-ml 中的ml是英文mainline stable的缩写，elrepo-kernel中罗列出来的是最新的稳定主线版本。 kernel-lt kernel-lt 中的lt是英文long term support的缩写，elrepo-kernel中罗列出来的长期支持版本。 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"release/changelog.html":{"url":"release/changelog.html","title":"k8s版本更新说明","keywords":"","body":"Kubernetes版本更新日志 Kubernetes基本以每3个月发布一个版本的速度不断迭代，为了追踪不同版本的新特性，有必要在此记录一下。 每个Kubernetes版本的详细更新日志请参考：https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md 更多 追踪Kubernetes最新特性，请访问互动教学网站提供商 Katacoda.com 创建的 kubernetesstatus.com。 Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"release/1.7.html":{"url":"release/1.7.html","title":"k8s-1.7版本更新日志","keywords":"","body":"Kubernetes1.7更新日志 2017年6月29日，kuberentes1.7发布。该版本的kubernetes在安全性、存储和可扩展性方面有了很大的提升。改版本的详细更新文档请查看Changelog。 这些新特性中包含了安全性更高的加密的secret、pod间通讯的网络策略，限制kubelet访问的节点授权程序以及客户端/服务器TLS证书轮换。 对于那些在Kubernetes上运行横向扩展数据库的人来说，这个版本有一个主要的特性，可以为StatefulSet添加自动更新并增强DaemonSet的更新。我们还宣布了对本地存储的Alpha支持，以及用于更快地缩放StatefulSets的突发模式。 此外，对于高级用户，此发行版中的API聚合允许使用用于自定义的API与API server同时运行。其他亮点包括支持可扩展的准入控制器，可插拔云供应商程序和容器运行时接口（CRI）增强功能。 新功能 安全 Network Policy API 提升为稳定版本。用户可以通过使用网络插件实现的网络策略来控制哪些Pod之间能够互相通信。 节点授权和准入控制插件是新增加的功能，可以用于限制kubelet可以访问的secret、pod和其它基于节点的对象。 加密的Secret和etcd中的其它资源，现在是alpha版本。 Kubelet TLS bootstrapping 现在支持客户端和服务器端的证书轮换。 由API server存储的审计日志现在更具可定制性和可扩展性，支持事件过滤和webhook。它们还为系统审计提供更丰富的数据。 有状态负载 StatefulSet更新是1.7版本的beta功能，它允许使用包括滚动更新在内的一系列更新策略自动更新诸如Kafka，Zookeeper和etcd等有状态应用程序。 StatefulSets现在还支持对不需要通过Pod管理策略进行排序的应用程序进行快速扩展和启动。这可以是主要的性能改进。 本地存储（alpha）是有状态应用程序最常用的功能之一。用户现在可以通过标准的PVC/PV接口和StatefulSet中的StorageClass访问本地存储卷。 DaemonSet——为每个节点创建一个Pod，现在有了更新功能，在1.7中增加了智能回滚和历史记录功能。 新的StorageOS Volume插件可以使用本地或附加节点存储中以提供高可用的集群范围的持久卷。 可扩展性 运行时的API聚合是此版本中最强大的扩展功能，允许高级用户将Kubernetes风格的预先构建的第三方或用户创建的API添加到其集群中。 容器运行时接口（CRI）已经增强，可以使用新的RPC调用从运行时检索容器度量。 CRI的验证测试已经发布，与containerd进行了Alpha集成，现在支持基本的生命周期和镜像管理。参考深入介绍CRI的文章。 其它功能 引入了对外部准入控制器的Alpha支持，提供了两个选项，用于向API server添加自定义业务逻辑，以便在创建对象和验证策略时对其进行修改。 基于策略的联合资源布局提供Alpha版本，用于根据自定义需求（如法规、定价或性能）为联合（federated）集群提供布局策略。 弃用 第三方资源（TPR）已被自定义资源定义（Custom Resource Definitions，CRD）取代，后者提供了一个更清晰的API，并解决了TPR测试期间引发的问题和案例。如果您使用TPR测试版功能，则建议您迁移，因为它将在Kubernetes 1.8中被移除。 以上是Kubernetes1.7中的主要新特性，详细更新文档请查看Changelog。 参考 Kuberentes 1.7: Security Hardening, Stateful Application Updates and Extensibility Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"release/1.8.html":{"url":"release/1.8.html","title":"k8s-1.8版本更新日志","keywords":"","body":"Kubernetes1.8更新日志 2017年9月28日，kubernetes1.8版本发布。该版本中包括了一些功能改进和增强，并增加了项目的成熟度，将强了kubernetes的治理模式，这些都将有利于kubernetes项目的持续发展。 聚焦安全性 Kubernetes1.8的基于角色的访问控制（RBAC）成为stable支持。RBAC允许集群管理员动态定义角色对于Kubernetes API的访问策略。通过网络策略筛选出站流量的Beta支持，增强了对入站流量进行过滤的现有支持。 RBAC和网络策略是强化Kubernetes内组织和监管安全要求的两个强大工具。 Kubelet的传输层安全性（TLS）证书轮换成为beta版。自动证书轮换减轻了集群安全性运维的负担。 聚焦工作负载支持 Kubernetes 1.8通过apps/v1beta2组和版本推动核心工作负载API的beta版本。Beta版本包含当前版本的Deployment、DaemonSet、ReplicaSet和StatefulSet。 工作负载API是将现有工作负载迁移到Kubernetes以及开发基于Kubernetes的云原生应用程序提供了基石。 对于那些考虑在Kubernetes上运行大数据任务的，现在的工作负载API支持运行kubernetes原生支持的Apache Spark。 批量工作负载，比如夜间ETL工作，将从CronJobs的beta版本中受益。 自定义资源定义（CRD）在Kubernetes 1.8中依然为测试版。CRD提供了一个强大的机制来扩展Kubernetes和用户定义的API对象。 CRD的一个用例是通过Operator Pattern自动执行复杂的有状态应用，例如键值存储、数据库和存储引擎。随着稳定性的继续推进，预计将继续加强对CRD的验证。 更多 Volume快照、PV调整大小、自动taint、pod优先级、kubectl插件等！ 除了稳定现有的功能，Kubernetes 1.8还提供了许多预览新功能的alpha版本。 社区中的每个特别兴趣小组（SIG）都在继续为所在领域的用户提供更多的功能。有关完整列表，请访问发行说明。 参考 Kubernetes 1.8: Security, Workloads and Feature Depth Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"release/1.9.html":{"url":"release/1.9.html","title":"k8s-1.9版本更新日志","keywords":"","body":"Kubernetes1.9更新日志 2017年12月15日，kubernetes1.9版本发布。Kubernetes依然按照每三个月一个大版本发布的速度稳定迭代，这是今年发布的第四个版本，也是今年的最后一个版本，该版本最大的改进是Apps Workloads API成为稳定版本，这消除了很多潜在用户对于该功能稳定性的担忧。还有一个重大更新，就是测试支持了Windows了，这打开了在kubernetes中运行Windows工作负载的大门。 Workloads API GA apps/v1 Workloads API成为GA（General Availability），且默认启用。 Apps Workloads API将DaemonSet、Deployment、ReplicaSet和StatefulSet API组合在一起，作为Kubernetes中长时间运行的无状态和有状态工作负载的基础。 Deployment和ReplicaSet是Kubernetes中最常用的两个对象，经过一年多的实际使用和反馈后，现在已经趋于稳定。SIG apps同时将这些经验应用到另外的两个对象上，使得DaemonSet和StatefulSet也能顺利毕业走向成熟。v1（GA）意味着已经生产可用，并保证长期的向后兼容。 Windows支持（beta） Kubernetes最初是为Linux系统开发的，但是用户逐渐意识到容器编排的好处，我们看到有人需要在Kubernetes上运行Windows工作负载。在12个月前，我们开始认真考虑在Kubernetes上支持Windows Server的工作。 SIG-Windows现在已经将这个功能推广到beta版本，这意味着我们可以评估它的使用情况。 增强存储 kubernetes从第一个版本开始就支持多种持久化数据存储，包括常用的NFS或iSCSI，以及对主要公共云和私有云提供商的存储解决方案的原生支持。随着项目和生态系统的发展，Kubernetes的存储选择越来越多。然而，为新的存储系统添加volume插件一直是一个挑战。 容器存储接口（CSI）是一个跨行业标准计划，旨在降低云原生存储开发的障碍并确保兼容性。 SIG-Storage和CSI社区正在合作提供一个单一接口，用于配置、附着和挂载与Kubernetes兼容的存储。 Kubernetes 1.9引入了容器存储接口（CSI）的alpha实现，这将使挂载新的volume插件就像部署一个pod一样简单，并且第三方存储提供商在开发他们的解决方案时也无需修改kubernetes的核心代码。 由于该功能在1.9版本中为alpha，因此必须明确启用该功能，不建议用于生产使用，但它为更具扩展性和基于标准的Kubernetes存储生态系统提供了清晰的路线图。 其它功能 自定义资源定义（CRD）校验，现在已经成为beta，默认情况下已启用，可以用来帮助CRD作者对于无效对象定义给出清晰和即时的反馈。 SIG Node硬件加速器转向alpha，启用GPU，从而实现机器学习和其他高性能工作负载。 CoreDNS alpha可以使用标准工具来安装CoreDNS。 kube-proxy的IPVS模式进入beta版，为大型集群提供更好的可扩展性和性能。 社区中的每个特别兴趣小组（SIG）继续提供其所在领域的用户最迫切需要的功能。有关完整列表，请访问发行说明。 获取 Kubernetes1.9已经可以通过GitHub下载。 参考 Kubernetes 1.9: Apps Workloads GA and Expanded Ecosystem Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"release/1.10.html":{"url":"release/1.10.html","title":"k8s-1.10版本更新日志","keywords":"","body":"Kubernetes1.10更新日志 2018年3月26日，kubernetes1.10版本发布，这是2018年发布的第一个版本。该版本的Kubernetes主要提升了Kubernetes的成熟度、可扩展性与可插入性。 该版本提升了三大关键性功能的稳定度，分别为存储、安全与网络。另外，此次新版本还引入了外部kubectl凭证提供程序（处于alpha测试阶段）、在安装时将默认的DNS服务切换为CoreDNS（beta测试阶段）以及容器存储接口（简称CSI）与持久化本地卷的beta测试版。 下面再分别说下三大关键更新。 存储 CSI（容器存储接口）迎来Beta版本，可以通过插件的形式安装存储。 持久化本地存储管理也迎来Beta版本。 对PV的一系列更新，可以自动阻止Pod正在使用的PVC的删除，阻止已绑定到PVC的PV的删除操作，这样可以保证所有存储对象可以按照正确的顺序被删除。 安全 kubectl可以对接不同的凭证提供程序 各云服务供应商、厂商以及其他平台开发者现在能够发布二进制插件以处理特定云供应商IAM服务的身价验证 网络 将原来的kube-dns切换为CoreDNS 获取 Kubernetes1.10已经可以通过GitHub下载。 参考 Kubernetes 1.10: Stabilizing Storage, Security, and Networking Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"release/1.11.html":{"url":"release/1.11.html","title":"k8s-1.11版本更新日志","keywords":"","body":"Kubernetes1.11更新日志 2018年6月27日，Kubernetes1.11版本发布，这也是Kubernetes在2018年度的第二个大版本。该版本最大的变化是增强了集群内负载均衡，CoreDNS毕业成为GA版本，作为Kubernetes内置默认DNS服务。 该版本的主要亮点集中在网络层面，SIG-API Machinery和SIG-Node的两项主要功能成为beta版本，同时进一步增强存储功能。利用该版本，用户将能够更轻松地将任意基础设施——无论云平台还是内部环境——接入Kubernetes当中。 本版本最值得关注的是增加了两项备受期待的功能，分别为：基于IPVS的集群内负载均衡，以及将CoreDNS作为集群DNS的附加选项——这意味着生产应用的可扩展性与灵活性都将得到提升。 可以称得上是亮点的功能有： 基于IPVS的集群内服务负载均衡正式上线 IPVS（即IP虚拟服务器）能够提供高性能内核负载均衡功能，且其编程接口较iptables更为简单，且网络吞吐量更上一层楼、提供更低的编程延迟，但是仍然不是默认配置，需要自己配置打开。 CoreDNS正式上线 CoreDNS之前一直是作为Kuberentes的一个插件独立开发，Kubernetes中最早使用的是名为KubeDNS的DNS组件，CoreDNS是一套灵活且可扩展的权威DNS服务器，可直接与Kubernetes API相集成。相较于原有DNS服务器，CoreDNS的移动部件更少——仅包含单一可执行文件与单一进程，且允许用户通过创建自定义DNS条目以支持更为灵活的用例。其同样使用Go语言编写而成，因此天然具备更理想的安全性水平。 动态Kubelet配置进入Beta阶段 通过这项改进，用户可以将新的Kubelet配置部署在运行中的集群之内。此前，Kubelet需要通过命令行标记进行配置，这导致我们很难对运行中的集群进行Kubelet配置更新。在此次beta功能的帮助下，用户将能够通过API Server随时配置集群中运行中的Kubelet。 自定义资源定义（CRD）现在可定义多个版本 CRD不再局限于对单一版本的定制化资源作出定义。如今，利用此项beta测试功能，用户可以为资源定义多个版本。未来，其还将进一步扩展以支持版本间自动转换功能。目前，该功能允许定制化资源作者“以安全变更的方式实现版本升级，例如由v1beta1切换至v1“，并为发生变化的资源创建迁移路径。 存储增强 主要是增强了CSI，1.11版本为CSI带来了对原始数据块分卷的alph支持能力，将CSI与新的kubelet插件注册机制相集成，同时降低了向CSI插件传递秘密凭证的难度。支持对持久分卷进行在线大小调整（alpha版本）。这意味着用户能够增加持久分卷的大小，而完全无需终止pod并卸载对应分卷。用户可以更新VPC以提交新的分卷大小请求，kubelet则负责完成文件系统的大小调整。 参考 Kubernetes 1.11: In-Cluster Load Balancing and CoreDNS Plugin Graduate to General Availability Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"release/1.12.html":{"url":"release/1.12.html","title":"k8s-1.12版本更新日志","keywords":"","body":"Kubernetes1.12更新日志 该版本发布继续关注Kubernetes的稳定性，主要是内部改进和一些功能的毕业。该版本中毕业的功能有安全性和Azure的关键功能。此版本中还有两个毕业的值得注意的新增功能：Kubelet TLS Bootstrap和 Azure Virtual Machine Scale Sets（AVMSS）支持。 这些新功能意味着更高的安全性、可用性、弹性和易用性，可以更快地将生产应用程序推向市场。该版本还表明Kubernetes在开发人员方面日益成熟。 下面是该版本中的一些关键功能介绍。 Kubelet TLS Bootstrap GA 我们很高兴地宣布Kubelet TLS Bootstrap GA。在Kubernetes 1.4中，我们引入了一个API，用于从集群级证书颁发机构（CA）请求证书。此API的初衷是为kubelet启用TLS客户端证书的配置。此功能允许kubelet将自身引导至TLS安全集群。最重要的是，它可以自动提供和分发签名证书。 之前，当kubelet第一次运行时，必须在集群启动期间在带外进程中为其提供客户端凭据。负担是运营商提供这些凭证的负担。由于此任务对于手动执行和复杂自动化而言非常繁重，因此许多运营商为所有kubelet部署了具有单个凭证和单一身份的集群。这些设置阻止了节点锁定功能的部署，如节点授权器和NodeRestriction准入控制器。 为了缓解这个问题，SIG Auth引入了一种方法，让kubelet生成私钥，CSR用于提交到集群级证书签署过程。v1（GA）标识表示生产加固和准备就绪，保证长期向后兼容。 除此之外，Kubelet服务器证书引导程序和轮换正在转向测试版。目前，当kubelet首次启动时，它会生成一个自签名证书/密钥对，用于接受传入的TLS连接。此功能引入了一个在本地生成密钥，然后向集群API server发出证书签名请求以获取由集群的根证书颁发机构签名的关联证书的过程。此外，当证书接近过期时，将使用相同的机制来请求更新的证书。 稳定支持Azure Virtual Machine Scale Sets（VMSS）和Cluster-Autoscaler Azure Virtual Machine Scale Sets（VMSS）允许您创建和管理可以根据需求或设置的计划自动增加或减少的同类VM池。这使您可以轻松管理、扩展和负载均衡多个VM，从而提供高可用性和应用程序弹性，非常适合可作为Kubernetes工作负载运行的大型应用程序。 凭借这一新的稳定功能，Kubernetes支持使用Azure VMSS扩展容器化应用程序，包括将其与cluster-autoscaler集成的功能根据相同的条件自动调整Kubernetes集群的大小。 其他值得注意的功能更新 RuntimeClass是一个新的集群作用域资源，它将容器运行时属性表示为作为alpha功能发布的控制平面。 Kubernetes和CSI的快照/恢复功能正在作为alpha功能推出。这提供了标准化的API设计（CRD），并为CSI卷驱动程序添加了PV快照/恢复支持。 拓扑感知动态配置现在处于测试阶段，存储资源现在可以感知自己的位置。这还包括对AWS EBS和GCE PD的beta支持。 可配置的pod进程命名空间共享处于测试阶段，用户可以通过在PodSpec中设置选项来配置pod中的容器以共享公共PID命名空间。 根据条件的taint节点现在处于测试阶段，用户可以通过使用taint来表示阻止调度的节点条件。 Horizo​ntal Pod Autoscaler中的任意/自定义指标正在转向第二个测试版，以测试一些其他增强功能。这项重新设计的Horizo​ntal Pod Autoscaler功能包括对自定义指标和状态条件的支持。 允许Horizo​ntal Pod Autoscaler更快地达到适当大小正在转向测试版。 Pod的垂直缩放现在处于测试阶段，使得可以在其生命周期内改变pod上的资源限制。 通过KMS进行静态加密目前处于测试阶段。增加了多个加密提供商，包括Google Cloud KMS、Azure Key Vault、AWS KMS和Hashicorp Vault，它们会在数据存储到etcd时对其进行加密。 可用性 Kubernetes 1.12可以在GitHub上下载。要开始使用Kubernetes，请查看这些交互式教程。您也可以使用Kubeadm来安装1.12。 参考 Kubernetes 1.12: Kubelet TLS Bootstrap and Azure Virtual Machine Scale Sets (VMSS) Move to General Availability Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"release/1.13.html":{"url":"release/1.13.html","title":"k8s-1.13版本更新日志","keywords":"","body":"Kubernetes 1.13 更新日志 2018年12月3日，Kubernetes 1.13发布，这是2018年发布的第四个也是最后一个大版本。该版本中最显著地改进包括： 使用 kubeadm 简化集群管理 CSI（容器存储接口），查看 CSI 规范 CoreDNS 作为默认的 DNS 以上功能正式成为 GA（General Available）。 还有其他一些小的功能更新，例如： 支持第三方设备监控插件成为 alpha 功能。 kubelet 设备插件注册 GA。 拓扑感知的 Volume 调度进入 stable。 APIServer DryRun 进入 beta。 kubectl diff 进入 beta。 使用 PV 源的原始块设备进入 beta。 详细的更新日志请访问 Kubernetes 1.13: Simplified Cluster Management with Kubeadm, Container Storage Interface (CSI), and CoreDNS as Default DNS are Now Generally Available。 参考 Overview of kubeadm Kubernetes 1.13: Simplified Cluster Management with Kubeadm, Container Storage Interface (CSI), and CoreDNS as Default DNS are Now Generally Available Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-03 17:30:35 "},"release/1.14.html":{"url":"release/1.14.html","title":"k8s-1.14版本更新日志","keywords":"","body":"Kubernetes 1.14 更新日志 2019年3月26日，Kubernetes 1.14发布，这是2019年发布的第一个版本。该版本中最显著地改进包括： 对于管理 Windows node 的生产级支持。在这之前，Windows节点一直吃处于beta阶段，如今，Kubernetes 的Pod，Service，应用编排，CNI 网络等绝大多数核心能力都已经在 Windows 节点上得到了支持。开始正式支持将Windows节点添加为工作节点并部署容器，提供了Linux/Windows应用混合部署的统一管理能力，再一次印证了这次发布的可用度。 重写了 kubectl 的文档，使用声明性Resource Config来管理资源。文档按照独立书籍格式，作为独立站点发布。新域名 https://kubectl.docs.kubernetes.io/，文档本身类似 Gitbook 的形式，使用 Resource Config 的形式组织，集成了 kustomize，还有了自己的 logo 和吉祥物 kubee-cuddle。 Kustomize 允许用户从一个基础 YAML 文件，通过 Overlay 的方式生成最终部署应用所需的 YAML 文件，而不是像 Helm 那样通过字符串替换的方式来直接修改基础 YAML 文件（模板）。这样，在一个用户通过 Overlay 生成新的 YAML 文件的同时，其他用户可以完全不受影响的使用任何一个基础 YAML 或者某一层生成出来的 YAML 。这使得每一个用户，都可以通过 fork/modify/rebase 这样 Git 风格的流程来管理海量的 YAML 文件。 kubectl 插件机制发布稳定版本。 Persistent Local Volume GA。允许用户将节点本地存储作为持久卷来源。持久化本地存储主要应用是数据库，本地存储与远程存储有着更好的性能，除了性能之外，本地存储通常也更便宜。 限制每个 Pod 的 PID 功能发布 beta 版本。 详细的更新日志请访问 Kubernetes 1.14: Production-level support for Windows Nodes, Kubectl Updates, Persistent Local Volumes GA。 参考 Kubernetes 1.14: Production-level support for Windows Nodes, Kubectl Updates, Persistent Local Volumes GA - kuberentes.io Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-23 11:20:29 "},"release/1.15.html":{"url":"release/1.15.html","title":"k8s-1.15版本更新日志","keywords":"","body":"Kubernetes 1.15 更新日志 2019年6月20日，Kubernetes 1.15 发布，这是 2019 年的第二个版本。该版本中最显著地改进包括： 核心Kubernetes API可扩展性：主要是CRD方面。 SIG API Machinery 相关的改进 集群生命周期的稳定性和可用性的改进 kubeadm高可用（HA）进入Beta阶段。kubeadm允许用户使用熟悉的kubeadm init和kubeadm join命令来配置和部署HA控制平面。 kubeadm无缝升级所有证书：kubeadm现在可以在证书到期之前，无缝升级所有证书（升级时）。 官网文档：https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-alpha 持续改进容器存储接口（CSI）：继续致力于将内部的卷插件迁移到CSI。目前所有存储主要有两种接入方式：CSI和Flexvolume。 详细的日志请访问：Kubernetes 1.15: Extensibility and Continuous Improvement。 参考 Kubernetes 1.15: Extensibility and Continuous Improvement Kubernetes 1.15 Changelog Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-23 11:22:48 "},"release/1.16.html":{"url":"release/1.16.html","title":"k8s-1.16版本更新日志","keywords":"","body":"Kubernetes 1.16 更新日志 2019年09月18日，Kubernetes 1.16 发布，这是 2019 年的第三个版本。该版本中最显著地改进包括： CRD（Custom resources）：CRD 是对 Kubernetes 扩展的一种方式，用以服务于新的资源类型，自v1.7版本以来，CRD一直在Beta阶段。如今，CRD 正式成为GA版本。 准入控制webhook：准入控制作为Kubernetes扩展机制，webhooks自1.9版本以来，一直在Beta阶段，如今，Admission webhook 也正式成为GA版本。 IPv4/IPv6双栈协议支持：对Pod和Service的IPv4与IPv6地址的支持。 CSI规范中支持调整卷大小：新版本有大量和 Volume 及 Volume 修改相关的增强。而 CSI 规范中对 Volume 调整的支持正在迁移至 Beta 阶段，使得任何卷插件都可以调整大小。 参考 Kubernetes 1.16 Changelog Copyright © suredandan 2018 all right reserved，powered by GitbookUpdateTime: 2019-10-23 11:24:38 "}}